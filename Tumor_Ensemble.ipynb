{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv(r\"E:\\Pending Assignments\\Ensemble_Techniques_Problem Statement\\Datasets_ET\\Tumor_Ensemble.csv\")\r\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87139402</td>\n",
       "      <td>B</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8910251</td>\n",
       "      <td>B</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>905520</td>\n",
       "      <td>B</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>868871</td>\n",
       "      <td>B</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9012568</td>\n",
       "      <td>B</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>911320502</td>\n",
       "      <td>B</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.28</td>\n",
       "      <td>537.3</td>\n",
       "      <td>0.07466</td>\n",
       "      <td>0.05994</td>\n",
       "      <td>0.04859</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>...</td>\n",
       "      <td>14.90</td>\n",
       "      <td>23.89</td>\n",
       "      <td>95.10</td>\n",
       "      <td>687.6</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.18760</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>898677</td>\n",
       "      <td>B</td>\n",
       "      <td>10.26</td>\n",
       "      <td>14.71</td>\n",
       "      <td>66.20</td>\n",
       "      <td>321.6</td>\n",
       "      <td>0.09882</td>\n",
       "      <td>0.09159</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>0.02037</td>\n",
       "      <td>...</td>\n",
       "      <td>10.88</td>\n",
       "      <td>19.48</td>\n",
       "      <td>70.89</td>\n",
       "      <td>357.1</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.07162</td>\n",
       "      <td>0.04074</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.08488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>873885</td>\n",
       "      <td>M</td>\n",
       "      <td>15.28</td>\n",
       "      <td>22.41</td>\n",
       "      <td>98.92</td>\n",
       "      <td>710.6</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.03263</td>\n",
       "      <td>...</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>113.80</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.09772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>911201</td>\n",
       "      <td>B</td>\n",
       "      <td>14.53</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.06495</td>\n",
       "      <td>...</td>\n",
       "      <td>15.80</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.13730</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.2606</td>\n",
       "      <td>0.07810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>9012795</td>\n",
       "      <td>M</td>\n",
       "      <td>21.37</td>\n",
       "      <td>15.10</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>...</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.84</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>0.1192</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.40240</td>\n",
       "      <td>0.19660</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.08666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "0     87139402         B        12.32         12.39           78.85   \n",
       "1      8910251         B        10.60         18.95           69.28   \n",
       "2       905520         B        11.04         16.83           70.92   \n",
       "3       868871         B        11.28         13.39           73.00   \n",
       "4      9012568         B        15.19         13.21           97.65   \n",
       "..         ...       ...          ...           ...             ...   \n",
       "564  911320502         B        13.17         18.22           84.28   \n",
       "565     898677         B        10.26         14.71           66.20   \n",
       "566     873885         M        15.28         22.41           98.92   \n",
       "567     911201         B        14.53         13.98           93.86   \n",
       "568    9012795         M        21.37         15.10          141.30   \n",
       "\n",
       "     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "0        464.1          0.10280           0.06981         0.03987   \n",
       "1        346.4          0.09688           0.11470         0.06387   \n",
       "2        373.2          0.10770           0.07804         0.03046   \n",
       "3        384.8          0.11640           0.11360         0.04635   \n",
       "4        711.8          0.07963           0.06934         0.03393   \n",
       "..         ...              ...               ...             ...   \n",
       "564      537.3          0.07466           0.05994         0.04859   \n",
       "565      321.6          0.09882           0.09159         0.03581   \n",
       "566      710.6          0.09057           0.10520         0.05375   \n",
       "567      644.2          0.10990           0.09242         0.06895   \n",
       "568     1386.0          0.10010           0.15150         0.19320   \n",
       "\n",
       "     points_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0        0.03700  ...         13.50          15.64            86.97   \n",
       "1        0.02642  ...         11.88          22.94            78.28   \n",
       "2        0.02480  ...         12.41          26.44            79.93   \n",
       "3        0.04796  ...         11.92          15.77            76.53   \n",
       "4        0.02657  ...         16.20          15.73           104.50   \n",
       "..           ...  ...           ...            ...              ...   \n",
       "564      0.02870  ...         14.90          23.89            95.10   \n",
       "565      0.02037  ...         10.88          19.48            70.89   \n",
       "566      0.03263  ...         17.80          28.03           113.80   \n",
       "567      0.06495  ...         15.80          16.93           103.10   \n",
       "568      0.12550  ...         22.69          21.84           152.10   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0         549.1            0.1385             0.1266          0.12420   \n",
       "1         424.8            0.1213             0.2515          0.19160   \n",
       "2         471.4            0.1369             0.1482          0.10670   \n",
       "3         434.0            0.1367             0.1822          0.08669   \n",
       "4         819.1            0.1126             0.1737          0.13620   \n",
       "..          ...               ...                ...              ...   \n",
       "564       687.6            0.1282             0.1965          0.18760   \n",
       "565       357.1            0.1360             0.1636          0.07162   \n",
       "566       973.1            0.1301             0.3299          0.36300   \n",
       "567       749.9            0.1347             0.1478          0.13730   \n",
       "568      1535.0            0.1192             0.2840          0.40240   \n",
       "\n",
       "     points_worst  symmetry_worst  dimension_worst  \n",
       "0         0.09391          0.2827          0.06771  \n",
       "1         0.07926          0.2940          0.07587  \n",
       "2         0.07431          0.2998          0.07881  \n",
       "3         0.08611          0.2102          0.06784  \n",
       "4         0.08178          0.2487          0.06766  \n",
       "..            ...             ...              ...  \n",
       "564       0.10450          0.2235          0.06925  \n",
       "565       0.04074          0.2434          0.08488  \n",
       "566       0.12260          0.3175          0.09772  \n",
       "567       0.10690          0.2606          0.07810  \n",
       "568       0.19660          0.2730          0.08666  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "categorical_features=[feature for feature in df.columns if df[feature].dtypes=='O']\r\n",
    "categorical_features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['diagnosis']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "lb = LabelEncoder()\r\n",
    "\r\n",
    "for i in categorical_features:\r\n",
    "    df[i] = lb.fit_transform(df[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87139402</td>\n",
       "      <td>0</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8910251</td>\n",
       "      <td>0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>905520</td>\n",
       "      <td>0</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>868871</td>\n",
       "      <td>0</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9012568</td>\n",
       "      <td>0</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>911320502</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.28</td>\n",
       "      <td>537.3</td>\n",
       "      <td>0.07466</td>\n",
       "      <td>0.05994</td>\n",
       "      <td>0.04859</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>...</td>\n",
       "      <td>14.90</td>\n",
       "      <td>23.89</td>\n",
       "      <td>95.10</td>\n",
       "      <td>687.6</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.18760</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>898677</td>\n",
       "      <td>0</td>\n",
       "      <td>10.26</td>\n",
       "      <td>14.71</td>\n",
       "      <td>66.20</td>\n",
       "      <td>321.6</td>\n",
       "      <td>0.09882</td>\n",
       "      <td>0.09159</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>0.02037</td>\n",
       "      <td>...</td>\n",
       "      <td>10.88</td>\n",
       "      <td>19.48</td>\n",
       "      <td>70.89</td>\n",
       "      <td>357.1</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.07162</td>\n",
       "      <td>0.04074</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.08488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>873885</td>\n",
       "      <td>1</td>\n",
       "      <td>15.28</td>\n",
       "      <td>22.41</td>\n",
       "      <td>98.92</td>\n",
       "      <td>710.6</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.03263</td>\n",
       "      <td>...</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>113.80</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.09772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>911201</td>\n",
       "      <td>0</td>\n",
       "      <td>14.53</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.06495</td>\n",
       "      <td>...</td>\n",
       "      <td>15.80</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.13730</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.2606</td>\n",
       "      <td>0.07810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>9012795</td>\n",
       "      <td>1</td>\n",
       "      <td>21.37</td>\n",
       "      <td>15.10</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>...</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.84</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>0.1192</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.40240</td>\n",
       "      <td>0.19660</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.08666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "0     87139402          0        12.32         12.39           78.85   \n",
       "1      8910251          0        10.60         18.95           69.28   \n",
       "2       905520          0        11.04         16.83           70.92   \n",
       "3       868871          0        11.28         13.39           73.00   \n",
       "4      9012568          0        15.19         13.21           97.65   \n",
       "..         ...        ...          ...           ...             ...   \n",
       "564  911320502          0        13.17         18.22           84.28   \n",
       "565     898677          0        10.26         14.71           66.20   \n",
       "566     873885          1        15.28         22.41           98.92   \n",
       "567     911201          0        14.53         13.98           93.86   \n",
       "568    9012795          1        21.37         15.10          141.30   \n",
       "\n",
       "     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "0        464.1          0.10280           0.06981         0.03987   \n",
       "1        346.4          0.09688           0.11470         0.06387   \n",
       "2        373.2          0.10770           0.07804         0.03046   \n",
       "3        384.8          0.11640           0.11360         0.04635   \n",
       "4        711.8          0.07963           0.06934         0.03393   \n",
       "..         ...              ...               ...             ...   \n",
       "564      537.3          0.07466           0.05994         0.04859   \n",
       "565      321.6          0.09882           0.09159         0.03581   \n",
       "566      710.6          0.09057           0.10520         0.05375   \n",
       "567      644.2          0.10990           0.09242         0.06895   \n",
       "568     1386.0          0.10010           0.15150         0.19320   \n",
       "\n",
       "     points_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0        0.03700  ...         13.50          15.64            86.97   \n",
       "1        0.02642  ...         11.88          22.94            78.28   \n",
       "2        0.02480  ...         12.41          26.44            79.93   \n",
       "3        0.04796  ...         11.92          15.77            76.53   \n",
       "4        0.02657  ...         16.20          15.73           104.50   \n",
       "..           ...  ...           ...            ...              ...   \n",
       "564      0.02870  ...         14.90          23.89            95.10   \n",
       "565      0.02037  ...         10.88          19.48            70.89   \n",
       "566      0.03263  ...         17.80          28.03           113.80   \n",
       "567      0.06495  ...         15.80          16.93           103.10   \n",
       "568      0.12550  ...         22.69          21.84           152.10   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0         549.1            0.1385             0.1266          0.12420   \n",
       "1         424.8            0.1213             0.2515          0.19160   \n",
       "2         471.4            0.1369             0.1482          0.10670   \n",
       "3         434.0            0.1367             0.1822          0.08669   \n",
       "4         819.1            0.1126             0.1737          0.13620   \n",
       "..          ...               ...                ...              ...   \n",
       "564       687.6            0.1282             0.1965          0.18760   \n",
       "565       357.1            0.1360             0.1636          0.07162   \n",
       "566       973.1            0.1301             0.3299          0.36300   \n",
       "567       749.9            0.1347             0.1478          0.13730   \n",
       "568      1535.0            0.1192             0.2840          0.40240   \n",
       "\n",
       "     points_worst  symmetry_worst  dimension_worst  \n",
       "0         0.09391          0.2827          0.06771  \n",
       "1         0.07926          0.2940          0.07587  \n",
       "2         0.07431          0.2998          0.07881  \n",
       "3         0.08611          0.2102          0.06784  \n",
       "4         0.08178          0.2487          0.06766  \n",
       "..            ...             ...              ...  \n",
       "564       0.10450          0.2235          0.06925  \n",
       "565       0.04074          0.2434          0.08488  \n",
       "566       0.12260          0.3175          0.09772  \n",
       "567       0.10690          0.2606          0.07810  \n",
       "568       0.19660          0.2730          0.08666  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df.columns.to_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.tolist of Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se',\n",
       "       'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'points_se', 'symmetry_se',\n",
       "       'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst',\n",
       "       'area_worst', 'smoothness_worst', 'compactness_worst',\n",
       "       'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst'],\n",
       "      dtype='object')>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#split data into inputs and targets\r\n",
    "X = df.drop(columns = ['diagnosis'])\r\n",
    "y = df['diagnosis']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "#split data into train and test sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Boosting**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "ada_clf = AdaBoostClassifier(learning_rate = 0.02, n_estimators = 5000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.02, n_estimators=5000)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Evaluation on Testing Data\r\n",
    "print(confusion_matrix(y_test, ada_clf.predict(X_test)))\r\n",
    "print(accuracy_score(y_test, ada_clf.predict(X_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[106   1]\n",
      " [  3  61]]\n",
      "0.9766081871345029\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "plot_confusion_matrix(ada_clf, X_test, y_test)\r\n",
    "plt.title(\"Confusion matrix of ADA Boosting\")\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Bagging**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from sklearn import tree\r\n",
    "clftree = tree.DecisionTreeClassifier()\r\n",
    "from sklearn.ensemble import BaggingClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "bag_clf = BaggingClassifier(base_estimator = clftree, n_estimators = 500,\r\n",
    "                            bootstrap = True, n_jobs = 1, random_state = 42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=1, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Evaluation on Testing Data\r\n",
    "print(confusion_matrix(y_test, bag_clf.predict(X_test)))\r\n",
    "print(accuracy_score(y_test, bag_clf.predict(X_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[104   3]\n",
      " [  2  62]]\n",
      "0.9707602339181286\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Evaluation on Training Data\r\n",
    "print(confusion_matrix(y_train, bag_clf.predict(X_train)))\r\n",
    "print(accuracy_score(y_train, bag_clf.predict(X_train)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[250   0]\n",
      " [  0 148]]\n",
      "1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "plot_confusion_matrix(bag_clf, X_test, y_test)\r\n",
    "plt.title(\"Confusion matrix of Bagging Boosting\")\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAHHCAYAAAC4M/EEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE5ElEQVR4nO3deVhUZfsH8O+wDfumAqKILIqYa1pE7oWSW5q7Py0wlxb3Na3XfaHUlDBzT9S0MktLLRNxy1xKXDIXFEUjEFwQEJR1nt8fvJzXEdQZZmCYOd/PdZ1L55mz3DPMcHM/z3POUQghBIiIiMhkmRk6ACIiIqpYTPZEREQmjsmeiIjIxDHZExERmTgmeyIiIhPHZE9ERGTimOyJiIhMHJM9ERGRiWOyJyIiMnFM9kbqypUr6NSpE5ycnKBQKLBjxw697v/69etQKBSIjo7W635NQd26dREeHl7px83OzsawYcPg4eEBhUKBcePGVXoM+hYdHQ2FQoHr168bOhSjEh4ejrp16xo6DDIiTPY6uHr1Kt555x34+vrC2toajo6OaNWqFT777DM8fPiwQo8dFhaGc+fOYf78+di0aRNatmxZocczRRcuXMCsWbOMJtEsWLAA0dHReO+997Bp0ya8+eabT1y3bt26UCgU0mJtbY169eph8uTJSE9Pr8Soq65Zs2apvUdmZmaoWbMmunXrhuPHjxs6PKSkpGDWrFk4c+aMoUMhE6DgtfHLZ/fu3ejbty+USiXeeustNGrUCPn5+Thy5Ai+//57hIeHY/Xq1RVy7IcPH8LW1hYfffQR5s2bVyHHEEIgLy8PlpaWMDc3r5BjGNq2bdvQt29fHDhwAO3bt9d4u7y8PJiZmcHS0rLigivDSy+9BAsLCxw5cuSZ69atWxcuLi6YOHEiACA3NxdxcXFYu3Ytmjdvjj/++KOiw9VIUVERCgoKoFQqoVAoKvXYs2bNwuzZs7FixQrY29tDpVIhKSkJa9asQUpKCv744w80a9asUmN61MmTJ/HCCy9g/fr1pXqSCgoKoFKpoFQqDRMcGR0LQwdgjBITEzFgwAB4e3tj//79qFmzpvTcyJEjkZCQgN27d1fY8W/fvg0AcHZ2rrBjlFSDVEwIgdzcXNjY2BjsF+ytW7fQsGFDjdevVasWBg8eLD0eNmwY7O3tsXjxYly5cgX16tWriDC1Ym5ubvA/Jvv06YPq1atLj3v27IlGjRrhu+++M2iyf5rK/kOTjB+78cth4cKFyM7Oxrp169QSfQl/f3+MHTtWelxYWIi5c+fCz88PSqUSdevWxYcffoi8vDy17erWrYtu3brhyJEjePHFF2FtbQ1fX19s3LhRWmfWrFnw9vYGAEyePBkKhUIau3vSOF5Jd+WjYmJi0Lp1azg7O8Pe3h4BAQH48MMPpeefNGa/f/9+tGnTBnZ2dnB2dkaPHj1w8eLFMo+XkJCA8PBwODs7w8nJCUOGDMGDBw+e/Mb+V/v27dGoUSP89ddfaNeuHWxtbeHv749t27YBAA4dOoSgoCDY2NggICAA+/btU9v+xo0beP/99xEQEAAbGxtUq1YNffv2Veuuj46ORt++fQEAHTp0kLpyDx48COB/P4tff/0VLVu2hI2NDVatWiU9V1JpCSHQoUMH1KhRA7du3ZL2n5+fj8aNG8PPzw85OTlPfb23bt3C0KFD4e7uDmtrazRt2hQbNmyQnj948CAUCgUSExOxe/duKdbyDD94eHgAACws/vd3/l9//YXw8HBpOMrDwwNvv/027t69W2r7gwcPomXLlrC2toafnx9WrVpV5ufr4cOHGDNmDKpXrw4HBwe8/vrrSE5OhkKhwKxZs6T1yhqz1+R78Gjs7dq1g42NDWrXro158+Zh/fr1Os0DKOs9Ap79cyqRk5ODiRMnwsvLC0qlEgEBAVi8eDEe70R92nfw4MGDeOGFFwAAQ4YMkX7mJd/Hx7/rJd/XxYsXY/Xq1dLvmhdeeAF//vlnqRi/++47NGzYENbW1mjUqBG2b9/OeQAmjpV9OezcuRO+vr54+eWXNVp/2LBh2LBhA/r06YOJEyfixIkTiIiIwMWLF7F9+3a1dRMSEtCnTx8MHToUYWFh+PLLLxEeHo4WLVrgueeeQ69eveDs7Izx48dj4MCB6NKlC+zt7bWK//z58+jWrRuaNGmCOXPmQKlUIiEhAb///vtTt9u3bx86d+4MX19fzJo1Cw8fPsSyZcvQqlUrnDp1qtQvin79+sHHxwcRERE4deoU1q5dCzc3N3zyySfPjPHevXvo1q0bBgwYgL59+2LFihUYMGAANm/ejHHjxuHdd9/F//3f/2HRokXo06cPkpKS4ODgAAD4888/cfToUQwYMAC1a9fG9evXsWLFCrRv3x4XLlyAra0t2rZtizFjxiAqKgoffvghAgMDAUD6FwDi4+MxcOBAvPPOOxg+fDgCAgJKxalQKPDll1+iSZMmePfdd/HDDz8AAGbOnInz58/j4MGDsLOze+LrfPjwIdq3b4+EhASMGjUKPj4++O677xAeHo6MjAyMHTsWgYGB2LRpE8aPH4/atWtLXfM1atR46ntYUFCAO3fuACjuxj99+jSWLFmCtm3bwsfHR1ovJiYG165dw5AhQ+Dh4YHz589j9erVOH/+PI4fPy4l8tOnT+O1115DzZo1MXv2bBQVFWHOnDllxhEeHo6tW7fizTffxEsvvYRDhw6ha9euT433Uc/6HgBAcnKy9IfatGnTYGdnh7Vr12rd81Iyh0GlUiE5ORlz586FtbU1+vXrJ62jyc8JKP7j7/XXX8eBAwcwdOhQNGvWDL/++ismT56M5ORkLF26FMCzv4OBgYGYM2cOZsyYgREjRqBNmzYA8MzfOVu2bMH9+/fxzjvvQKFQYOHChejVqxeuXbsm9Qbs3r0b/fv3R+PGjREREYF79+5h6NChqFWrllbvGxkZQVrJzMwUAESPHj00Wv/MmTMCgBg2bJha+6RJkwQAsX//fqnN29tbABCHDx+W2m7duiWUSqWYOHGi1JaYmCgAiEWLFqntMywsTHh7e5eKYebMmeLRH/XSpUsFAHH79u0nxl1yjPXr10ttzZo1E25ubuLu3btS29mzZ4WZmZl46623Sh3v7bffVtvnG2+8IapVq/bEY5Zo166dACC2bNkitV26dEkAEGZmZuL48eNS+6+//loqzgcPHpTa57FjxwQAsXHjRqntu+++EwDEgQMHSq1f8rPYs2dPmc+FhYWpta1atUoAEF999ZU4fvy4MDc3F+PGjXvma42MjJS2K5Gfny+Cg4OFvb29yMrKUjtu165dn7nPR+N/fGnVqpW4c+eO2rplvV9ff/11qc9i9+7dha2trUhOTpbarly5IiwsLNQ+X3FxcQJAqdcfHh4uAIiZM2dKbevXrxcARGJiYqnYn/U9GD16tFAoFOL06dNS2927d4Wrq2upfZal5HP6+OLs7Fzq567pz2nHjh0CgJg3b57a9n369BEKhUIkJCQIITT7Dv7555+lPtslHv+ul3xfq1WrJtLT06X2H3/8UQAQO3fulNoaN24sateuLe7fvy+1HTx4UAAo8/cHmQZ242spKysLAKQq8ll+/vlnAMCECRPU2kuqs8fH9hs2bCj9FQ8UV28BAQG4du1auWN+XMlY/48//giVSqXRNjdv3sSZM2cQHh4OV1dXqb1Jkybo2LGj9Dof9e6776o9btOmDe7evSu9h09jb2+PAQMGSI8DAgLg7OyMwMBABAUFSe0l/3/0/bGxsZH+X1BQgLt378Lf3x/Ozs44deqUBq+2mI+PD0JDQzVad8SIEQgNDcXo0aPx5ptvws/PDwsWLHjmdj///DM8PDwwcOBAqc3S0hJjxoxBdnY2Dh06pHG8jwsKCkJMTAxiYmKwa9cuzJ8/H+fPn8frr7+udrbIo+9Xbm4u7ty5g5deegkApPerqKgI+/btQ8+ePeHp6Smt7+/vj86dO6sdd8+ePQCA999/X6199OjRGseuyfdgz549CA4OVhtXd3V1xaBBgzQ+DgB8//33iImJwd69e7F+/XrUr18fvXv3xtGjR6V1NP05/fzzzzA3N8eYMWPUjjFx4kQIIfDLL78AKN93UBP9+/eHi4uL9LjkPSx531JSUnDu3Dm89dZbaj2C7dq1Q+PGjfUWB1U9TPZacnR0BADcv39fo/Vv3LgBMzMz+Pv7q7V7eHjA2dkZN27cUGuvU6dOqX24uLjg3r175Yy4tP79+6NVq1YYNmwY3N3dMWDAAGzduvWpv3RK4iyrKzswMBB37twpNTb9+Gsp+SWkyWupXbt2qXFgJycneHl5lWp7fJ8PHz7EjBkzpDHT6tWro0aNGsjIyEBmZuYzj13i0a5uTaxbtw4PHjzAlStXEB0drZZEn+TGjRuoV68ezMzUv4olwwmPfz60Ub16dYSEhCAkJARdu3bFhx9+iLVr1+Lo0aNYu3attF56ejrGjh0Ld3d32NjYoEaNGtJrL3m/bt26hYcPH5b6HAMo1VbymX/8/Str2yfR5Htw48YNjeJ5lrZt2yIkJAQdO3ZEeHg4YmNj4eDgoPbHiaY/pxs3bsDT07NUMfD4euX5DmriWd+5kuPr430j48JkryVHR0d4enri77//1mo7TU8retLMZKHBGZJPOkZRUZHaYxsbGxw+fBj79u3Dm2++ib/++gv9+/dHx44dS62rC11ey5O21WSfo0ePxvz589GvXz9s3boVe/fuRUxMDKpVq6bVL1NNkvWjDh48KE26PHfunFbbVpZXX30VAHD48GGprV+/flizZo0052Dv3r1Sda7PqlMbunx2dGVvb4+goCCcOnXqmZMry6uivoOGfN+oamOyL4du3brh6tWrOHbs2DPX9fb2hkqlwpUrV9Ta09LSkJGRIc2s1wcXFxdkZGSUai+rOjQzM8Orr76KJUuW4MKFC5g/fz7279+PAwcOlLnvkjjj4+NLPXfp0iVUr179qRPRKtO2bdsQFhaGTz/9FH369EHHjh3RunXrUu+NPs/rvnnzJkaPHo1OnTqhW7dumDRpkkZVube3N65cuVIqqV66dEl6Xp8KCwsBFF+NDyiu+GJjYzF16lTMnj0bb7zxBjp27AhfX1+17dzc3GBtbY2EhIRS+3y8reQzn5iY+NT1dOXt7a1RPOXx+Puk6c/J29sbKSkppXr+yvp5Pus7WBHXHSg5fkW9b1R1MdmXw5QpU2BnZ4dhw4YhLS2t1PNXr17FZ599BgDo0qULACAyMlJtnSVLlgCAVjOUn8XPzw+ZmZn466+/pLabN2+WmvFf1hXUSsY9Hz8dsETNmjXRrFkzbNiwQS1p/v3339i7d6/0OqsCc3PzUpXMsmXLSlVMJX+clPUHkraGDx8OlUqFdevWYfXq1bCwsMDQoUOfWVF16dIFqamp+Pbbb6W2wsJCLFu2DPb29mjXrp3OsT1q586dAICmTZsC+F8l+Hicj39ezc3NERISgh07diAlJUVqT0hIkMahS5TMc/jiiy/U2pctW6b7C3jsOMeOHVO7wlx6ejo2b96s037T09Nx9OhReHh4wM3NDYDmP6cuXbqgqKgIn3/+udo+ly5dCoVCIc1v0OQ7qM/PZwlPT080atQIGzdulP6QAYpPZ62qvVGkHzz1rhz8/PywZcsW9O/fH4GBgWpX0Dt69Kh0Sg5Q/Es1LCwMq1evRkZGBtq1a4c//vgDGzZsQM+ePdGhQwe9xTVgwAB88MEHeOONNzBmzBg8ePAAK1asQP369dUmps2ZMweHDx9G165d4e3tjVu3buGLL75A7dq10bp16yfuf9GiRejcuTOCg4MxdOhQ6dQ7JycntXOnDa1bt27YtGkTnJyc0LBhQxw7dgz79u1DtWrV1NZr1qwZzM3N8cknnyAzMxNKpRKvvPKK9AteU+vXr8fu3bsRHR2N2rVrAyhObIMHD8aKFStKTVR71IgRI7Bq1SqEh4cjLi4OdevWxbZt2/D7778jMjJS44mgZUlOTsZXX30FoPi8/7Nnz2LVqlWoXr26NB7t6OiItm3bYuHChSgoKECtWrWwd+/eUlU5UHz9hL1796JVq1Z47733pKTWqFEjtYTbokUL9O7dG5GRkbh796506t3ly5cB6K9inTJlCr766it07NgRo0ePlk69q1OnDtLT0zU+zrZt22Bvbw8hBFJSUrBu3Trcu3cPK1eulPah6c+pe/fu6NChAz766CNcv34dTZs2xd69e/Hjjz9i3Lhx8PPzA6DZd9DPzw/Ozs5YuXIlHBwcYGdnh6CgIK3nkjxuwYIF6NGjB1q1aoUhQ4bg3r170s/x0T8AyMQY6jQAU3D58mUxfPhwUbduXWFlZSUcHBxEq1atxLJly0Rubq60XkFBgZg9e7bw8fERlpaWwsvLS0ybNk1tHSGefGpVu3btRLt27aTHTzr1Tggh9u7dKxo1aiSsrKxEQECA+Oqrr0qdehcbGyt69OghPD09hZWVlfD09BQDBw4Uly9fLnWMx0/72bdvn2jVqpWwsbERjo6Oonv37uLChQtq65Qc7/HTiso6zaos7dq1E88991yp9ie9PwDEyJEjpcf37t0TQ4YMEdWrVxf29vYiNDRUXLp0qcxT5tasWSN8fX2Fubm52ml4TzvN7dH9JCUlCScnJ9G9e/dS673xxhvCzs5OXLt27amvNy0tTYrXyspKNG7cuMzTrXQ59c7MzEy4ubmJgQMHSqd/lfj333/FG2+8IZydnYWTk5Po27evSElJKXWanBDFn53mzZsLKysr4efnJ9auXSsmTpworK2t1dbLyckRI0eOFK6ursLe3l707NlTxMfHCwDi448/ltZ70ql3mnwPhBDi9OnTok2bNkKpVIratWuLiIgIERUVJQCI1NTUp75HZZ16Z2dnJ4KDg8XWrVtLra/pz+n+/fti/PjxwtPTU1haWop69eqJRYsWCZVKpfY+Pus7KETxqXMNGzaUTm8sOd6TTr0r63dCWT/Hb775RjRo0EAolUrRqFEj8dNPP4nevXuLBg0aPPU9I+PFa+MTkU569uyJ8+fPl5qX8rgzZ86gefPm+Oqrr7Q+PU4b48aNw6pVq5CdnW3wS/Eak2bNmqFGjRqIiYkxdChUAThmT0Qae/xujleuXMHPP/9c6kZCZd31MTIyEmZmZmjbtm2FxXP37l1s2rQJrVu3ZqJ/goKCAmkCYomDBw/i7NmzWt0QiowLx+yJSGO+vr7SdfRv3LiBFStWwMrKClOmTFFbb+HChYiLi0OHDh1gYWGBX375Bb/88gtGjBhR6loJuggODkb79u0RGBiItLQ0rFu3DllZWZg+fbrejmFqkpOTERISgsGDB8PT0xOXLl3CypUr4eHhUepCWGRCDD2OQETGIzw8XHh7ewulUikcHR1FaGioiIuLK7Xe3r17RatWrYSLi4uwtLQUfn5+YtasWaKgoECv8UybNk3Uq1dP2NjYCFtbW9G6dWsRExOj12OYmoyMDNGvXz9Rq1YtYWVlJVxcXESfPn1KzeUg08IxeyIiIhPHMXsiIiITx2RPRERk4ox6gp5KpUJKSgocHBwq5NKSRERUsYQQuH//Pjw9PUvdaEifcnNzkZ+fr/N+rKysYG1trYeIKpdRJ/uUlBS9zuwlIiLDSEpKkq5AqW+5ubnw8bZH6i3db/Tl4eGBxMREo0v4Rp3sSy5ReeNUXTjac0SCTNMbDZoZOgSiClMoCnBE7NTp0tDPkp+fj9RbRbgRVxeODuXPFVn3VfBucR35+flM9pWppOve0d5Mpx8gUVVmobA0dAhEFUtUzF3+HmfvoIC9Q/mPo4LxDhcbdbInIiLSVJFQoUiHk82LhOrZK1VRTPZERCQLKgioUP5sr8u2hsa+byIiIhPHyp6IiGRBBRV06YjXbWvDYrInIiJZKBICRTpcIV6XbQ2N3fhEREQmjpU9ERHJAifoERERmTgVBIp0WLRN9ocPH0b37t3h6ekJhUKBHTt2qD0vhMCMGTNQs2ZN2NjYICQkBFeuXFFbJz09HYMGDYKjoyOcnZ0xdOhQZGdna/3ameyJiIgqQE5ODpo2bYrly5eX+fzChQsRFRWFlStX4sSJE7Czs0NoaChyc3OldQYNGoTz588jJiYGu3btwuHDhzFixAitY2E3PhERyUJld+N37twZnTt3LvM5IQQiIyPxn//8Bz169AAAbNy4Ee7u7tixYwcGDBiAixcvYs+ePfjzzz/RsmVLAMCyZcvQpUsXLF68GJ6enhrHwsqeiIhkoWQ2vi6LviQmJiI1NRUhISFSm5OTE4KCgnDs2DEAwLFjx+Ds7CwlegAICQmBmZkZTpw4odXxWNkTERFpISsrS+2xUqmEUqnUah+pqakAAHd3d7V2d3d36bnU1FS4ubmpPW9hYQFXV1dpHU2xsiciIllQ6WEBAC8vLzg5OUlLREREpb6O8mBlT0REslAyq16X7QEgKSkJjo6OUru2VT0AeHh4AADS0tJQs2ZNqT0tLQ3NmjWT1rl165badoWFhUhPT5e21xQreyIikoUiofsCAI6OjmpLeZK9j48PPDw8EBsbK7VlZWXhxIkTCA4OBgAEBwcjIyMDcXFx0jr79++HSqVCUFCQVsdjZU9ERFQBsrOzkZCQID1OTEzEmTNn4Orqijp16mDcuHGYN28e6tWrBx8fH0yfPh2enp7o2bMnACAwMBCvvfYahg8fjpUrV6KgoACjRo3CgAEDtJqJDzDZExGRTDw67l7e7bVx8uRJdOjQQXo8YcIEAEBYWBiio6MxZcoU5OTkYMSIEcjIyEDr1q2xZ88eWFtbS9ts3rwZo0aNwquvvgozMzP07t0bUVFRWseuEMJ4r+yflZUFJycn3LvsC0cHjkiQaQqt3cLQIRBVmEJRgIOqH5CZmak2Dq5PJbni1AV32OuQK7Lvq/B8w7QKjbWiMEMSERGZOHbjExGRLKhE8aLL9saKyZ6IiGShCAoUQaHT9saK3fhEREQmjpU9ERHJgpwreyZ7IiKSBZVQQCXKn7B12dbQ2I1PRERk4ljZExGRLLAbn4iIyMQVwQxFOnRoF+kxlsrGZE9ERLIgdByzFxyzJyIioqqKlT0REckCx+yJiIhMXJEwQ5HQYczeiC+Xy258IiIiE8fKnoiIZEEFBVQ61LgqGG9pz2RPRESyIOcxe3bjExERmThW9kREJAu6T9BjNz4REVGVVjxmr8ONcNiNT0RERFUVK3siIpIFlY7XxudsfCIioiqOY/ZEREQmTgUz2Z5nzzF7IiIiE8fKnoiIZKFIKFCkw21qddnW0JjsiYhIFop0nKBXxG58IiIiqqpY2RMRkSyohBlUOszGV3E2PhERUdXGbnwiIiIyWazsiYhIFlTQbUa9Sn+hVDomeyIikgXdL6pjvJ3hxhs5ERERaYSVPRERyYLu18Y33vqYyZ6IiGRBzvezZ7InIiJZkHNlb7yRExERkUZY2RMRkSzoflEd462PmeyJiEgWVEIBlS7n2RvxXe+M988UIiIi0ggreyIikgWVjt34xnxRHSZ7IiKSBd3veme8yd54IyciIiKNsLInIiJZKIICRTpcGEeXbQ2NyZ6IiGSB3fhERERksljZExGRLBRBt674Iv2FUumY7ImISBbk3I3PZE9ERLLAG+EQERGRyWJlT0REsiB0vJ+94Kl3REREVRu78YmIiMhksbInIiJZkPMtbpnsiYhIFop0vOudLtsamvFGTkRERBphZU9ERLLAbnwiIiITp4IZVDp0aOuyraEZb+RERESkEVb2REQkC0VCgSIduuJ12dbQmOyJiEgWOGZPRERk4oSOd70TvIIeERERPaqoqAjTp0+Hj48PbGxs4Ofnh7lz50IIIa0jhMCMGTNQs2ZN2NjYICQkBFeuXNF7LEz2REQkC0VQ6Lxo45NPPsGKFSvw+eef4+LFi/jkk0+wcOFCLFu2TFpn4cKFiIqKwsqVK3HixAnY2dkhNDQUubm5en3t7MYnIiJZUAndxt1V4tnrPOro0aPo0aMHunbtCgCoW7cuvv76a/zxxx8Aiqv6yMhI/Oc//0GPHj0AABs3boS7uzt27NiBAQMGlDvWx7GyJyIi0kJWVpbakpeXV+Z6L7/8MmJjY3H58mUAwNmzZ3HkyBF07twZAJCYmIjU1FSEhIRI2zg5OSEoKAjHjh3Ta8ys7Annjtvhuy/ccOWcLdLTLDFzXSJe7pwpPS8EsHGRB/ZsqYbsLHM0bJmDMR8noZZvfql95ecpMLZrfVy7YIMv9sbDr9HDynwpROXS7c3b6PrWbbjXLv5M37hsg82RHjh5wMnAkZE+qXScoFeyrZeXl1r7zJkzMWvWrFLrT506FVlZWWjQoAHMzc1RVFSE+fPnY9CgQQCA1NRUAIC7u7vadu7u7tJz+lIlKvvly5ejbt26sLa2RlBQkNTFQZUj94EZfJ97iFEL/i3z+a3L3fDjlzUw+uMkfLbrMqxtVfjw//yQn1u6O2zdPE9U8yio6JCJ9Or2TUt8GVELo7o0wOguDXD2d3vMWncN3vX5x6opUUGh8wIASUlJyMzMlJZp06aVebytW7di8+bN2LJlC06dOoUNGzZg8eLF2LBhQ2W+bABVINl/++23mDBhAmbOnIlTp06hadOmCA0Nxa1btwwdmmy88Mp9hH+QilaPVPMlhAB2rK2BgWNT8fJrWfBtmIspUTdwN80SR/eoVz1/7ndA3CEHDJ+RXFmhE+nFiX3O+HO/E1ISrZGcaI3ohbWQ+8AMDZ7PMXRoVAU5OjqqLUqlssz1Jk+ejKlTp2LAgAFo3Lgx3nzzTYwfPx4REREAAA8PDwBAWlqa2nZpaWnSc/pi8GS/ZMkSDB8+HEOGDEHDhg2xcuVK2Nra4ssvvzR0aAQg9R8rpN+yxPNtsqU2O0cVGjR/gItxdlLbvdsWiJzshSnLbkBpo+UsFqIqxMxMoN3r6VDaqNQ+42T8Sq6gp8uijQcPHsDMTD3NmpubQ6VSAQB8fHzg4eGB2NhY6fmsrCycOHECwcHBur/gRxh0zD4/Px9xcXFqXSBmZmYICQnR++QEKp/0W8UfEeca6l3zzjUKpOeEABaPq4Oub95F/aYPkZpkVelxEumqboOHiPwxHlZKFR7mmGPOcF/8c8XG0GGRHulrzF5T3bt3x/z581GnTh0899xzOH36NJYsWYK3334bAKBQKDBu3DjMmzcP9erVg4+PD6ZPnw5PT0/07Nmz3HGWxaDJ/s6dOygqKipzcsKlS5dKrZ+Xl6c26zErK6vCY6Rn+3FddTzMNkP/0WnPXpmoivr3qhLvhzaArYMKbbrew6SlNzC5Tz0mfCq3ZcuWYfr06Xj//fdx69YteHp64p133sGMGTOkdaZMmYKcnByMGDECGRkZaN26Nfbs2QNra2u9xmJUs/EjIiIwe/ZsQ4chK65uhQCAjNuWqOZeKLVn3LaE33PFk5fO/O6Ai3F26Fa3qdq2ozrXxyu97mHyZ/9UXsBE5VRYYIaU68W/YBPO2SKg6QP0HHobUVPrGDgy0hcVdLw2vpYX1XFwcEBkZCQiIyOfuI5CocCcOXMwZ86ccselCYMm++rVq8Pc3FzjyQnTpk3DhAkTpMdZWVmlToEg/fKokw9XtwKcPmIvnUaXc98Ml07bottbdwAA78/9F+EfmEvb3E21xIf/54cPV15Hg+YPDBI3ka4UZgKWVipDh0F6JB6ZUV/e7Y2VQZO9lZUVWrRogdjYWGl8QqVSITY2FqNGjSq1vlKpfOKsRyq/hzlmSEn83/uammSFq3/bwMG5EG61C9Bz2G18/Zk7avnkwaNOPjYsrIlq7gV4+bXi2ftutQsA/G9M39qu+Bekp3c+anjyNDyq+oZMTcafBxxxO9kKNvYqdOiZjibB2fhokL+hQyM94l3vDGjChAkICwtDy5Yt8eKLLyIyMhI5OTkYMmSIoUOTjctnbTGlz/9+qa2aVQsA0LFfOiZF/oN+I28h94EZPpvihewsczz3Qg7mb74GK2vOuifT4Fy9EJMjb8DVrQAP7psj8aINPhrkj1O/ORo6NCK9MHiy79+/P27fvo0ZM2YgNTUVzZo1w549e0pN2qOK0/TlbPyacuaJzysUQNiUVIRN0eyKTh5e+U/dH1FVs3SSt6FDoEpQ2bPxqxKDJ3sAGDVqVJnd9kRERPoi52584/0zhYiIiDRSJSp7IiKiiqbScTa+LtsaGpM9ERHJArvxiYiIyGSxsiciIlmQc2XPZE9ERLIg52TPbnwiIiITx8qeiIhkQc6VPZM9ERHJgoBup88Z8wXCmeyJiEgW5FzZc8yeiIjIxLGyJyIiWZBzZc9kT0REsiDnZM9ufCIiIhPHyp6IiGRBzpU9kz0REcmCEAoIHRK2LtsaGrvxiYiITBwreyIikgXez56IiMjEyXnMnt34REREJo6VPRERyYKcJ+gx2RMRkSzIuRufyZ6IiGRBzpU9x+yJiIhMHCt7IiKSBaFjN74xV/ZM9kREJAsCgBC6bW+s2I1PRERk4ljZExGRLKiggIJX0CMiIjJdnI1PREREJouVPRERyYJKKKDgRXWIiIhMlxA6zsY34un47MYnIiIycazsiYhIFuQ8QY/JnoiIZIHJnoiIyMTJeYIex+yJiIhMHCt7IiKSBTnPxmeyJyIiWShO9rqM2esxmErGbnwiIiITx8qeiIhkgbPxiYiITJyAbvekN+JefHbjExERmTpW9kREJAvsxiciIjJ1Mu7HZ7InIiJ50LGyhxFX9hyzJyIiMnGs7ImISBZ4BT0iIiITJ+cJeuzGJyIiMnGs7ImISB6EQrdJdkZc2TPZExGRLMh5zJ7d+ERERCaOlT0REckDL6rzdD/99JPGO3z99dfLHQwREVFFkfNsfI2Sfc+ePTXamUKhQFFRkS7xEBERkZ5pNGavUqk0WpjoiYioShM6LOWQnJyMwYMHo1q1arCxsUHjxo1x8uTJ/4UjBGbMmIGaNWvCxsYGISEhuHLlSrlf3pPoNEEvNzdXX3EQERFVqJJufF0Wbdy7dw+tWrWCpaUlfvnlF1y4cAGffvopXFxcpHUWLlyIqKgorFy5EidOnICdnR1CQ0P1nl+1TvZFRUWYO3cuatWqBXt7e1y7dg0AMH36dKxbt06vwREREemNLlV9Oar7Tz75BF5eXli/fj1efPFF+Pj4oFOnTvDz8ysORwhERkbiP//5D3r06IEmTZpg48aNSElJwY4dO3R/vY/QOtnPnz8f0dHRWLhwIaysrKT2Ro0aYe3atXoNjoiIyFj99NNPaNmyJfr27Qs3Nzc0b94ca9askZ5PTExEamoqQkJCpDYnJycEBQXh2LFjeo1F62S/ceNGrF69GoMGDYK5ubnU3rRpU1y6dEmvwREREemPQg8LkJWVpbbk5eWVebRr165hxYoVqFevHn799Ve89957GDNmDDZs2AAASE1NBQC4u7urbefu7i49py9aJ/vk5GT4+/uXalepVCgoKNBLUERERHqnp258Ly8vODk5SUtERESZh1OpVHj++eexYMECNG/eHCNGjMDw4cOxcuXKCnyRZdP6ojoNGzbEb7/9Bm9vb7X2bdu2oXnz5noLjIiIqCpKSkqCo6Oj9FipVJa5Xs2aNdGwYUO1tsDAQHz//fcAAA8PDwBAWloaatasKa2TlpaGZs2a6TVmrZP9jBkzEBYWhuTkZKhUKvzwww+Ij4/Hxo0bsWvXLr0GR0REpDd6uoKeo6OjWrJ/klatWiE+Pl6t7fLly1Kx7OPjAw8PD8TGxkrJPSsrCydOnMB7772nQ6Clad2N36NHD+zcuRP79u2DnZ0dZsyYgYsXL2Lnzp3o2LGjXoMjIiLSm5K73umyaGH8+PE4fvw4FixYgISEBGzZsgWrV6/GyJEjARRfiG7cuHGYN28efvrpJ5w7dw5vvfUWPD09Nb6YnabKdW38Nm3aICYmRq+BEBERmZIXXngB27dvx7Rp0zBnzhz4+PggMjISgwYNktaZMmUKcnJyMGLECGRkZKB169bYs2cPrK2t9RpLuW+Ec/LkSVy8eBFA8Th+ixYt9BYUERGRvhniFrfdunVDt27dnvi8QqHAnDlzMGfOnPIHpgGtk/2///6LgQMH4vfff4ezszMAICMjAy+//DK++eYb1K5dW98xEhER6U7Gd73Tesx+2LBhKCgowMWLF5Geno709HRcvHgRKpUKw4YNq4gYiYiISAdaV/aHDh3C0aNHERAQILUFBARg2bJlaNOmjV6DIyIi0ptyTLIrtb2R0jrZe3l5lXnxnKKiInh6euolKCIiIn1TiOJFl+2Nldbd+IsWLcLo0aPVbtF38uRJjB07FosXL9ZrcERERHpTyTfCqUo0quxdXFygUPyv+yInJwdBQUGwsCjevLCwEBYWFnj77bf1fm4gERER6UajZB8ZGVnBYRAREVUwjtk/XVhYWEXHQUREVLFkfOpduS+qAwC5ubnIz89Xa9PkesFERERUebSeoJeTk4NRo0bBzc0NdnZ2cHFxUVuIiIiqJBlP0NM62U+ZMgX79+/HihUroFQqsXbtWsyePRuenp7YuHFjRcRIRESkOxkne6278Xfu3ImNGzeiffv2GDJkCNq0aQN/f394e3tj8+bNahf4JyIiIsPTurJPT0+Hr68vgOLx+fT0dABA69atcfjwYf1GR0REpC+VfIvbqkTrZO/r64vExEQAQIMGDbB161YAxRV/yY1xiIiIqpqSK+jpshgrrZP9kCFDcPbsWQDA1KlTsXz5clhbW2P8+PGYPHmy3gMkIiIi3Wg9Zj9+/Hjp/yEhIbh06RLi4uLg7++PJk2a6DU4IiIiveF59uXn7e0Nb29vfcRCREREFUCjZB8VFaXxDseMGVPuYIiIiCqKAjre9U5vkVQ+jZL90qVLNdqZQqFgsiciIqpiNEr2JbPvq6o36jeGhcLS0GEQVYiErzgXhkyX6kEuMPyHyjkYb4RDRERk4mQ8QU/rU++IiIjIuLCyJyIieZBxZc9kT0REsqDrVfBkdQU9IiIiMi7lSva//fYbBg8ejODgYCQnJwMANm3ahCNHjug1OCIiIr2R8S1utU7233//PUJDQ2FjY4PTp08jLy8PAJCZmYkFCxboPUAiIiK9YLLX3Lx587By5UqsWbMGlpb/O7e9VatWOHXqlF6DIyIiIt1pPUEvPj4ebdu2LdXu5OSEjIwMfcRERESkd5ygpwUPDw8kJCSUaj9y5Ah8fX31EhQREZHelVxBT5fFSGmd7IcPH46xY8fixIkTUCgUSElJwebNmzFp0iS89957FREjERGR7mQ8Zq91N/7UqVOhUqnw6quv4sGDB2jbti2USiUmTZqE0aNHV0SMREREpAOtk71CocBHH32EyZMnIyEhAdnZ2WjYsCHs7e0rIj4iIiK9kPOYfbmvoGdlZYWGDRvqMxYiIqKKw8vlaq5Dhw5QKJ48SWH//v06BURERET6pXWyb9asmdrjgoICnDlzBn///TfCwsL0FRcREZF+6diNL6vKfunSpWW2z5o1C9nZ2ToHREREVCFk3I2vtxvhDB48GF9++aW+dkdERER6ordb3B47dgzW1tb62h0REZF+ybiy1zrZ9+rVS+2xEAI3b97EyZMnMX36dL0FRkREpE889U4LTk5Oao/NzMwQEBCAOXPmoFOnTnoLjIiIiPRDq2RfVFSEIUOGoHHjxnBxcamomIiIiEiPtJqgZ25ujk6dOvHudkREZHxkfG18rWfjN2rUCNeuXauIWIiIiCpMyZi9Loux0jrZz5s3D5MmTcKuXbtw8+ZNZGVlqS1ERERUtWg8Zj9nzhxMnDgRXbp0AQC8/vrrapfNFUJAoVCgqKhI/1ESERHpgxFX57rQONnPnj0b7777Lg4cOFCR8RAREVUMnmf/bEIUv8p27dpVWDBERESkf1qdeve0u90RERFVZbyojobq16//zISfnp6uU0BEREQVgt34mpk9e3apK+gRERFR1aZVsh8wYADc3NwqKhYiIqIKw258DXC8noiIjJqMu/E1vqhOyWx8IiIiMi4aV/Yqlaoi4yAiIqpYMq7stb7FLRERkTHimD0REZGpk3Flr/WNcIiIiMi4sLInIiJ5kHFlz2RPRESyIOcxe3bjExERmThW9kREJA8y7sZnZU9ERLJQ0o2vy1JeH3/8MRQKBcaNGye15ebmYuTIkahWrRrs7e3Ru3dvpKWl6f5Cy8BkT0REVIH+/PNPrFq1Ck2aNFFrHz9+PHbu3InvvvsOhw4dQkpKCnr16lUhMTDZExGRPAg9LFrKzs7GoEGDsGbNGri4uEjtmZmZWLduHZYsWYJXXnkFLVq0wPr163H06FEcP35chxdZNiZ7IiKSBz0l+6ysLLUlLy/viYccOXIkunbtipCQELX2uLg4FBQUqLU3aNAAderUwbFjx/Tych/FZE9ERKQFLy8vODk5SUtERESZ633zzTc4depUmc+npqbCysoKzs7Oau3u7u5ITU3Ve8ycjU9ERLKg+O+iy/YAkJSUBEdHR6ldqVSWWjcpKQljx45FTEwMrK2tdTiqfrCyJyIiedBTN76jo6PaUlayj4uLw61bt/D888/DwsICFhYWOHToEKKiomBhYQF3d3fk5+cjIyNDbbu0tDR4eHjo/aWzsiciIlmozCvovfrqqzh37pxa25AhQ9CgQQN88MEH8PLygqWlJWJjY9G7d28AQHx8PP755x8EBweXP8gnYLInIiLSMwcHBzRq1Eitzc7ODtWqVZPahw4digkTJsDV1RWOjo4YPXo0goOD8dJLL+k9HiZ7IiKShyp2Bb2lS5fCzMwMvXv3Rl5eHkJDQ/HFF1/o9yD/xWRPRETyYcBL3h48eFDtsbW1NZYvX47ly5dX+LE5QY+IiMjEsbInIiJZkPMtbpnsiYhIHqrYmH1lYjc+ERGRiWNlT0REssBufCIiIlPHbnwiIiIyVazsiYhIFtiNT0REZOpk3I3PZE9ERPIg42TPMXsiIiITx8qeiIhkgWP2REREpo7d+ERERGSqWNkTEZEsKISAQpS/PNdlW0NjsiciInlgNz4RERGZKlb2REQkC5yNT0REZOrYjU9ERESmipU9ERHJArvxiYiITJ2Mu/GZ7ImISBbkXNlzzJ6IiMjEsbInIiJ5YDc+ERGR6TPmrnhdsBufiIjIxLGyJyIieRCieNFleyPFZE9ERLLA2fhERERksljZExGRPHA2PhERkWlTqIoXXbY3VuzGJyIiMnGs7OmZ+o9KQ6sumfDyz0N+rhkunLTFuvk18e9Va0OHRlRu5un5qP5NCmz/yoIiT4UCdyVujfBGnq8tUChQbVsKbM9kwfJ2PlQ2ZnjQyAF3+9dCkYuloUOn8pJxN75BK/vDhw+je/fu8PT0hEKhwI4dOwwZDj1Bk+Ac7IyujnHd6mHaAF+YWwgs+PoalDZFhg6NqFzMcgpRe84VCHMFUib74Z9PAnFnUC0U2ZkXP5+vgvL6Q9zr6YGkuQG4Oc4XVjfzUHPJVQNHTroomY2vy2KsDFrZ5+TkoGnTpnj77bfRq1cvQ4ZCT/HRIF+1x5+Oq4Otf59HvSYP8fcJewNFRVR+LjvTUOhqiVvveEtthW5K6f8qW3OkTPVX2+b2W7XhNfMyLO7ko7C6VaXFSnrE8+wNo3PnzujcubMhQ6BysHMsrujvZ5gbOBKi8rE7lYUHTRzgEZUI60vZKHKxRGZIdWR1qP7EbcweFkEogCJbfu7J+BjVmH1eXh7y8vKkx1lZWQaMRp4UCoF3Zyfj7z9scSPextDhEJWLxe08OMbmIeM1N6S/7g7raw9QfeO/EOYK3G9brdT6inwVqn2TguxgFwgme6PFi+oYiYiICDg5OUmLl5eXoUOSnVELkuHdIBcR73k/e2WiKkqhAvLq2iK9vyfy69oi65Xiqt5p/53SKxcKeCxLBARwK5y/c4ya0MNipIwq2U+bNg2ZmZnSkpSUZOiQZGXk/H8R1DELU/r44c5NjlmS8Sp0tkC+p/rZJPmeSljcLXhsxeJEb3E3HylT/VnVk9Eyqm58pVIJpVL57BVJzwRGzk/Gy69lYnIff6Ql8WdAxi23vj2sbuaqtVml5qHg0Yl3/030lml5SP7QHyoHo/p1SWVgNz7RU4xakIxXet3DxyO98TDbDC41CuBSowBW1kZ8OSmStYzXasD6ag5cfkyFZWoe7I+mw/HAXWSG/HeCXqGAR1QilIkPkPaeNxQqwDyjAOYZBUAhP/dGq2Q2vi6LkTLon6rZ2dlISEiQHicmJuLMmTNwdXVFnTp1DBgZPap7+F0AwOIf1M8xXjzOCzFbXQ0REpFO8vzscHOcL6p9mwKXHakorGGFO4NrIbtV8efZ4l4+7E9lAgDqfBSvtm3yh/542NCh0mMm0oVBk/3JkyfRoUMH6fGECRMAAGFhYYiOjjZQVPS4UM+mhg6BSO8eNHfCg+ZOZT5XWEOJhK+aV3JEVNHk3I1v0GTfvn17CCPuFiEiIiPCy+USERGRqeL0UiIikgV24xMREZk6lShedNneSDHZExGRPHDMnoiIiEwVK3siIpIFBXQcs9dbJJWPyZ6IiORBxvezZzc+ERGRiWNlT0REssBT74iIiEwdZ+MTERGRqWJlT0REsqAQAgodJtnpsq2hMdkTEZE8qP676LK9kWI3PhERkYljZU9ERLLAbnwiIiJTx9n4REREJq7kCnq6LFqIiIjACy+8AAcHB7i5uaFnz56Ij49XWyc3NxcjR45EtWrVYG9vj969eyMtLU2frxoAkz0REVGFOHToEEaOHInjx48jJiYGBQUF6NSpE3JycqR1xo8fj507d+K7777DoUOHkJKSgl69euk9FnbjExGRLFT2FfT27Nmj9jg6Ohpubm6Ii4tD27ZtkZmZiXXr1mHLli145ZVXAADr169HYGAgjh8/jpdeeqn8wT6GlT0REcmDnrrxs7Ky1Ja8vDyNDp+ZmQkAcHV1BQDExcWhoKAAISEh0joNGjRAnTp1cOzYMb2+dCZ7IiIiLXh5ecHJyUlaIiIinrmNSqXCuHHj0KpVKzRq1AgAkJqaCisrKzg7O6ut6+7ujtTUVL3GzG58IiKSBYWqeNFlewBISkqCo6Oj1K5UKp+57ciRI/H333/jyJEj5Q9AB0z2REQkD3q6n72jo6Nasn+WUaNGYdeuXTh8+DBq164ttXt4eCA/Px8ZGRlq1X1aWho8PDzKH2cZ2I1PRERUAYQQGDVqFLZv3479+/fDx8dH7fkWLVrA0tISsbGxUlt8fDz++ecfBAcH6zUWVvZERCQPlXxRnZEjR2LLli348ccf4eDgII3DOzk5wcbGBk5OThg6dCgmTJgAV1dXODo6YvTo0QgODtbrTHyAyZ6IiGSisi+Xu2LFCgBA+/bt1drXr1+P8PBwAMDSpUthZmaG3r17Iy8vD6Ghofjiiy/KHeOTMNkTERFVAKHBHwfW1tZYvnw5li9fXqGxMNkTEZE86GmCnjFisiciInkQ0O2e9Mab65nsiYhIHuR8i1ueekdERGTiWNkTEZE8COg4Zq+3SCodkz0REcmDjCfosRufiIjIxLGyJyIieVABUOi4vZFisiciIlngbHwiIiIyWazsiYhIHmQ8QY/JnoiI5EHGyZ7d+ERERCaOlT0REcmDjCt7JnsiIpIHnnpHRERk2njqHREREZksVvZERCQPHLMnIiIycSoBKHRI2CrjTfbsxiciIjJxrOyJiEge2I1PRERk6nRM9jDeZM9ufCIiIhPHyp6IiOSB3fhEREQmTiWgU1c8Z+MTERFRVcXKnoiI5EGoihddtjdSTPZERCQPHLMnIiIycRyzJyIiIlPFyp6IiOSB3fhEREQmTkDHZK+3SCodu/GJiIhMHCt7IiKSB3bjExERmTiVCoAO58qrjPc8e3bjExERmThW9kREJA/sxiciIjJxMk727MYnIiIycazsiYhIHmR8uVwmeyIikgUhVBA63LlOl20NjcmeiIjkQQjdqnOO2RMREVFVxcqeiIjkQeg4Zm/ElT2TPRERyYNKBSh0GHc34jF7duMTERGZOFb2REQkD+zGJyIiMm1CpYLQoRvfmE+9Yzc+ERGRiWNlT0RE8sBufCIiIhOnEoBCnsme3fhEREQmjpU9ERHJgxAAdDnP3ngreyZ7IiKSBaESEDp04wsmeyIioipOqKBbZc9T74iIiKiKYmVPRESywG58IiIiUyfjbnyjTvYlf2UVokCn6yQQVWWqB7mGDoGowqge5gGonKpZ11xRiAL9BVPJFMKI+yX+/fdfeHl5GToMIiLSUVJSEmrXrl0h+87NzYWPjw9SU1N13peHhwcSExNhbW2th8gqj1Ene5VKhZSUFDg4OEChUBg6HFnIysqCl5cXkpKS4OjoaOhwiPSKn+/KJ4TA/fv34enpCTOzipsznpubi/z8fJ33Y2VlZXSJHjDybnwzM7MK+0uQns7R0ZG/DMlk8fNduZycnCr8GNbW1kaZpPWFp94RERGZOCZ7IiIiE8dkT1pRKpWYOXMmlEqloUMh0jt+vslUGfUEPSIiIno2VvZEREQmjsmeiIjIxDHZExERmTgmeyIiIhPHZE8aW758OerWrQtra2sEBQXhjz/+MHRIRHpx+PBhdO/eHZ6enlAoFNixY4ehQyLSKyZ70si3336LCRMmYObMmTh16hSaNm2K0NBQ3Lp1y9ChEeksJycHTZs2xfLlyw0dClGF4Kl3pJGgoCC88MIL+PzzzwEU35fAy8sLo0ePxtSpUw0cHZH+KBQKbN++HT179jR0KER6w8qenik/Px9xcXEICQmR2szMzBASEoJjx44ZMDIiItIEkz090507d1BUVAR3d3e1dnd3d73cMpKIiCoWkz0REZGJY7KnZ6pevTrMzc2Rlpam1p6WlgYPDw8DRUVERJpisqdnsrKyQosWLRAbGyu1qVQqxMbGIjg42ICRERGRJiwMHQAZhwkTJiAsLAwtW7bEiy++iMjISOTk5GDIkCGGDo1IZ9nZ2UhISJAeJyYm4syZM3B1dUWdOnUMGBmRfvDUO9LY559/jkWLFiE1NRXNmjVDVFQUgoKCDB0Wkc4OHjyIDh06lGoPCwtDdHR05QdEpGdM9kRERCaOY/ZEREQmjsmeiIjIxDHZExERmTgmeyIiIhPHZE9ERGTimOyJiIhMHJM9ERGRiWOyJ9JReHi42r3P27dvj3HjxlV6HAcPHoRCoUBGRsYT11EoFNixY4fG+5w1axaaNWumU1zXr1+HQqHAmTNndNoPEZUfkz2ZpPDwcCgUCigUClhZWcHf3x9z5sxBYWFhhR/7hx9+wNy5czVaV5METUSkK14bn0zWa6+9hvXr1yMvLw8///wzRo4cCUtLS0ybNq3Uuvn5+bCystLLcV1dXfWyHyIifWFlTyZLqVTCw8MD3t7eeO+99xASEoKffvoJwP+63ufPnw9PT08EBAQAAJKSktCvXz84OzvD1dUVPXr0wPXr16V9FhUVYcKECXB2dka1atUwZcoUPH7F6ce78fPy8vDBBx/Ay8sLSqUS/v7+WLduHa5fvy5dj93FxQUKhQLh4eEAiu8qGBERAR8fH9jY2KBp06bYtm2b2nF+/vln1K9fHzY2NujQoYNanJr64IMPUL9+fdja2sLX1xfTp09HQUFBqfVWrVoFLy8v2Nraol+/fsjMzFR7fu3atQgMDIS1tTUaNGiAL774QutYiKjiMNmTbNjY2CA/P196HBsbi/j4eMTExGDXrl0oKChAaGgoHBwc8Ntvv+H333+Hvb09XnvtNWm7Tz/9FNHR0fjyyy9x5MgRpKenY/v27U897ltvvYWvv/4aUVFRuHjxIlatWgV7e3t4eXnh+++/BwDEx8fj5s2b+OyzzwAAERER2LhxI1auXInz589j/PjxGDx4MA4dOgSg+I+SXr16oXv37jhz5gyGDRuGqVOnav2eODg4IDo6GhcuXMBnn32GNWvWYOnSpWrrJCQkYOvWrdi5cyf27NmD06dP4/3335ee37x5M2bMmIH58+fj4sWLWLBgAaZPn44NGzZoHQ8RVRBBZILCwsJEjx49hBBCqFQqERMTI5RKpZg0aZL0vLu7u8jLy5O22bRpkwgICBAqlUpqy8vLEzY2NuLXX38VQghRs2ZNsXDhQun5goICUbt2belYQgjRrl07MXbsWCGEEPHx8QKAiImJKTPOAwcOCADi3r17Ultubq6wtbUVR48eVVt36NChYuDAgUIIIaZNmyYaNmyo9vwHH3xQal+PAyC2b9/+xOcXLVokWrRoIT2eOXOmMDc3F//++6/U9ssvvwgzMzNx8+ZNIYQQfn5+YsuWLWr7mTt3rggODhZCCJGYmCgAiNOnTz/xuERUsThmTyZr165dsLe3R0FBAVQqFf7v//4Ps2bNkp5v3Lix2jj92bNnkZCQAAcHB7X95Obm4urVq8jMzMTNmzfVbutrYWGBli1blurKL3HmzBmYm5ujXbt2GsedkJCABw8eoGPHjmrt+fn5aN68OQDg4sWLpW4vHBwcrPExSnz77beIiorC1atXkZ2djcLCQjg6OqqtU6dOHdSqVUvtOCqVCvHx8XBwcMDVq1cxdOhQDB8+XFqnsLAQTk5OWsdDRBWDyZ5MVocOHbBixQpYWVnB09MTFhbqH3c7Ozu1x9nZ2WjRogU2b95cal81atQoVww2NjZab5OdnQ0A2L17t1qSBYrnIejLsWPHMGjQIMyePRuhoaFwcnLCN998g08//VTrWNesWVPqjw9zc3O9xUpEumGyJ5NlZ2cHf39/jdd//vnn8e2338LNza1UdVuiZs2aOHHiBNq2bQuguIKNi4vD888/X+b6jRs3hkqlwqFDhxASElLq+ZKehaKiIqmtYcOGUCqV+Oeff57YIxAYGChNNixx/PjxZ7/IRxw9ehTe3t746KOPpLYbN26UWu+ff/5BSkoKPD09peOYmZkhICAA7u7u8PT0xLVr1zBo0CCtjk9ElYcT9Ij+a9CgQahevTp69OiB3377DYmJiTh48CDGjBmDf//9FwAwduxYfPzxx9ixYwcuXbqE999//6nnyNetWxdhYWF4++23sWPHDmmfW7duBQB4e3tDoVBg165duH37NrKzs+Hg4IBJkyZh/Pjx2LBhA65evYpTp05h2bJl0qS3d999F1euXMHkyZMRHx+PLVu2IDo6WqvXW69ePfzzzz/45ptvcPXqVURFRZU52dDa2hphYWE4e/YsfvvtN4wZMwb9+vWDh4cHAGD27NmIiIhAVFQULl++jHPnzmH9+vVYsmSJVvEQUcVhsif6L1tbWxw+fBh16tRBr169EBgYiKFDhyI3N1eq9CdOnIg333wTYWFhCA4OhoODA954442n7nfFihXo06cP3n//fTRo0ADDhw9HTk4OAKBWrVqYPXs2pk6dCnd3d4waNQoAMHfuXEyfPh0REREIDAzEa6+9ht27d8PHxwdA8Tj6999/jx07dqBp06ZYuXIlFixYoNXrff311zF+/HiMGjUKzZo1w9GjRzF9+vRS6/n7+6NXr17o0qULOnXqhCZNmqidWjds2DCsXbsW69evR+PGjdGuXTtER0dLsRKR4SnEk2YWERERkUlgZU9ERGTimOyJiIhMHJM9ERGRiWOyJyIiMnFM9kRERCaOyZ6IiMjEMdkTERGZOCZ7IiIiE8dkT0REZOKY7ImIiEwckz0REZGJY7InIiIycf8PFOcr16z0PS8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Stacking**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.ensemble  import AdaBoostClassifier\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.ensemble  import GradientBoostingClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.svm import SVC \r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\r\n",
    "from sklearn.linear_model import Perceptron\r\n",
    "from mlxtend.classifier import StackingClassifier\r\n",
    "from sklearn.ensemble import VotingClassifier\r\n",
    "%matplotlib inline\r\n",
    "from sklearn import model_selection\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Feature Scaling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "min_train = X_train.min()\r\n",
    "range_train = (X_train - min_train).max()\r\n",
    "X_train_scaled = (X_train - min_train)/range_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "min_test = X_test.min()\r\n",
    "range_test = (X_test - min_test).max()\r\n",
    "X_test_scaled = (X_test - min_test)/range_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "logi = LogisticRegression()\r\n",
    "logi.fit(X_train_scaled, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "y_predict = logi.predict(X_test_scaled)\r\n",
    "from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "results = pd.DataFrame([['Logistic Regression', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.95082</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision   Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912        1.0  0.90625   0.95082  0.953125"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from xgboost import XGBClassifier\r\n",
    "xgb_classifier = XGBClassifier()\r\n",
    "xgb_classifier.fit(X_train_scaled, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:39:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "y_predict = xgb_classifier.predict(X_test_scaled)\r\n",
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "model_results = pd.DataFrame([['XGBOOST', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.95082</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.95935</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912        1.0  0.906250   0.95082  0.953125\n",
       "1              XGBOOST  0.970760        1.0  0.921875   0.95935  0.960938"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "random_forest = RandomForestClassifier()\r\n",
    "random_forest.fit(X_train_scaled, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "y_predict = random_forest.predict(X_test_scaled)\r\n",
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "model_results = pd.DataFrame([['Random Forest', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "sgd = SGDClassifier(max_iter=1000)\r\n",
    "\r\n",
    "sgd.fit(X_train_scaled, y_train)\r\n",
    "y_predict = sgd.predict(X_test_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "model_results = pd.DataFrame([['SGD', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "adaboost =AdaBoostClassifier()\r\n",
    "adaboost.fit(X_train_scaled, y_train)\r\n",
    "y_predict = adaboost.predict(X_test_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "model_results = pd.DataFrame([['Adaboost', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "4             Adaboost  0.970760   0.983607  0.937500  0.960000  0.964077"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "gboost =GradientBoostingClassifier()\r\n",
    "gboost.fit(X_train_scaled, y_train)\r\n",
    "y_predict = gboost.predict(X_test_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "model_results = pd.DataFrame([['Gboost', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gboost</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "4             Adaboost  0.970760   0.983607  0.937500  0.960000  0.964077\n",
       "5               Gboost  0.976608   1.000000  0.937500  0.967742  0.968750"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\r\n",
    "knn.fit(X_train_scaled, y_train)\r\n",
    "y_predict = knn.predict(X_test_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "roc=roc_auc_score(y_test, y_predict)\r\n",
    "acc = accuracy_score(y_test, y_predict)\r\n",
    "prec = precision_score(y_test, y_predict)\r\n",
    "rec = recall_score(y_test, y_predict)\r\n",
    "f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "model_results = pd.DataFrame([['KNN7', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gboost</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN7</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.951592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "4             Adaboost  0.970760   0.983607  0.937500  0.960000  0.964077\n",
       "5               Gboost  0.976608   1.000000  0.937500  0.967742  0.968750\n",
       "6                 KNN7  0.959064   0.967213  0.921875  0.944000  0.951592"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# from sklearn.svm import SVC \r\n",
    "\r\n",
    "\r\n",
    "# svc_model = SVC(kernel='linear')\r\n",
    "# svc_model.fit(X_train, y_train)\r\n",
    "# y_predict = svc_model.predict(X_test_scaled)\r\n",
    "# roc=roc_auc_score(y_test, y_predict)\r\n",
    "# acc = accuracy_score(y_test, y_predict)\r\n",
    "# prec = precision_score(y_test, y_predict)\r\n",
    "# rec = recall_score(y_test, y_predict)\r\n",
    "# f1 = f1_score(y_test, y_predict)\r\n",
    "\r\n",
    "# model_results = pd.DataFrame([['SVC Linear', acc,prec,rec, f1,roc]],\r\n",
    "#             columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "# results = results.append(model_results, ignore_index = True)\r\n",
    "# results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Voting Classifier**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "clf1=LogisticRegression()\r\n",
    "clf2 = RandomForestClassifier()\r\n",
    "clf3=AdaBoostClassifier()\r\n",
    "clf4=XGBClassifier()\r\n",
    "clf5=SGDClassifier(max_iter=1000,loss='log')\r\n",
    "clf6=KNeighborsClassifier(n_neighbors = 7)\r\n",
    "clf7=GradientBoostingClassifier()\r\n",
    "\r\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('ada', clf3),('xgb',clf4),('sgd',clf5),('knn',clf6),('gboost',clf7)], voting='soft', weights=[1,1,2,2,1,3,2])\r\n",
    "eclf1.fit(X_train_scaled,y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:39:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()),\n",
       "                             ('ada', AdaBoostClassifier()),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, gamma=None,\n",
       "                                            gpu_id=None, importance_type='gain',\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, m...\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None,\n",
       "                                            scale_pos_weight=None,\n",
       "                                            subsample=None, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None)),\n",
       "                             ('sgd', SGDClassifier(loss='log')),\n",
       "                             ('knn', KNeighborsClassifier(n_neighbors=7)),\n",
       "                             ('gboost', GradientBoostingClassifier())],\n",
       "                 voting='soft', weights=[1, 1, 2, 2, 1, 3, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "eclf_predictions = eclf1.predict(X_test_scaled)\r\n",
    "acc = accuracy_score(y_test, eclf_predictions)\r\n",
    "prec = precision_score(y_test, eclf_predictions)\r\n",
    "rec = recall_score(y_test, eclf_predictions)\r\n",
    "f1 = f1_score(y_test, eclf_predictions)\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "roc=roc_auc_score(y_test, eclf_predictions)\r\n",
    "model_results = pd.DataFrame([['Voting Classifier ', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gboost</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN7</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.951592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "4             Adaboost  0.970760   0.983607  0.937500  0.960000  0.964077\n",
       "5               Gboost  0.976608   1.000000  0.937500  0.967742  0.968750\n",
       "6                 KNN7  0.959064   0.967213  0.921875  0.944000  0.951592\n",
       "7   Voting Classifier   0.982456   1.000000  0.953125  0.976000  0.976562"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Stacking**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "clf1=LogisticRegression()\r\n",
    "clf2 = RandomForestClassifier()\r\n",
    "clf3=AdaBoostClassifier()\r\n",
    "clf4=XGBClassifier()\r\n",
    "clf5=SGDClassifier(max_iter=1000,loss='log')\r\n",
    "clf6=GradientBoostingClassifier()\r\n",
    "knn=KNeighborsClassifier(n_neighbors = 7)\r\n",
    "\r\n",
    "\r\n",
    "sclf = StackingClassifier(classifiers=[clf1,clf2, clf3, clf4,clf5,clf6], \r\n",
    "                        meta_classifier=knn)\r\n",
    "\r\n",
    "print('10-fold cross validation:\\n')\r\n",
    "\r\n",
    "for clf, label in zip([clf1,clf2, clf3, clf4,clf5,clf6, sclf], \r\n",
    "                    ['Logistic Regression'\r\n",
    "                    'Random Forest', \r\n",
    "                    'Adaboost',\r\n",
    "                        'XGB','SGD','Gradient',\r\n",
    "                    'StackingClassifier']):\r\n",
    "\r\n",
    "    scores = model_selection.cross_val_score(clf, X_test_scaled, y_test,\r\n",
    "                                            cv=10, scoring='accuracy')\r\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \r\n",
    "        % (scores.mean(), scores.std(), label))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.96 (+/- 0.05) [Logistic RegressionRandom Forest]\n",
      "Accuracy: 0.95 (+/- 0.06) [Adaboost]\n",
      "Accuracy: 0.95 (+/- 0.06) [XGB]\n",
      "[12:40:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.94 (+/- 0.06) [SGD]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.97 (+/- 0.04) [Gradient]\n",
      "Accuracy: 0.91 (+/- 0.05) [StackingClassifier]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import time\r\n",
    "parameters = {\r\n",
    "        'min_child_weight': [1, 5,7, 10],\r\n",
    "        'max_depth': [2,3, 5,7,10,12],\r\n",
    "        'n_estimators':[10,50,100,200]\r\n",
    "        }\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "grid_search = GridSearchCV(estimator = xgb_classifier, # Make sure classifier points to the RF model\r\n",
    "                        param_grid = parameters,\r\n",
    "                        scoring = \"accuracy\",\r\n",
    "                        cv = 5,\r\n",
    "                        n_jobs = -1)\r\n",
    "\r\n",
    "t0 = time.time()\r\n",
    "grid_search.fit(X_train_scaled, y_train)\r\n",
    "t1 = time.time()\r\n",
    "print(\"Took %0.2f seconds\" % (t1 - t0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:41:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Took 39.81 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 200}"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "grid_predictions = grid_search.predict(X_test_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\r\n",
    "cm = confusion_matrix(y_test, grid_predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "sns.heatmap(cm, annot=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 48
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "acc = accuracy_score(y_test, grid_predictions)\r\n",
    "prec = precision_score(y_test, grid_predictions)\r\n",
    "rec = recall_score(y_test, grid_predictions)\r\n",
    "f1 = f1_score(y_test, grid_predictions)\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "roc=roc_auc_score(y_test, grid_predictions)\r\n",
    "model_results = pd.DataFrame([['XGBoost Optimized', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gboost</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN7</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.951592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost Optimized</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "4             Adaboost  0.970760   0.983607  0.937500  0.960000  0.964077\n",
       "5               Gboost  0.976608   1.000000  0.937500  0.967742  0.968750\n",
       "6                 KNN7  0.959064   0.967213  0.921875  0.944000  0.951592\n",
       "7   Voting Classifier   0.982456   1.000000  0.953125  0.976000  0.976562\n",
       "8    XGBoost Optimized  0.970760   1.000000  0.921875  0.959350  0.960938"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "\r\n",
    "\r\n",
    "from sklearn import svm\r\n",
    "svc = svm.SVC()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "import time\r\n",
    "parameters = {\r\n",
    "        'C':[0.1, 1, 10, 100,1000],\r\n",
    "        'gamma':[1, 0.1, 0.01, 0.001,0.0001],\r\n",
    "    'kernel':['rbf','linear']\r\n",
    "        }\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "grid_search = GridSearchCV(estimator = svc_model, # Make sure classifier points to the RF model\r\n",
    "                        param_grid = parameters,\r\n",
    "                        scoring = \"accuracy\",\r\n",
    "                        cv = 5,\r\n",
    "                        n_jobs = -1)\r\n",
    "\r\n",
    "t0 = time.time()\r\n",
    "grid_search.fit(X_train_scaled, y_train)\r\n",
    "t1 = time.time()\r\n",
    "print(\"Took %0.2f seconds\" % (t1 - t0))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'svc_model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-152d11423b6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m grid_search = GridSearchCV(estimator = svc_model, # Make sure classifier points to the RF model\n\u001b[0m\u001b[0;32m     10\u001b[0m                         \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svc_model' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 200}"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "grid_predictions = grid_search.predict(X_test_scaled)\r\n",
    "acc = accuracy_score(y_test, grid_predictions)\r\n",
    "prec = precision_score(y_test, grid_predictions)\r\n",
    "rec = recall_score(y_test, grid_predictions)\r\n",
    "f1 = f1_score(y_test, grid_predictions)\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "roc=roc_auc_score(y_test, grid_predictions)\r\n",
    "model_results = pd.DataFrame([['SVC Optimized', acc,prec,rec, f1,roc]],\r\n",
    "            columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\r\n",
    "results = results.append(model_results, ignore_index = True)\r\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gboost</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN7</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.951592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost Optimized</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVC Optimized</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.960938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score       ROC\n",
       "0  Logistic Regression  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "1              XGBOOST  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "2        Random Forest  0.953216   0.966667  0.906250  0.935484  0.943779\n",
       "3                  SGD  0.964912   1.000000  0.906250  0.950820  0.953125\n",
       "4             Adaboost  0.970760   0.983607  0.937500  0.960000  0.964077\n",
       "5               Gboost  0.976608   1.000000  0.937500  0.967742  0.968750\n",
       "6                 KNN7  0.959064   0.967213  0.921875  0.944000  0.951592\n",
       "7   Voting Classifier   0.982456   1.000000  0.953125  0.976000  0.976562\n",
       "8    XGBoost Optimized  0.970760   1.000000  0.921875  0.959350  0.960938\n",
       "9        SVC Optimized  0.970760   1.000000  0.921875  0.959350  0.960938"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import lightgbm\r\n",
    "train_data = lightgbm.Dataset(X_train_scaled, label=y_train)\r\n",
    "test_data = lightgbm.Dataset(X_test_scaled, label=y_test)\r\n",
    "\r\n",
    "\r\n",
    "#\r\n",
    "# Train the model\r\n",
    "#\r\n",
    "\r\n",
    "parameters = {\r\n",
    "    'application': 'binary',\r\n",
    "    'objective': 'binary',\r\n",
    "    'metric': 'binary_logloss',\r\n",
    "    'max_bin': 200,\r\n",
    "    'boosting': 'gbdt',\r\n",
    "    'num_leaves': 10,\r\n",
    "    'bagging_freq': 20,\r\n",
    "    'learning_rate': 0.003,\r\n",
    "    'verbose': 0\r\n",
    "}\r\n",
    "\r\n",
    "model = lightgbm.train(parameters,\r\n",
    "                    train_data,\r\n",
    "                    valid_sets=test_data,\r\n",
    "                    num_boost_round=5000,\r\n",
    "                    early_stopping_rounds=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] objective is set=binary, application=binary will be ignored. Current value: objective=binary\n",
      "[LightGBM] [Warning] objective is set=binary, application=binary will be ignored. Current value: objective=binary\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] objective is set=binary, application=binary will be ignored. Current value: objective=binary\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.658876\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.656474\n",
      "[3]\tvalid_0's binary_logloss: 0.654054\n",
      "[4]\tvalid_0's binary_logloss: 0.651729\n",
      "[5]\tvalid_0's binary_logloss: 0.649421\n",
      "[6]\tvalid_0's binary_logloss: 0.647093\n",
      "[7]\tvalid_0's binary_logloss: 0.64477\n",
      "[8]\tvalid_0's binary_logloss: 0.64255\n",
      "[9]\tvalid_0's binary_logloss: 0.640257\n",
      "[10]\tvalid_0's binary_logloss: 0.637989\n",
      "[11]\tvalid_0's binary_logloss: 0.63577\n",
      "[12]\tvalid_0's binary_logloss: 0.633564\n",
      "[13]\tvalid_0's binary_logloss: 0.631373\n",
      "[14]\tvalid_0's binary_logloss: 0.629163\n",
      "[15]\tvalid_0's binary_logloss: 0.626957\n",
      "[16]\tvalid_0's binary_logloss: 0.62485\n",
      "[17]\tvalid_0's binary_logloss: 0.622671\n",
      "[18]\tvalid_0's binary_logloss: 0.620517\n",
      "[19]\tvalid_0's binary_logloss: 0.618407\n",
      "[20]\tvalid_0's binary_logloss: 0.616312\n",
      "[21]\tvalid_0's binary_logloss: 0.614156\n",
      "[22]\tvalid_0's binary_logloss: 0.612128\n",
      "[23]\tvalid_0's binary_logloss: 0.610029\n",
      "[24]\tvalid_0's binary_logloss: 0.607956\n",
      "[25]\tvalid_0's binary_logloss: 0.605883\n",
      "[26]\tvalid_0's binary_logloss: 0.603862\n",
      "[27]\tvalid_0's binary_logloss: 0.601857\n",
      "[28]\tvalid_0's binary_logloss: 0.599861\n",
      "[29]\tvalid_0's binary_logloss: 0.597851\n",
      "[30]\tvalid_0's binary_logloss: 0.59584\n",
      "[31]\tvalid_0's binary_logloss: 0.593922\n",
      "[32]\tvalid_0's binary_logloss: 0.591906\n",
      "[33]\tvalid_0's binary_logloss: 0.589973\n",
      "[34]\tvalid_0's binary_logloss: 0.588049\n",
      "[35]\tvalid_0's binary_logloss: 0.586139\n",
      "[36]\tvalid_0's binary_logloss: 0.58417\n",
      "[37]\tvalid_0's binary_logloss: 0.582321\n",
      "[38]\tvalid_0's binary_logloss: 0.580404\n",
      "[39]\tvalid_0's binary_logloss: 0.578577\n",
      "[40]\tvalid_0's binary_logloss: 0.576655\n",
      "[41]\tvalid_0's binary_logloss: 0.574812\n",
      "[42]\tvalid_0's binary_logloss: 0.572976\n",
      "[43]\tvalid_0's binary_logloss: 0.571155\n",
      "[44]\tvalid_0's binary_logloss: 0.569277\n",
      "[45]\tvalid_0's binary_logloss: 0.567514\n",
      "[46]\tvalid_0's binary_logloss: 0.565684\n",
      "[47]\tvalid_0's binary_logloss: 0.563879\n",
      "[48]\tvalid_0's binary_logloss: 0.561933\n",
      "[49]\tvalid_0's binary_logloss: 0.560169\n",
      "[50]\tvalid_0's binary_logloss: 0.558245\n",
      "[51]\tvalid_0's binary_logloss: 0.556507\n",
      "[52]\tvalid_0's binary_logloss: 0.554604\n",
      "[53]\tvalid_0's binary_logloss: 0.552859\n",
      "[54]\tvalid_0's binary_logloss: 0.551075\n",
      "[55]\tvalid_0's binary_logloss: 0.549201\n",
      "[56]\tvalid_0's binary_logloss: 0.547511\n",
      "[57]\tvalid_0's binary_logloss: 0.545756\n",
      "[58]\tvalid_0's binary_logloss: 0.543923\n",
      "[59]\tvalid_0's binary_logloss: 0.542101\n",
      "[60]\tvalid_0's binary_logloss: 0.540422\n",
      "[61]\tvalid_0's binary_logloss: 0.538705\n",
      "[62]\tvalid_0's binary_logloss: 0.536999\n",
      "[63]\tvalid_0's binary_logloss: 0.535214\n",
      "[64]\tvalid_0's binary_logloss: 0.533596\n",
      "[65]\tvalid_0's binary_logloss: 0.53183\n",
      "[66]\tvalid_0's binary_logloss: 0.53016\n",
      "[67]\tvalid_0's binary_logloss: 0.52854\n",
      "[68]\tvalid_0's binary_logloss: 0.526801\n",
      "[69]\tvalid_0's binary_logloss: 0.525158\n",
      "[70]\tvalid_0's binary_logloss: 0.52359\n",
      "[71]\tvalid_0's binary_logloss: 0.521879\n",
      "[72]\tvalid_0's binary_logloss: 0.520261\n",
      "[73]\tvalid_0's binary_logloss: 0.518692\n",
      "[74]\tvalid_0's binary_logloss: 0.517006\n",
      "[75]\tvalid_0's binary_logloss: 0.515414\n",
      "[76]\tvalid_0's binary_logloss: 0.513895\n",
      "[77]\tvalid_0's binary_logloss: 0.512235\n",
      "[78]\tvalid_0's binary_logloss: 0.51067\n",
      "[79]\tvalid_0's binary_logloss: 0.509148\n",
      "[80]\tvalid_0's binary_logloss: 0.507513\n",
      "[81]\tvalid_0's binary_logloss: 0.506122\n",
      "[82]\tvalid_0's binary_logloss: 0.504586\n",
      "[83]\tvalid_0's binary_logloss: 0.503059\n",
      "[84]\tvalid_0's binary_logloss: 0.501457\n",
      "[85]\tvalid_0's binary_logloss: 0.500006\n",
      "[86]\tvalid_0's binary_logloss: 0.498419\n",
      "[87]\tvalid_0's binary_logloss: 0.49707\n",
      "[88]\tvalid_0's binary_logloss: 0.495581\n",
      "[89]\tvalid_0's binary_logloss: 0.494102\n",
      "[90]\tvalid_0's binary_logloss: 0.492546\n",
      "[91]\tvalid_0's binary_logloss: 0.491223\n",
      "[92]\tvalid_0's binary_logloss: 0.489764\n",
      "[93]\tvalid_0's binary_logloss: 0.488231\n",
      "[94]\tvalid_0's binary_logloss: 0.486844\n",
      "[95]\tvalid_0's binary_logloss: 0.485409\n",
      "[96]\tvalid_0's binary_logloss: 0.484118\n",
      "[97]\tvalid_0's binary_logloss: 0.482695\n",
      "[98]\tvalid_0's binary_logloss: 0.481198\n",
      "[99]\tvalid_0's binary_logloss: 0.479819\n",
      "[100]\tvalid_0's binary_logloss: 0.478417\n",
      "[101]\tvalid_0's binary_logloss: 0.476971\n",
      "[102]\tvalid_0's binary_logloss: 0.475585\n",
      "[103]\tvalid_0's binary_logloss: 0.474153\n",
      "[104]\tvalid_0's binary_logloss: 0.472779\n",
      "[105]\tvalid_0's binary_logloss: 0.471427\n",
      "[106]\tvalid_0's binary_logloss: 0.470068\n",
      "[107]\tvalid_0's binary_logloss: 0.468662\n",
      "[108]\tvalid_0's binary_logloss: 0.46733\n",
      "[109]\tvalid_0's binary_logloss: 0.465989\n",
      "[110]\tvalid_0's binary_logloss: 0.46467\n",
      "[111]\tvalid_0's binary_logloss: 0.463291\n",
      "[112]\tvalid_0's binary_logloss: 0.461971\n",
      "[113]\tvalid_0's binary_logloss: 0.460671\n",
      "[114]\tvalid_0's binary_logloss: 0.459362\n",
      "[115]\tvalid_0's binary_logloss: 0.458007\n",
      "[116]\tvalid_0's binary_logloss: 0.456713\n",
      "[117]\tvalid_0's binary_logloss: 0.455437\n",
      "[118]\tvalid_0's binary_logloss: 0.454153\n",
      "[119]\tvalid_0's binary_logloss: 0.452823\n",
      "[120]\tvalid_0's binary_logloss: 0.451566\n",
      "[121]\tvalid_0's binary_logloss: 0.4503\n",
      "[122]\tvalid_0's binary_logloss: 0.449056\n",
      "[123]\tvalid_0's binary_logloss: 0.44775\n",
      "[124]\tvalid_0's binary_logloss: 0.446503\n",
      "[125]\tvalid_0's binary_logloss: 0.445276\n",
      "[126]\tvalid_0's binary_logloss: 0.44397\n",
      "[127]\tvalid_0's binary_logloss: 0.44273\n",
      "[128]\tvalid_0's binary_logloss: 0.441436\n",
      "[129]\tvalid_0's binary_logloss: 0.440208\n",
      "[130]\tvalid_0's binary_logloss: 0.438941\n",
      "[131]\tvalid_0's binary_logloss: 0.437679\n",
      "[132]\tvalid_0's binary_logloss: 0.436469\n",
      "[133]\tvalid_0's binary_logloss: 0.435219\n",
      "[134]\tvalid_0's binary_logloss: 0.434006\n",
      "[135]\tvalid_0's binary_logloss: 0.432771\n",
      "[136]\tvalid_0's binary_logloss: 0.431567\n",
      "[137]\tvalid_0's binary_logloss: 0.430387\n",
      "[138]\tvalid_0's binary_logloss: 0.429169\n",
      "[139]\tvalid_0's binary_logloss: 0.427981\n",
      "[140]\tvalid_0's binary_logloss: 0.426818\n",
      "[141]\tvalid_0's binary_logloss: 0.425641\n",
      "[142]\tvalid_0's binary_logloss: 0.424446\n",
      "[143]\tvalid_0's binary_logloss: 0.423281\n",
      "[144]\tvalid_0's binary_logloss: 0.42219\n",
      "[145]\tvalid_0's binary_logloss: 0.421005\n",
      "[146]\tvalid_0's binary_logloss: 0.419883\n",
      "[147]\tvalid_0's binary_logloss: 0.418738\n",
      "[148]\tvalid_0's binary_logloss: 0.417626\n",
      "[149]\tvalid_0's binary_logloss: 0.416461\n",
      "[150]\tvalid_0's binary_logloss: 0.415333\n",
      "[151]\tvalid_0's binary_logloss: 0.414235\n",
      "[152]\tvalid_0's binary_logloss: 0.413092\n",
      "[153]\tvalid_0's binary_logloss: 0.411993\n",
      "[154]\tvalid_0's binary_logloss: 0.410952\n",
      "[155]\tvalid_0's binary_logloss: 0.409848\n",
      "[156]\tvalid_0's binary_logloss: 0.40871\n",
      "[157]\tvalid_0's binary_logloss: 0.407585\n",
      "[158]\tvalid_0's binary_logloss: 0.406457\n",
      "[159]\tvalid_0's binary_logloss: 0.405387\n",
      "[160]\tvalid_0's binary_logloss: 0.404309\n",
      "[161]\tvalid_0's binary_logloss: 0.403248\n",
      "[162]\tvalid_0's binary_logloss: 0.40214\n",
      "[163]\tvalid_0's binary_logloss: 0.401076\n",
      "[164]\tvalid_0's binary_logloss: 0.40003\n",
      "[165]\tvalid_0's binary_logloss: 0.398936\n",
      "[166]\tvalid_0's binary_logloss: 0.397899\n",
      "[167]\tvalid_0's binary_logloss: 0.396853\n",
      "[168]\tvalid_0's binary_logloss: 0.395825\n",
      "[169]\tvalid_0's binary_logloss: 0.394814\n",
      "[170]\tvalid_0's binary_logloss: 0.393782\n",
      "[171]\tvalid_0's binary_logloss: 0.392767\n",
      "[172]\tvalid_0's binary_logloss: 0.391769\n",
      "[173]\tvalid_0's binary_logloss: 0.390717\n",
      "[174]\tvalid_0's binary_logloss: 0.389702\n",
      "[175]\tvalid_0's binary_logloss: 0.388711\n",
      "[176]\tvalid_0's binary_logloss: 0.387705\n",
      "[177]\tvalid_0's binary_logloss: 0.386723\n",
      "[178]\tvalid_0's binary_logloss: 0.385692\n",
      "[179]\tvalid_0's binary_logloss: 0.384718\n",
      "[180]\tvalid_0's binary_logloss: 0.383728\n",
      "[181]\tvalid_0's binary_logloss: 0.382816\n",
      "[182]\tvalid_0's binary_logloss: 0.381801\n",
      "[183]\tvalid_0's binary_logloss: 0.380844\n",
      "[184]\tvalid_0's binary_logloss: 0.379994\n",
      "[185]\tvalid_0's binary_logloss: 0.379041\n",
      "[186]\tvalid_0's binary_logloss: 0.378148\n",
      "[187]\tvalid_0's binary_logloss: 0.377154\n",
      "[188]\tvalid_0's binary_logloss: 0.376268\n",
      "[189]\tvalid_0's binary_logloss: 0.375436\n",
      "[190]\tvalid_0's binary_logloss: 0.37452\n",
      "[191]\tvalid_0's binary_logloss: 0.373593\n",
      "[192]\tvalid_0's binary_logloss: 0.372772\n",
      "[193]\tvalid_0's binary_logloss: 0.37185\n",
      "[194]\tvalid_0's binary_logloss: 0.370986\n",
      "[195]\tvalid_0's binary_logloss: 0.370087\n",
      "[196]\tvalid_0's binary_logloss: 0.36923\n",
      "[197]\tvalid_0's binary_logloss: 0.368427\n",
      "[198]\tvalid_0's binary_logloss: 0.367538\n",
      "[199]\tvalid_0's binary_logloss: 0.366692\n",
      "[200]\tvalid_0's binary_logloss: 0.365898\n",
      "[201]\tvalid_0's binary_logloss: 0.365044\n",
      "[202]\tvalid_0's binary_logloss: 0.364258\n",
      "[203]\tvalid_0's binary_logloss: 0.363372\n",
      "[204]\tvalid_0's binary_logloss: 0.362555\n",
      "[205]\tvalid_0's binary_logloss: 0.361778\n",
      "[206]\tvalid_0's binary_logloss: 0.360968\n",
      "[207]\tvalid_0's binary_logloss: 0.360109\n",
      "[208]\tvalid_0's binary_logloss: 0.359343\n",
      "[209]\tvalid_0's binary_logloss: 0.358542\n",
      "[210]\tvalid_0's binary_logloss: 0.357782\n",
      "[211]\tvalid_0's binary_logloss: 0.356922\n",
      "[212]\tvalid_0's binary_logloss: 0.356118\n",
      "[213]\tvalid_0's binary_logloss: 0.355278\n",
      "[214]\tvalid_0's binary_logloss: 0.354494\n",
      "[215]\tvalid_0's binary_logloss: 0.353749\n",
      "[216]\tvalid_0's binary_logloss: 0.352933\n",
      "[217]\tvalid_0's binary_logloss: 0.35212\n",
      "[218]\tvalid_0's binary_logloss: 0.351385\n",
      "[219]\tvalid_0's binary_logloss: 0.350616\n",
      "[220]\tvalid_0's binary_logloss: 0.349886\n",
      "[221]\tvalid_0's binary_logloss: 0.349086\n",
      "[222]\tvalid_0's binary_logloss: 0.348289\n",
      "[223]\tvalid_0's binary_logloss: 0.347569\n",
      "[224]\tvalid_0's binary_logloss: 0.346766\n",
      "[225]\tvalid_0's binary_logloss: 0.345974\n",
      "[226]\tvalid_0's binary_logloss: 0.345263\n",
      "[227]\tvalid_0's binary_logloss: 0.344477\n",
      "[228]\tvalid_0's binary_logloss: 0.343771\n",
      "[229]\tvalid_0's binary_logloss: 0.34299\n",
      "[230]\tvalid_0's binary_logloss: 0.342291\n",
      "[231]\tvalid_0's binary_logloss: 0.341523\n",
      "[232]\tvalid_0's binary_logloss: 0.340741\n",
      "[233]\tvalid_0's binary_logloss: 0.340051\n",
      "[234]\tvalid_0's binary_logloss: 0.339275\n",
      "[235]\tvalid_0's binary_logloss: 0.33859\n",
      "[236]\tvalid_0's binary_logloss: 0.33782\n",
      "[237]\tvalid_0's binary_logloss: 0.337141\n",
      "[238]\tvalid_0's binary_logloss: 0.336378\n",
      "[239]\tvalid_0's binary_logloss: 0.335704\n",
      "[240]\tvalid_0's binary_logloss: 0.334963\n",
      "[241]\tvalid_0's binary_logloss: 0.334208\n",
      "[242]\tvalid_0's binary_logloss: 0.333543\n",
      "[243]\tvalid_0's binary_logloss: 0.332794\n",
      "[244]\tvalid_0's binary_logloss: 0.332135\n",
      "[245]\tvalid_0's binary_logloss: 0.331392\n",
      "[246]\tvalid_0's binary_logloss: 0.330738\n",
      "[247]\tvalid_0's binary_logloss: 0.330016\n",
      "[248]\tvalid_0's binary_logloss: 0.329281\n",
      "[249]\tvalid_0's binary_logloss: 0.328636\n",
      "[250]\tvalid_0's binary_logloss: 0.327906\n",
      "[251]\tvalid_0's binary_logloss: 0.327102\n",
      "[252]\tvalid_0's binary_logloss: 0.326394\n",
      "[253]\tvalid_0's binary_logloss: 0.325596\n",
      "[254]\tvalid_0's binary_logloss: 0.324894\n",
      "[255]\tvalid_0's binary_logloss: 0.324178\n",
      "[256]\tvalid_0's binary_logloss: 0.323384\n",
      "[257]\tvalid_0's binary_logloss: 0.32268\n",
      "[258]\tvalid_0's binary_logloss: 0.321892\n",
      "[259]\tvalid_0's binary_logloss: 0.321204\n",
      "[260]\tvalid_0's binary_logloss: 0.320422\n",
      "[261]\tvalid_0's binary_logloss: 0.319739\n",
      "[262]\tvalid_0's binary_logloss: 0.319042\n",
      "[263]\tvalid_0's binary_logloss: 0.318269\n",
      "[264]\tvalid_0's binary_logloss: 0.317584\n",
      "[265]\tvalid_0's binary_logloss: 0.316809\n",
      "[266]\tvalid_0's binary_logloss: 0.316141\n",
      "[267]\tvalid_0's binary_logloss: 0.315465\n",
      "[268]\tvalid_0's binary_logloss: 0.314705\n",
      "[269]\tvalid_0's binary_logloss: 0.314044\n",
      "[270]\tvalid_0's binary_logloss: 0.313368\n",
      "[271]\tvalid_0's binary_logloss: 0.312608\n",
      "[272]\tvalid_0's binary_logloss: 0.311956\n",
      "[273]\tvalid_0's binary_logloss: 0.311211\n",
      "[274]\tvalid_0's binary_logloss: 0.310553\n",
      "[275]\tvalid_0's binary_logloss: 0.309738\n",
      "[276]\tvalid_0's binary_logloss: 0.309096\n",
      "[277]\tvalid_0's binary_logloss: 0.308437\n",
      "[278]\tvalid_0's binary_logloss: 0.307697\n",
      "[279]\tvalid_0's binary_logloss: 0.307063\n",
      "[280]\tvalid_0's binary_logloss: 0.30627\n",
      "[281]\tvalid_0's binary_logloss: 0.305556\n",
      "[282]\tvalid_0's binary_logloss: 0.304919\n",
      "[283]\tvalid_0's binary_logloss: 0.304295\n",
      "[284]\tvalid_0's binary_logloss: 0.303653\n",
      "[285]\tvalid_0's binary_logloss: 0.302891\n",
      "[286]\tvalid_0's binary_logloss: 0.302174\n",
      "[287]\tvalid_0's binary_logloss: 0.301559\n",
      "[288]\tvalid_0's binary_logloss: 0.300927\n",
      "[289]\tvalid_0's binary_logloss: 0.300242\n",
      "[290]\tvalid_0's binary_logloss: 0.299625\n",
      "[291]\tvalid_0's binary_logloss: 0.298855\n",
      "[292]\tvalid_0's binary_logloss: 0.298252\n",
      "[293]\tvalid_0's binary_logloss: 0.297512\n",
      "[294]\tvalid_0's binary_logloss: 0.296915\n",
      "[295]\tvalid_0's binary_logloss: 0.296277\n",
      "[296]\tvalid_0's binary_logloss: 0.295606\n",
      "[297]\tvalid_0's binary_logloss: 0.294881\n",
      "[298]\tvalid_0's binary_logloss: 0.294283\n",
      "[299]\tvalid_0's binary_logloss: 0.293657\n",
      "[300]\tvalid_0's binary_logloss: 0.292985\n",
      "[301]\tvalid_0's binary_logloss: 0.292403\n",
      "[302]\tvalid_0's binary_logloss: 0.29167\n",
      "[303]\tvalid_0's binary_logloss: 0.291054\n",
      "[304]\tvalid_0's binary_logloss: 0.290391\n",
      "[305]\tvalid_0's binary_logloss: 0.289884\n",
      "[306]\tvalid_0's binary_logloss: 0.289226\n",
      "[307]\tvalid_0's binary_logloss: 0.288657\n",
      "[308]\tvalid_0's binary_logloss: 0.28794\n",
      "[309]\tvalid_0's binary_logloss: 0.287366\n",
      "[310]\tvalid_0's binary_logloss: 0.286782\n",
      "[311]\tvalid_0's binary_logloss: 0.286136\n",
      "[312]\tvalid_0's binary_logloss: 0.285578\n",
      "[313]\tvalid_0's binary_logloss: 0.284874\n",
      "[314]\tvalid_0's binary_logloss: 0.284298\n",
      "[315]\tvalid_0's binary_logloss: 0.283662\n",
      "[316]\tvalid_0's binary_logloss: 0.283101\n",
      "[317]\tvalid_0's binary_logloss: 0.282405\n",
      "[318]\tvalid_0's binary_logloss: 0.281922\n",
      "[319]\tvalid_0's binary_logloss: 0.281313\n",
      "[320]\tvalid_0's binary_logloss: 0.280752\n",
      "[321]\tvalid_0's binary_logloss: 0.280202\n",
      "[322]\tvalid_0's binary_logloss: 0.279726\n",
      "[323]\tvalid_0's binary_logloss: 0.279045\n",
      "[324]\tvalid_0's binary_logloss: 0.278428\n",
      "[325]\tvalid_0's binary_logloss: 0.277885\n",
      "[326]\tvalid_0's binary_logloss: 0.277356\n",
      "[327]\tvalid_0's binary_logloss: 0.276683\n",
      "[328]\tvalid_0's binary_logloss: 0.276139\n",
      "[329]\tvalid_0's binary_logloss: 0.275532\n",
      "[330]\tvalid_0's binary_logloss: 0.275\n",
      "[331]\tvalid_0's binary_logloss: 0.27448\n",
      "[332]\tvalid_0's binary_logloss: 0.273819\n",
      "[333]\tvalid_0's binary_logloss: 0.273304\n",
      "[334]\tvalid_0's binary_logloss: 0.272733\n",
      "[335]\tvalid_0's binary_logloss: 0.27221\n",
      "[336]\tvalid_0's binary_logloss: 0.271617\n",
      "[337]\tvalid_0's binary_logloss: 0.271092\n",
      "[338]\tvalid_0's binary_logloss: 0.270667\n",
      "[339]\tvalid_0's binary_logloss: 0.270021\n",
      "[340]\tvalid_0's binary_logloss: 0.269508\n",
      "[341]\tvalid_0's binary_logloss: 0.268867\n",
      "[342]\tvalid_0's binary_logloss: 0.268455\n",
      "[343]\tvalid_0's binary_logloss: 0.267948\n",
      "[344]\tvalid_0's binary_logloss: 0.267313\n",
      "[345]\tvalid_0's binary_logloss: 0.266906\n",
      "[346]\tvalid_0's binary_logloss: 0.266336\n",
      "[347]\tvalid_0's binary_logloss: 0.265753\n",
      "[348]\tvalid_0's binary_logloss: 0.265173\n",
      "[349]\tvalid_0's binary_logloss: 0.264772\n",
      "[350]\tvalid_0's binary_logloss: 0.264209\n",
      "[351]\tvalid_0's binary_logloss: 0.263635\n",
      "[352]\tvalid_0's binary_logloss: 0.263109\n",
      "[353]\tvalid_0's binary_logloss: 0.262708\n",
      "[354]\tvalid_0's binary_logloss: 0.262152\n",
      "[355]\tvalid_0's binary_logloss: 0.261587\n",
      "[356]\tvalid_0's binary_logloss: 0.261023\n",
      "[357]\tvalid_0's binary_logloss: 0.260635\n",
      "[358]\tvalid_0's binary_logloss: 0.260087\n",
      "[359]\tvalid_0's binary_logloss: 0.259529\n",
      "[360]\tvalid_0's binary_logloss: 0.258974\n",
      "[361]\tvalid_0's binary_logloss: 0.258592\n",
      "[362]\tvalid_0's binary_logloss: 0.258041\n",
      "[363]\tvalid_0's binary_logloss: 0.257662\n",
      "[364]\tvalid_0's binary_logloss: 0.257125\n",
      "[365]\tvalid_0's binary_logloss: 0.256596\n",
      "[366]\tvalid_0's binary_logloss: 0.256215\n",
      "[367]\tvalid_0's binary_logloss: 0.255683\n",
      "[368]\tvalid_0's binary_logloss: 0.255159\n",
      "[369]\tvalid_0's binary_logloss: 0.254695\n",
      "[370]\tvalid_0's binary_logloss: 0.254175\n",
      "[371]\tvalid_0's binary_logloss: 0.253801\n",
      "[372]\tvalid_0's binary_logloss: 0.253278\n",
      "[373]\tvalid_0's binary_logloss: 0.252763\n",
      "[374]\tvalid_0's binary_logloss: 0.252307\n",
      "[375]\tvalid_0's binary_logloss: 0.251851\n",
      "[376]\tvalid_0's binary_logloss: 0.251486\n",
      "[377]\tvalid_0's binary_logloss: 0.250971\n",
      "[378]\tvalid_0's binary_logloss: 0.250464\n",
      "[379]\tvalid_0's binary_logloss: 0.250109\n",
      "[380]\tvalid_0's binary_logloss: 0.249599\n",
      "[381]\tvalid_0's binary_logloss: 0.249096\n",
      "[382]\tvalid_0's binary_logloss: 0.248739\n",
      "[383]\tvalid_0's binary_logloss: 0.248234\n",
      "[384]\tvalid_0's binary_logloss: 0.247736\n",
      "[385]\tvalid_0's binary_logloss: 0.247204\n",
      "[386]\tvalid_0's binary_logloss: 0.246767\n",
      "[387]\tvalid_0's binary_logloss: 0.24633\n",
      "[388]\tvalid_0's binary_logloss: 0.245982\n",
      "[389]\tvalid_0's binary_logloss: 0.245456\n",
      "[390]\tvalid_0's binary_logloss: 0.244963\n",
      "[391]\tvalid_0's binary_logloss: 0.244476\n",
      "[392]\tvalid_0's binary_logloss: 0.243956\n",
      "[393]\tvalid_0's binary_logloss: 0.243615\n",
      "[394]\tvalid_0's binary_logloss: 0.243098\n",
      "[395]\tvalid_0's binary_logloss: 0.242617\n",
      "[396]\tvalid_0's binary_logloss: 0.24228\n",
      "[397]\tvalid_0's binary_logloss: 0.241769\n",
      "[398]\tvalid_0's binary_logloss: 0.241259\n",
      "[399]\tvalid_0's binary_logloss: 0.240785\n",
      "[400]\tvalid_0's binary_logloss: 0.240453\n",
      "[401]\tvalid_0's binary_logloss: 0.239955\n",
      "[402]\tvalid_0's binary_logloss: 0.239481\n",
      "[403]\tvalid_0's binary_logloss: 0.239013\n",
      "[404]\tvalid_0's binary_logloss: 0.238686\n",
      "[405]\tvalid_0's binary_logloss: 0.238195\n",
      "[406]\tvalid_0's binary_logloss: 0.237705\n",
      "[407]\tvalid_0's binary_logloss: 0.237383\n",
      "[408]\tvalid_0's binary_logloss: 0.236919\n",
      "[409]\tvalid_0's binary_logloss: 0.236459\n",
      "[410]\tvalid_0's binary_logloss: 0.235977\n",
      "[411]\tvalid_0's binary_logloss: 0.235607\n",
      "[412]\tvalid_0's binary_logloss: 0.235128\n",
      "[413]\tvalid_0's binary_logloss: 0.234674\n",
      "[414]\tvalid_0's binary_logloss: 0.234361\n",
      "[415]\tvalid_0's binary_logloss: 0.233887\n",
      "[416]\tvalid_0's binary_logloss: 0.233524\n",
      "[417]\tvalid_0's binary_logloss: 0.233073\n",
      "[418]\tvalid_0's binary_logloss: 0.232625\n",
      "[419]\tvalid_0's binary_logloss: 0.232158\n",
      "[420]\tvalid_0's binary_logloss: 0.23178\n",
      "[421]\tvalid_0's binary_logloss: 0.231337\n",
      "[422]\tvalid_0's binary_logloss: 0.230981\n",
      "[423]\tvalid_0's binary_logloss: 0.23052\n",
      "[424]\tvalid_0's binary_logloss: 0.230062\n",
      "[425]\tvalid_0's binary_logloss: 0.229763\n",
      "[426]\tvalid_0's binary_logloss: 0.22932\n",
      "[427]\tvalid_0's binary_logloss: 0.228885\n",
      "[428]\tvalid_0's binary_logloss: 0.228509\n",
      "[429]\tvalid_0's binary_logloss: 0.228162\n",
      "[430]\tvalid_0's binary_logloss: 0.227713\n",
      "[431]\tvalid_0's binary_logloss: 0.227276\n",
      "[432]\tvalid_0's binary_logloss: 0.226848\n",
      "[433]\tvalid_0's binary_logloss: 0.226564\n",
      "[434]\tvalid_0's binary_logloss: 0.226121\n",
      "[435]\tvalid_0's binary_logloss: 0.225781\n",
      "[436]\tvalid_0's binary_logloss: 0.225341\n",
      "[437]\tvalid_0's binary_logloss: 0.224919\n",
      "[438]\tvalid_0's binary_logloss: 0.224516\n",
      "[439]\tvalid_0's binary_logloss: 0.224234\n",
      "[440]\tvalid_0's binary_logloss: 0.2238\n",
      "[441]\tvalid_0's binary_logloss: 0.223401\n",
      "[442]\tvalid_0's binary_logloss: 0.22307\n",
      "[443]\tvalid_0's binary_logloss: 0.22264\n",
      "[444]\tvalid_0's binary_logloss: 0.222237\n",
      "[445]\tvalid_0's binary_logloss: 0.221962\n",
      "[446]\tvalid_0's binary_logloss: 0.22157\n",
      "[447]\tvalid_0's binary_logloss: 0.221147\n",
      "[448]\tvalid_0's binary_logloss: 0.220822\n",
      "[449]\tvalid_0's binary_logloss: 0.220435\n",
      "[450]\tvalid_0's binary_logloss: 0.220016\n",
      "[451]\tvalid_0's binary_logloss: 0.219633\n",
      "[452]\tvalid_0's binary_logloss: 0.219361\n",
      "[453]\tvalid_0's binary_logloss: 0.218946\n",
      "[454]\tvalid_0's binary_logloss: 0.218602\n",
      "[455]\tvalid_0's binary_logloss: 0.218281\n",
      "[456]\tvalid_0's binary_logloss: 0.217902\n",
      "[457]\tvalid_0's binary_logloss: 0.217591\n",
      "[458]\tvalid_0's binary_logloss: 0.217183\n",
      "[459]\tvalid_0's binary_logloss: 0.21681\n",
      "[460]\tvalid_0's binary_logloss: 0.216547\n",
      "[461]\tvalid_0's binary_logloss: 0.216143\n",
      "[462]\tvalid_0's binary_logloss: 0.215809\n",
      "[463]\tvalid_0's binary_logloss: 0.215549\n",
      "[464]\tvalid_0's binary_logloss: 0.21515\n",
      "[465]\tvalid_0's binary_logloss: 0.214772\n",
      "[466]\tvalid_0's binary_logloss: 0.214463\n",
      "[467]\tvalid_0's binary_logloss: 0.214098\n",
      "[468]\tvalid_0's binary_logloss: 0.213704\n",
      "[469]\tvalid_0's binary_logloss: 0.213451\n",
      "[470]\tvalid_0's binary_logloss: 0.213091\n",
      "[471]\tvalid_0's binary_logloss: 0.212701\n",
      "[472]\tvalid_0's binary_logloss: 0.212407\n",
      "[473]\tvalid_0's binary_logloss: 0.212113\n",
      "[474]\tvalid_0's binary_logloss: 0.211757\n",
      "[475]\tvalid_0's binary_logloss: 0.211372\n",
      "[476]\tvalid_0's binary_logloss: 0.211053\n",
      "[477]\tvalid_0's binary_logloss: 0.210818\n",
      "[478]\tvalid_0's binary_logloss: 0.210437\n",
      "[479]\tvalid_0's binary_logloss: 0.210086\n",
      "[480]\tvalid_0's binary_logloss: 0.2098\n",
      "[481]\tvalid_0's binary_logloss: 0.209452\n",
      "[482]\tvalid_0's binary_logloss: 0.209077\n",
      "[483]\tvalid_0's binary_logloss: 0.208848\n",
      "[484]\tvalid_0's binary_logloss: 0.208504\n",
      "[485]\tvalid_0's binary_logloss: 0.208132\n",
      "[486]\tvalid_0's binary_logloss: 0.207789\n",
      "[487]\tvalid_0's binary_logloss: 0.20751\n",
      "[488]\tvalid_0's binary_logloss: 0.207232\n",
      "[489]\tvalid_0's binary_logloss: 0.206925\n",
      "[490]\tvalid_0's binary_logloss: 0.20656\n",
      "[491]\tvalid_0's binary_logloss: 0.206306\n",
      "[492]\tvalid_0's binary_logloss: 0.205944\n",
      "[493]\tvalid_0's binary_logloss: 0.205608\n",
      "[494]\tvalid_0's binary_logloss: 0.205357\n",
      "[495]\tvalid_0's binary_logloss: 0.205026\n",
      "[496]\tvalid_0's binary_logloss: 0.204668\n",
      "[497]\tvalid_0's binary_logloss: 0.204421\n",
      "[498]\tvalid_0's binary_logloss: 0.204093\n",
      "[499]\tvalid_0's binary_logloss: 0.203799\n",
      "[500]\tvalid_0's binary_logloss: 0.203532\n",
      "[501]\tvalid_0's binary_logloss: 0.20318\n",
      "[502]\tvalid_0's binary_logloss: 0.202937\n",
      "[503]\tvalid_0's binary_logloss: 0.202588\n",
      "[504]\tvalid_0's binary_logloss: 0.202263\n",
      "[505]\tvalid_0's binary_logloss: 0.202023\n",
      "[506]\tvalid_0's binary_logloss: 0.201704\n",
      "[507]\tvalid_0's binary_logloss: 0.20136\n",
      "[508]\tvalid_0's binary_logloss: 0.201076\n",
      "[509]\tvalid_0's binary_logloss: 0.20084\n",
      "[510]\tvalid_0's binary_logloss: 0.200526\n",
      "[511]\tvalid_0's binary_logloss: 0.200186\n",
      "[512]\tvalid_0's binary_logloss: 0.199953\n",
      "[513]\tvalid_0's binary_logloss: 0.199698\n",
      "[514]\tvalid_0's binary_logloss: 0.199362\n",
      "[515]\tvalid_0's binary_logloss: 0.199048\n",
      "[516]\tvalid_0's binary_logloss: 0.19877\n",
      "[517]\tvalid_0's binary_logloss: 0.198541\n",
      "[518]\tvalid_0's binary_logloss: 0.198209\n",
      "[519]\tvalid_0's binary_logloss: 0.197911\n",
      "[520]\tvalid_0's binary_logloss: 0.197685\n",
      "[521]\tvalid_0's binary_logloss: 0.197338\n",
      "[522]\tvalid_0's binary_logloss: 0.197047\n",
      "[523]\tvalid_0's binary_logloss: 0.19674\n",
      "[524]\tvalid_0's binary_logloss: 0.196515\n",
      "[525]\tvalid_0's binary_logloss: 0.196172\n",
      "[526]\tvalid_0's binary_logloss: 0.195904\n",
      "[527]\tvalid_0's binary_logloss: 0.19568\n",
      "[528]\tvalid_0's binary_logloss: 0.195341\n",
      "[529]\tvalid_0's binary_logloss: 0.19511\n",
      "[530]\tvalid_0's binary_logloss: 0.194845\n",
      "[531]\tvalid_0's binary_logloss: 0.194528\n",
      "[532]\tvalid_0's binary_logloss: 0.194229\n",
      "[533]\tvalid_0's binary_logloss: 0.193916\n",
      "[534]\tvalid_0's binary_logloss: 0.193604\n",
      "[535]\tvalid_0's binary_logloss: 0.193294\n",
      "[536]\tvalid_0's binary_logloss: 0.193069\n",
      "[537]\tvalid_0's binary_logloss: 0.192774\n",
      "[538]\tvalid_0's binary_logloss: 0.192467\n",
      "[539]\tvalid_0's binary_logloss: 0.192161\n",
      "[540]\tvalid_0's binary_logloss: 0.191853\n",
      "[541]\tvalid_0's binary_logloss: 0.191549\n",
      "[542]\tvalid_0's binary_logloss: 0.191246\n",
      "[543]\tvalid_0's binary_logloss: 0.190932\n",
      "[544]\tvalid_0's binary_logloss: 0.190609\n",
      "[545]\tvalid_0's binary_logloss: 0.190309\n",
      "[546]\tvalid_0's binary_logloss: 0.190024\n",
      "[547]\tvalid_0's binary_logloss: 0.189726\n",
      "[548]\tvalid_0's binary_logloss: 0.189417\n",
      "[549]\tvalid_0's binary_logloss: 0.189224\n",
      "[550]\tvalid_0's binary_logloss: 0.188929\n",
      "[551]\tvalid_0's binary_logloss: 0.188632\n",
      "[552]\tvalid_0's binary_logloss: 0.18834\n",
      "[553]\tvalid_0's binary_logloss: 0.188062\n",
      "[554]\tvalid_0's binary_logloss: 0.187771\n",
      "[555]\tvalid_0's binary_logloss: 0.187469\n",
      "[556]\tvalid_0's binary_logloss: 0.18718\n",
      "[557]\tvalid_0's binary_logloss: 0.18687\n",
      "[558]\tvalid_0's binary_logloss: 0.186583\n",
      "[559]\tvalid_0's binary_logloss: 0.186311\n",
      "[560]\tvalid_0's binary_logloss: 0.186024\n",
      "[561]\tvalid_0's binary_logloss: 0.18584\n",
      "[562]\tvalid_0's binary_logloss: 0.185557\n",
      "[563]\tvalid_0's binary_logloss: 0.185274\n",
      "[564]\tvalid_0's binary_logloss: 0.184991\n",
      "[565]\tvalid_0's binary_logloss: 0.184711\n",
      "[566]\tvalid_0's binary_logloss: 0.184444\n",
      "[567]\tvalid_0's binary_logloss: 0.184163\n",
      "[568]\tvalid_0's binary_logloss: 0.183886\n",
      "[569]\tvalid_0's binary_logloss: 0.183708\n",
      "[570]\tvalid_0's binary_logloss: 0.183432\n",
      "[571]\tvalid_0's binary_logloss: 0.183155\n",
      "[572]\tvalid_0's binary_logloss: 0.182881\n",
      "[573]\tvalid_0's binary_logloss: 0.182622\n",
      "[574]\tvalid_0's binary_logloss: 0.182337\n",
      "[575]\tvalid_0's binary_logloss: 0.182053\n",
      "[576]\tvalid_0's binary_logloss: 0.18178\n",
      "[577]\tvalid_0's binary_logloss: 0.181469\n",
      "[578]\tvalid_0's binary_logloss: 0.181188\n",
      "[579]\tvalid_0's binary_logloss: 0.180933\n",
      "[580]\tvalid_0's binary_logloss: 0.180654\n",
      "[581]\tvalid_0's binary_logloss: 0.180385\n",
      "[582]\tvalid_0's binary_logloss: 0.180217\n",
      "[583]\tvalid_0's binary_logloss: 0.179953\n",
      "[584]\tvalid_0's binary_logloss: 0.179702\n",
      "[585]\tvalid_0's binary_logloss: 0.17944\n",
      "[586]\tvalid_0's binary_logloss: 0.179137\n",
      "[587]\tvalid_0's binary_logloss: 0.178889\n",
      "[588]\tvalid_0's binary_logloss: 0.178629\n",
      "[589]\tvalid_0's binary_logloss: 0.178381\n",
      "[590]\tvalid_0's binary_logloss: 0.178123\n",
      "[591]\tvalid_0's binary_logloss: 0.177866\n",
      "[592]\tvalid_0's binary_logloss: 0.177568\n",
      "[593]\tvalid_0's binary_logloss: 0.17733\n",
      "[594]\tvalid_0's binary_logloss: 0.177075\n",
      "[595]\tvalid_0's binary_logloss: 0.17684\n",
      "[596]\tvalid_0's binary_logloss: 0.176599\n",
      "[597]\tvalid_0's binary_logloss: 0.176355\n",
      "[598]\tvalid_0's binary_logloss: 0.176063\n",
      "[599]\tvalid_0's binary_logloss: 0.175813\n",
      "[600]\tvalid_0's binary_logloss: 0.175603\n",
      "[601]\tvalid_0's binary_logloss: 0.175373\n",
      "[602]\tvalid_0's binary_logloss: 0.175125\n",
      "[603]\tvalid_0's binary_logloss: 0.17489\n",
      "[604]\tvalid_0's binary_logloss: 0.174683\n",
      "[605]\tvalid_0's binary_logloss: 0.174457\n",
      "[606]\tvalid_0's binary_logloss: 0.17427\n",
      "[607]\tvalid_0's binary_logloss: 0.174026\n",
      "[608]\tvalid_0's binary_logloss: 0.173822\n",
      "[609]\tvalid_0's binary_logloss: 0.173598\n",
      "[610]\tvalid_0's binary_logloss: 0.173357\n",
      "[611]\tvalid_0's binary_logloss: 0.173207\n",
      "[612]\tvalid_0's binary_logloss: 0.172986\n",
      "[613]\tvalid_0's binary_logloss: 0.172785\n",
      "[614]\tvalid_0's binary_logloss: 0.172534\n",
      "[615]\tvalid_0's binary_logloss: 0.172354\n",
      "[616]\tvalid_0's binary_logloss: 0.172105\n",
      "[617]\tvalid_0's binary_logloss: 0.171906\n",
      "[618]\tvalid_0's binary_logloss: 0.171659\n",
      "[619]\tvalid_0's binary_logloss: 0.171412\n",
      "[620]\tvalid_0's binary_logloss: 0.171267\n",
      "[621]\tvalid_0's binary_logloss: 0.171046\n",
      "[622]\tvalid_0's binary_logloss: 0.17085\n",
      "[623]\tvalid_0's binary_logloss: 0.170606\n",
      "[624]\tvalid_0's binary_logloss: 0.170429\n",
      "[625]\tvalid_0's binary_logloss: 0.170187\n",
      "[626]\tvalid_0's binary_logloss: 0.169994\n",
      "[627]\tvalid_0's binary_logloss: 0.169753\n",
      "[628]\tvalid_0's binary_logloss: 0.169513\n",
      "[629]\tvalid_0's binary_logloss: 0.169374\n",
      "[630]\tvalid_0's binary_logloss: 0.169183\n",
      "[631]\tvalid_0's binary_logloss: 0.168945\n",
      "[632]\tvalid_0's binary_logloss: 0.168731\n",
      "[633]\tvalid_0's binary_logloss: 0.168574\n",
      "[634]\tvalid_0's binary_logloss: 0.168338\n",
      "[635]\tvalid_0's binary_logloss: 0.168103\n",
      "[636]\tvalid_0's binary_logloss: 0.167968\n",
      "[637]\tvalid_0's binary_logloss: 0.167781\n",
      "[638]\tvalid_0's binary_logloss: 0.167553\n",
      "[639]\tvalid_0's binary_logloss: 0.167327\n",
      "[640]\tvalid_0's binary_logloss: 0.16716\n",
      "[641]\tvalid_0's binary_logloss: 0.166935\n",
      "[642]\tvalid_0's binary_logloss: 0.166803\n",
      "[643]\tvalid_0's binary_logloss: 0.166619\n",
      "[644]\tvalid_0's binary_logloss: 0.166396\n",
      "[645]\tvalid_0's binary_logloss: 0.166191\n",
      "[646]\tvalid_0's binary_logloss: 0.166042\n",
      "[647]\tvalid_0's binary_logloss: 0.165818\n",
      "[648]\tvalid_0's binary_logloss: 0.165595\n",
      "[649]\tvalid_0's binary_logloss: 0.165467\n",
      "[650]\tvalid_0's binary_logloss: 0.165284\n",
      "[651]\tvalid_0's binary_logloss: 0.165093\n",
      "[652]\tvalid_0's binary_logloss: 0.164873\n",
      "[653]\tvalid_0's binary_logloss: 0.16467\n",
      "[654]\tvalid_0's binary_logloss: 0.16449\n",
      "[655]\tvalid_0's binary_logloss: 0.164295\n",
      "[656]\tvalid_0's binary_logloss: 0.164078\n",
      "[657]\tvalid_0's binary_logloss: 0.163861\n",
      "[658]\tvalid_0's binary_logloss: 0.163656\n",
      "[659]\tvalid_0's binary_logloss: 0.163485\n",
      "[660]\tvalid_0's binary_logloss: 0.1633\n",
      "[661]\tvalid_0's binary_logloss: 0.163086\n",
      "[662]\tvalid_0's binary_logloss: 0.162896\n",
      "[663]\tvalid_0's binary_logloss: 0.162683\n",
      "[664]\tvalid_0's binary_logloss: 0.162511\n",
      "[665]\tvalid_0's binary_logloss: 0.1623\n",
      "[666]\tvalid_0's binary_logloss: 0.162133\n",
      "[667]\tvalid_0's binary_logloss: 0.161963\n",
      "[668]\tvalid_0's binary_logloss: 0.161751\n",
      "[669]\tvalid_0's binary_logloss: 0.161585\n",
      "[670]\tvalid_0's binary_logloss: 0.161411\n",
      "[671]\tvalid_0's binary_logloss: 0.161247\n",
      "[672]\tvalid_0's binary_logloss: 0.16108\n",
      "[673]\tvalid_0's binary_logloss: 0.160806\n",
      "[674]\tvalid_0's binary_logloss: 0.160634\n",
      "[675]\tvalid_0's binary_logloss: 0.160469\n",
      "[676]\tvalid_0's binary_logloss: 0.160197\n",
      "[677]\tvalid_0's binary_logloss: 0.159925\n",
      "[678]\tvalid_0's binary_logloss: 0.159655\n",
      "[679]\tvalid_0's binary_logloss: 0.159492\n",
      "[680]\tvalid_0's binary_logloss: 0.15933\n",
      "[681]\tvalid_0's binary_logloss: 0.159062\n",
      "[682]\tvalid_0's binary_logloss: 0.158794\n",
      "[683]\tvalid_0's binary_logloss: 0.158528\n",
      "[684]\tvalid_0's binary_logloss: 0.158362\n",
      "[685]\tvalid_0's binary_logloss: 0.158207\n",
      "[686]\tvalid_0's binary_logloss: 0.158027\n",
      "[687]\tvalid_0's binary_logloss: 0.157854\n",
      "[688]\tvalid_0's binary_logloss: 0.157674\n",
      "[689]\tvalid_0's binary_logloss: 0.157526\n",
      "[690]\tvalid_0's binary_logloss: 0.157264\n",
      "[691]\tvalid_0's binary_logloss: 0.157093\n",
      "[692]\tvalid_0's binary_logloss: 0.156915\n",
      "[693]\tvalid_0's binary_logloss: 0.156673\n",
      "[694]\tvalid_0's binary_logloss: 0.156527\n",
      "[695]\tvalid_0's binary_logloss: 0.156268\n",
      "[696]\tvalid_0's binary_logloss: 0.156118\n",
      "[697]\tvalid_0's binary_logloss: 0.155943\n",
      "[698]\tvalid_0's binary_logloss: 0.155758\n",
      "[699]\tvalid_0's binary_logloss: 0.155515\n",
      "[700]\tvalid_0's binary_logloss: 0.15526\n",
      "[701]\tvalid_0's binary_logloss: 0.155076\n",
      "[702]\tvalid_0's binary_logloss: 0.154904\n",
      "[703]\tvalid_0's binary_logloss: 0.154741\n",
      "[704]\tvalid_0's binary_logloss: 0.154599\n",
      "[705]\tvalid_0's binary_logloss: 0.154428\n",
      "[706]\tvalid_0's binary_logloss: 0.154228\n",
      "[707]\tvalid_0's binary_logloss: 0.154009\n",
      "[708]\tvalid_0's binary_logloss: 0.153839\n",
      "[709]\tvalid_0's binary_logloss: 0.153658\n",
      "[710]\tvalid_0's binary_logloss: 0.153423\n",
      "[711]\tvalid_0's binary_logloss: 0.153175\n",
      "[712]\tvalid_0's binary_logloss: 0.153036\n",
      "[713]\tvalid_0's binary_logloss: 0.152829\n",
      "[714]\tvalid_0's binary_logloss: 0.152614\n",
      "[715]\tvalid_0's binary_logloss: 0.152408\n",
      "[716]\tvalid_0's binary_logloss: 0.152163\n",
      "[717]\tvalid_0's binary_logloss: 0.151986\n",
      "[718]\tvalid_0's binary_logloss: 0.151821\n",
      "[719]\tvalid_0's binary_logloss: 0.151669\n",
      "[720]\tvalid_0's binary_logloss: 0.151456\n",
      "[721]\tvalid_0's binary_logloss: 0.151254\n",
      "[722]\tvalid_0's binary_logloss: 0.151052\n",
      "[723]\tvalid_0's binary_logloss: 0.150856\n",
      "[724]\tvalid_0's binary_logloss: 0.150646\n",
      "[725]\tvalid_0's binary_logloss: 0.150452\n",
      "[726]\tvalid_0's binary_logloss: 0.15029\n",
      "[727]\tvalid_0's binary_logloss: 0.150118\n",
      "[728]\tvalid_0's binary_logloss: 0.149991\n",
      "[729]\tvalid_0's binary_logloss: 0.149783\n",
      "[730]\tvalid_0's binary_logloss: 0.149592\n",
      "[731]\tvalid_0's binary_logloss: 0.149401\n",
      "[732]\tvalid_0's binary_logloss: 0.149206\n",
      "[733]\tvalid_0's binary_logloss: 0.149\n",
      "[734]\tvalid_0's binary_logloss: 0.148811\n",
      "[735]\tvalid_0's binary_logloss: 0.148575\n",
      "[736]\tvalid_0's binary_logloss: 0.148409\n",
      "[737]\tvalid_0's binary_logloss: 0.148205\n",
      "[738]\tvalid_0's binary_logloss: 0.148048\n",
      "[739]\tvalid_0's binary_logloss: 0.147883\n",
      "[740]\tvalid_0's binary_logloss: 0.147724\n",
      "[741]\tvalid_0's binary_logloss: 0.147539\n",
      "[742]\tvalid_0's binary_logloss: 0.147349\n",
      "[743]\tvalid_0's binary_logloss: 0.147193\n",
      "[744]\tvalid_0's binary_logloss: 0.146992\n",
      "[745]\tvalid_0's binary_logloss: 0.146809\n",
      "[746]\tvalid_0's binary_logloss: 0.146627\n",
      "[747]\tvalid_0's binary_logloss: 0.146504\n",
      "[748]\tvalid_0's binary_logloss: 0.146349\n",
      "[749]\tvalid_0's binary_logloss: 0.146165\n",
      "[750]\tvalid_0's binary_logloss: 0.146043\n",
      "[751]\tvalid_0's binary_logloss: 0.14586\n",
      "[752]\tvalid_0's binary_logloss: 0.145739\n",
      "[753]\tvalid_0's binary_logloss: 0.145586\n",
      "[754]\tvalid_0's binary_logloss: 0.145404\n",
      "[755]\tvalid_0's binary_logloss: 0.145285\n",
      "[756]\tvalid_0's binary_logloss: 0.145104\n",
      "[757]\tvalid_0's binary_logloss: 0.144986\n",
      "[758]\tvalid_0's binary_logloss: 0.14484\n",
      "[759]\tvalid_0's binary_logloss: 0.144689\n",
      "[760]\tvalid_0's binary_logloss: 0.14451\n",
      "[761]\tvalid_0's binary_logloss: 0.144394\n",
      "[762]\tvalid_0's binary_logloss: 0.144208\n",
      "[763]\tvalid_0's binary_logloss: 0.144031\n",
      "[764]\tvalid_0's binary_logloss: 0.143916\n",
      "[765]\tvalid_0's binary_logloss: 0.143728\n",
      "[766]\tvalid_0's binary_logloss: 0.143552\n",
      "[767]\tvalid_0's binary_logloss: 0.143318\n",
      "[768]\tvalid_0's binary_logloss: 0.143206\n",
      "[769]\tvalid_0's binary_logloss: 0.143019\n",
      "[770]\tvalid_0's binary_logloss: 0.142845\n",
      "[771]\tvalid_0's binary_logloss: 0.142614\n",
      "[772]\tvalid_0's binary_logloss: 0.142477\n",
      "[773]\tvalid_0's binary_logloss: 0.142331\n",
      "[774]\tvalid_0's binary_logloss: 0.142224\n",
      "[775]\tvalid_0's binary_logloss: 0.142077\n",
      "[776]\tvalid_0's binary_logloss: 0.141894\n",
      "[777]\tvalid_0's binary_logloss: 0.141707\n",
      "[778]\tvalid_0's binary_logloss: 0.141479\n",
      "[779]\tvalid_0's binary_logloss: 0.14131\n",
      "[780]\tvalid_0's binary_logloss: 0.141188\n",
      "[781]\tvalid_0's binary_logloss: 0.141046\n",
      "[782]\tvalid_0's binary_logloss: 0.14082\n",
      "[783]\tvalid_0's binary_logloss: 0.140702\n",
      "[784]\tvalid_0's binary_logloss: 0.14057\n",
      "[785]\tvalid_0's binary_logloss: 0.140386\n",
      "[786]\tvalid_0's binary_logloss: 0.140219\n",
      "[787]\tvalid_0's binary_logloss: 0.140104\n",
      "[788]\tvalid_0's binary_logloss: 0.13994\n",
      "[789]\tvalid_0's binary_logloss: 0.139799\n",
      "[790]\tvalid_0's binary_logloss: 0.139617\n",
      "[791]\tvalid_0's binary_logloss: 0.139436\n",
      "[792]\tvalid_0's binary_logloss: 0.139214\n",
      "[793]\tvalid_0's binary_logloss: 0.139101\n",
      "[794]\tvalid_0's binary_logloss: 0.138944\n",
      "[795]\tvalid_0's binary_logloss: 0.138797\n",
      "[796]\tvalid_0's binary_logloss: 0.138603\n",
      "[797]\tvalid_0's binary_logloss: 0.138491\n",
      "[798]\tvalid_0's binary_logloss: 0.138314\n",
      "[799]\tvalid_0's binary_logloss: 0.138176\n",
      "[800]\tvalid_0's binary_logloss: 0.138\n",
      "[801]\tvalid_0's binary_logloss: 0.137814\n",
      "[802]\tvalid_0's binary_logloss: 0.13766\n",
      "[803]\tvalid_0's binary_logloss: 0.137468\n",
      "[804]\tvalid_0's binary_logloss: 0.137278\n",
      "[805]\tvalid_0's binary_logloss: 0.137135\n",
      "[806]\tvalid_0's binary_logloss: 0.136951\n",
      "[807]\tvalid_0's binary_logloss: 0.136799\n",
      "[808]\tvalid_0's binary_logloss: 0.136662\n",
      "[809]\tvalid_0's binary_logloss: 0.136479\n",
      "[810]\tvalid_0's binary_logloss: 0.136344\n",
      "[811]\tvalid_0's binary_logloss: 0.136173\n",
      "[812]\tvalid_0's binary_logloss: 0.135991\n",
      "[813]\tvalid_0's binary_logloss: 0.13584\n",
      "[814]\tvalid_0's binary_logloss: 0.135654\n",
      "[815]\tvalid_0's binary_logloss: 0.13552\n",
      "[816]\tvalid_0's binary_logloss: 0.135364\n",
      "[817]\tvalid_0's binary_logloss: 0.135184\n",
      "[818]\tvalid_0's binary_logloss: 0.135039\n",
      "[819]\tvalid_0's binary_logloss: 0.134908\n",
      "[820]\tvalid_0's binary_logloss: 0.134754\n",
      "[821]\tvalid_0's binary_logloss: 0.134608\n",
      "[822]\tvalid_0's binary_logloss: 0.134455\n",
      "[823]\tvalid_0's binary_logloss: 0.134278\n",
      "[824]\tvalid_0's binary_logloss: 0.134134\n",
      "[825]\tvalid_0's binary_logloss: 0.133951\n",
      "[826]\tvalid_0's binary_logloss: 0.13382\n",
      "[827]\tvalid_0's binary_logloss: 0.133667\n",
      "[828]\tvalid_0's binary_logloss: 0.133524\n",
      "[829]\tvalid_0's binary_logloss: 0.133347\n",
      "[830]\tvalid_0's binary_logloss: 0.133182\n",
      "[831]\tvalid_0's binary_logloss: 0.13307\n",
      "[832]\tvalid_0's binary_logloss: 0.132944\n",
      "[833]\tvalid_0's binary_logloss: 0.132793\n",
      "[834]\tvalid_0's binary_logloss: 0.132652\n",
      "[835]\tvalid_0's binary_logloss: 0.132502\n",
      "[836]\tvalid_0's binary_logloss: 0.132328\n",
      "[837]\tvalid_0's binary_logloss: 0.132225\n",
      "[838]\tvalid_0's binary_logloss: 0.1321\n",
      "[839]\tvalid_0's binary_logloss: 0.131961\n",
      "[840]\tvalid_0's binary_logloss: 0.131813\n",
      "[841]\tvalid_0's binary_logloss: 0.131666\n",
      "[842]\tvalid_0's binary_logloss: 0.131532\n",
      "[843]\tvalid_0's binary_logloss: 0.131408\n",
      "[844]\tvalid_0's binary_logloss: 0.131302\n",
      "[845]\tvalid_0's binary_logloss: 0.13113\n",
      "[846]\tvalid_0's binary_logloss: 0.13102\n",
      "[847]\tvalid_0's binary_logloss: 0.130875\n",
      "[848]\tvalid_0's binary_logloss: 0.130745\n",
      "[849]\tvalid_0's binary_logloss: 0.130636\n",
      "[850]\tvalid_0's binary_logloss: 0.130528\n",
      "[851]\tvalid_0's binary_logloss: 0.13037\n",
      "[852]\tvalid_0's binary_logloss: 0.130262\n",
      "[853]\tvalid_0's binary_logloss: 0.130129\n",
      "[854]\tvalid_0's binary_logloss: 0.129986\n",
      "[855]\tvalid_0's binary_logloss: 0.129831\n",
      "[856]\tvalid_0's binary_logloss: 0.129733\n",
      "[857]\tvalid_0's binary_logloss: 0.129627\n",
      "[858]\tvalid_0's binary_logloss: 0.129522\n",
      "[859]\tvalid_0's binary_logloss: 0.129381\n",
      "[860]\tvalid_0's binary_logloss: 0.129249\n",
      "[861]\tvalid_0's binary_logloss: 0.129082\n",
      "[862]\tvalid_0's binary_logloss: 0.128956\n",
      "[863]\tvalid_0's binary_logloss: 0.128852\n",
      "[864]\tvalid_0's binary_logloss: 0.128749\n",
      "[865]\tvalid_0's binary_logloss: 0.128649\n",
      "[866]\tvalid_0's binary_logloss: 0.128498\n",
      "[867]\tvalid_0's binary_logloss: 0.128357\n",
      "[868]\tvalid_0's binary_logloss: 0.128232\n",
      "[869]\tvalid_0's binary_logloss: 0.12813\n",
      "[870]\tvalid_0's binary_logloss: 0.127993\n",
      "[871]\tvalid_0's binary_logloss: 0.127864\n",
      "[872]\tvalid_0's binary_logloss: 0.127726\n",
      "[873]\tvalid_0's binary_logloss: 0.127629\n",
      "[874]\tvalid_0's binary_logloss: 0.127491\n",
      "[875]\tvalid_0's binary_logloss: 0.127347\n",
      "[876]\tvalid_0's binary_logloss: 0.127246\n",
      "[877]\tvalid_0's binary_logloss: 0.127124\n",
      "[878]\tvalid_0's binary_logloss: 0.127024\n",
      "[879]\tvalid_0's binary_logloss: 0.12689\n",
      "[880]\tvalid_0's binary_logloss: 0.126795\n",
      "[881]\tvalid_0's binary_logloss: 0.126659\n",
      "[882]\tvalid_0's binary_logloss: 0.126518\n",
      "[883]\tvalid_0's binary_logloss: 0.126397\n",
      "[884]\tvalid_0's binary_logloss: 0.126299\n",
      "[885]\tvalid_0's binary_logloss: 0.126236\n",
      "[886]\tvalid_0's binary_logloss: 0.126112\n",
      "[887]\tvalid_0's binary_logloss: 0.125979\n",
      "[888]\tvalid_0's binary_logloss: 0.125846\n",
      "[889]\tvalid_0's binary_logloss: 0.12572\n",
      "[890]\tvalid_0's binary_logloss: 0.125599\n",
      "[891]\tvalid_0's binary_logloss: 0.125467\n",
      "[892]\tvalid_0's binary_logloss: 0.125347\n",
      "[893]\tvalid_0's binary_logloss: 0.125218\n",
      "[894]\tvalid_0's binary_logloss: 0.125099\n",
      "[895]\tvalid_0's binary_logloss: 0.12498\n",
      "[896]\tvalid_0's binary_logloss: 0.12485\n",
      "[897]\tvalid_0's binary_logloss: 0.124732\n",
      "[898]\tvalid_0's binary_logloss: 0.124603\n",
      "[899]\tvalid_0's binary_logloss: 0.124486\n",
      "[900]\tvalid_0's binary_logloss: 0.124369\n",
      "[901]\tvalid_0's binary_logloss: 0.12425\n",
      "[902]\tvalid_0's binary_logloss: 0.124134\n",
      "[903]\tvalid_0's binary_logloss: 0.124018\n",
      "[904]\tvalid_0's binary_logloss: 0.123901\n",
      "[905]\tvalid_0's binary_logloss: 0.123786\n",
      "[906]\tvalid_0's binary_logloss: 0.123671\n",
      "[907]\tvalid_0's binary_logloss: 0.123555\n",
      "[908]\tvalid_0's binary_logloss: 0.123441\n",
      "[909]\tvalid_0's binary_logloss: 0.123339\n",
      "[910]\tvalid_0's binary_logloss: 0.123224\n",
      "[911]\tvalid_0's binary_logloss: 0.123111\n",
      "[912]\tvalid_0's binary_logloss: 0.122998\n",
      "[913]\tvalid_0's binary_logloss: 0.122884\n",
      "[914]\tvalid_0's binary_logloss: 0.122783\n",
      "[915]\tvalid_0's binary_logloss: 0.122671\n",
      "[916]\tvalid_0's binary_logloss: 0.122558\n",
      "[917]\tvalid_0's binary_logloss: 0.122447\n",
      "[918]\tvalid_0's binary_logloss: 0.122348\n",
      "[919]\tvalid_0's binary_logloss: 0.122235\n",
      "[920]\tvalid_0's binary_logloss: 0.122122\n",
      "[921]\tvalid_0's binary_logloss: 0.122021\n",
      "[922]\tvalid_0's binary_logloss: 0.12191\n",
      "[923]\tvalid_0's binary_logloss: 0.1218\n",
      "[924]\tvalid_0's binary_logloss: 0.12169\n",
      "[925]\tvalid_0's binary_logloss: 0.12159\n",
      "[926]\tvalid_0's binary_logloss: 0.12148\n",
      "[927]\tvalid_0's binary_logloss: 0.121377\n",
      "[928]\tvalid_0's binary_logloss: 0.121267\n",
      "[929]\tvalid_0's binary_logloss: 0.121165\n",
      "[930]\tvalid_0's binary_logloss: 0.121067\n",
      "[931]\tvalid_0's binary_logloss: 0.120959\n",
      "[932]\tvalid_0's binary_logloss: 0.120857\n",
      "[933]\tvalid_0's binary_logloss: 0.120749\n",
      "[934]\tvalid_0's binary_logloss: 0.120649\n",
      "[935]\tvalid_0's binary_logloss: 0.120541\n",
      "[936]\tvalid_0's binary_logloss: 0.120434\n",
      "[937]\tvalid_0's binary_logloss: 0.120334\n",
      "[938]\tvalid_0's binary_logloss: 0.1202\n",
      "[939]\tvalid_0's binary_logloss: 0.120101\n",
      "[940]\tvalid_0's binary_logloss: 0.119968\n",
      "[941]\tvalid_0's binary_logloss: 0.119862\n",
      "[942]\tvalid_0's binary_logloss: 0.119731\n",
      "[943]\tvalid_0's binary_logloss: 0.119634\n",
      "[944]\tvalid_0's binary_logloss: 0.119504\n",
      "[945]\tvalid_0's binary_logloss: 0.119407\n",
      "[946]\tvalid_0's binary_logloss: 0.119323\n",
      "[947]\tvalid_0's binary_logloss: 0.119227\n",
      "[948]\tvalid_0's binary_logloss: 0.119101\n",
      "[949]\tvalid_0's binary_logloss: 0.119016\n",
      "[950]\tvalid_0's binary_logloss: 0.118921\n",
      "[951]\tvalid_0's binary_logloss: 0.118796\n",
      "[952]\tvalid_0's binary_logloss: 0.118713\n",
      "[953]\tvalid_0's binary_logloss: 0.118619\n",
      "[954]\tvalid_0's binary_logloss: 0.118496\n",
      "[955]\tvalid_0's binary_logloss: 0.118402\n",
      "[956]\tvalid_0's binary_logloss: 0.118314\n",
      "[957]\tvalid_0's binary_logloss: 0.118192\n",
      "[958]\tvalid_0's binary_logloss: 0.118104\n",
      "[959]\tvalid_0's binary_logloss: 0.118011\n",
      "[960]\tvalid_0's binary_logloss: 0.11789\n",
      "[961]\tvalid_0's binary_logloss: 0.117807\n",
      "[962]\tvalid_0's binary_logloss: 0.117652\n",
      "[963]\tvalid_0's binary_logloss: 0.117569\n",
      "[964]\tvalid_0's binary_logloss: 0.117435\n",
      "[965]\tvalid_0's binary_logloss: 0.117343\n",
      "[966]\tvalid_0's binary_logloss: 0.117261\n",
      "[967]\tvalid_0's binary_logloss: 0.11718\n",
      "[968]\tvalid_0's binary_logloss: 0.117098\n",
      "[969]\tvalid_0's binary_logloss: 0.116966\n",
      "[970]\tvalid_0's binary_logloss: 0.116875\n",
      "[971]\tvalid_0's binary_logloss: 0.116791\n",
      "[972]\tvalid_0's binary_logloss: 0.116691\n",
      "[973]\tvalid_0's binary_logloss: 0.116592\n",
      "[974]\tvalid_0's binary_logloss: 0.116508\n",
      "[975]\tvalid_0's binary_logloss: 0.116431\n",
      "[976]\tvalid_0's binary_logloss: 0.116339\n",
      "[977]\tvalid_0's binary_logloss: 0.11623\n",
      "[978]\tvalid_0's binary_logloss: 0.116139\n",
      "[979]\tvalid_0's binary_logloss: 0.11606\n",
      "[980]\tvalid_0's binary_logloss: 0.11597\n",
      "[981]\tvalid_0's binary_logloss: 0.115894\n",
      "[982]\tvalid_0's binary_logloss: 0.115817\n",
      "[983]\tvalid_0's binary_logloss: 0.115728\n",
      "[984]\tvalid_0's binary_logloss: 0.115654\n",
      "[985]\tvalid_0's binary_logloss: 0.115577\n",
      "[986]\tvalid_0's binary_logloss: 0.11549\n",
      "[987]\tvalid_0's binary_logloss: 0.115412\n",
      "[988]\tvalid_0's binary_logloss: 0.115323\n",
      "[989]\tvalid_0's binary_logloss: 0.115248\n",
      "[990]\tvalid_0's binary_logloss: 0.115152\n",
      "[991]\tvalid_0's binary_logloss: 0.115066\n",
      "[992]\tvalid_0's binary_logloss: 0.114993\n",
      "[993]\tvalid_0's binary_logloss: 0.114899\n",
      "[994]\tvalid_0's binary_logloss: 0.114804\n",
      "[995]\tvalid_0's binary_logloss: 0.114734\n",
      "[996]\tvalid_0's binary_logloss: 0.114644\n",
      "[997]\tvalid_0's binary_logloss: 0.114551\n",
      "[998]\tvalid_0's binary_logloss: 0.114457\n",
      "[999]\tvalid_0's binary_logloss: 0.114365\n",
      "[1000]\tvalid_0's binary_logloss: 0.114297\n",
      "[1001]\tvalid_0's binary_logloss: 0.114206\n",
      "[1002]\tvalid_0's binary_logloss: 0.114114\n",
      "[1003]\tvalid_0's binary_logloss: 0.114023\n",
      "[1004]\tvalid_0's binary_logloss: 0.113959\n",
      "[1005]\tvalid_0's binary_logloss: 0.113869\n",
      "[1006]\tvalid_0's binary_logloss: 0.1138\n",
      "[1007]\tvalid_0's binary_logloss: 0.11371\n",
      "[1008]\tvalid_0's binary_logloss: 0.11364\n",
      "[1009]\tvalid_0's binary_logloss: 0.11356\n",
      "[1010]\tvalid_0's binary_logloss: 0.113471\n",
      "[1011]\tvalid_0's binary_logloss: 0.113408\n",
      "[1012]\tvalid_0's binary_logloss: 0.113328\n",
      "[1013]\tvalid_0's binary_logloss: 0.113249\n",
      "[1014]\tvalid_0's binary_logloss: 0.11316\n",
      "[1015]\tvalid_0's binary_logloss: 0.113072\n",
      "[1016]\tvalid_0's binary_logloss: 0.113009\n",
      "[1017]\tvalid_0's binary_logloss: 0.112925\n",
      "[1018]\tvalid_0's binary_logloss: 0.112845\n",
      "[1019]\tvalid_0's binary_logloss: 0.112758\n",
      "[1020]\tvalid_0's binary_logloss: 0.11268\n",
      "[1021]\tvalid_0's binary_logloss: 0.112593\n",
      "[1022]\tvalid_0's binary_logloss: 0.112506\n",
      "[1023]\tvalid_0's binary_logloss: 0.112412\n",
      "[1024]\tvalid_0's binary_logloss: 0.112329\n",
      "[1025]\tvalid_0's binary_logloss: 0.11225\n",
      "[1026]\tvalid_0's binary_logloss: 0.112172\n",
      "[1027]\tvalid_0's binary_logloss: 0.112087\n",
      "[1028]\tvalid_0's binary_logloss: 0.112001\n",
      "[1029]\tvalid_0's binary_logloss: 0.111908\n",
      "[1030]\tvalid_0's binary_logloss: 0.11177\n",
      "[1031]\tvalid_0's binary_logloss: 0.111692\n",
      "[1032]\tvalid_0's binary_logloss: 0.111615\n",
      "[1033]\tvalid_0's binary_logloss: 0.11155\n",
      "[1034]\tvalid_0's binary_logloss: 0.111464\n",
      "[1035]\tvalid_0's binary_logloss: 0.111386\n",
      "[1036]\tvalid_0's binary_logloss: 0.111321\n",
      "[1037]\tvalid_0's binary_logloss: 0.111236\n",
      "[1038]\tvalid_0's binary_logloss: 0.111151\n",
      "[1039]\tvalid_0's binary_logloss: 0.111074\n",
      "[1040]\tvalid_0's binary_logloss: 0.111001\n",
      "[1041]\tvalid_0's binary_logloss: 0.110917\n",
      "[1042]\tvalid_0's binary_logloss: 0.110833\n",
      "[1043]\tvalid_0's binary_logloss: 0.110757\n",
      "[1044]\tvalid_0's binary_logloss: 0.110706\n",
      "[1045]\tvalid_0's binary_logloss: 0.110623\n",
      "[1046]\tvalid_0's binary_logloss: 0.110547\n",
      "[1047]\tvalid_0's binary_logloss: 0.110497\n",
      "[1048]\tvalid_0's binary_logloss: 0.110414\n",
      "[1049]\tvalid_0's binary_logloss: 0.11034\n",
      "[1050]\tvalid_0's binary_logloss: 0.110292\n",
      "[1051]\tvalid_0's binary_logloss: 0.110211\n",
      "[1052]\tvalid_0's binary_logloss: 0.110129\n",
      "[1053]\tvalid_0's binary_logloss: 0.11008\n",
      "[1054]\tvalid_0's binary_logloss: 0.109999\n",
      "[1055]\tvalid_0's binary_logloss: 0.109926\n",
      "[1056]\tvalid_0's binary_logloss: 0.109879\n",
      "[1057]\tvalid_0's binary_logloss: 0.109799\n",
      "[1058]\tvalid_0's binary_logloss: 0.109717\n",
      "[1059]\tvalid_0's binary_logloss: 0.109644\n",
      "[1060]\tvalid_0's binary_logloss: 0.109596\n",
      "[1061]\tvalid_0's binary_logloss: 0.109517\n",
      "[1062]\tvalid_0's binary_logloss: 0.10941\n",
      "[1063]\tvalid_0's binary_logloss: 0.109328\n",
      "[1064]\tvalid_0's binary_logloss: 0.10925\n",
      "[1065]\tvalid_0's binary_logloss: 0.10918\n",
      "[1066]\tvalid_0's binary_logloss: 0.109132\n",
      "[1067]\tvalid_0's binary_logloss: 0.109055\n",
      "[1068]\tvalid_0's binary_logloss: 0.108982\n",
      "[1069]\tvalid_0's binary_logloss: 0.108938\n",
      "[1070]\tvalid_0's binary_logloss: 0.108861\n",
      "[1071]\tvalid_0's binary_logloss: 0.108785\n",
      "[1072]\tvalid_0's binary_logloss: 0.108739\n",
      "[1073]\tvalid_0's binary_logloss: 0.108661\n",
      "[1074]\tvalid_0's binary_logloss: 0.108593\n",
      "[1075]\tvalid_0's binary_logloss: 0.10847\n",
      "[1076]\tvalid_0's binary_logloss: 0.108396\n",
      "[1077]\tvalid_0's binary_logloss: 0.108318\n",
      "[1078]\tvalid_0's binary_logloss: 0.108251\n",
      "[1079]\tvalid_0's binary_logloss: 0.108191\n",
      "[1080]\tvalid_0's binary_logloss: 0.108117\n",
      "[1081]\tvalid_0's binary_logloss: 0.108044\n",
      "[1082]\tvalid_0's binary_logloss: 0.107975\n",
      "[1083]\tvalid_0's binary_logloss: 0.107912\n",
      "[1084]\tvalid_0's binary_logloss: 0.10784\n",
      "[1085]\tvalid_0's binary_logloss: 0.107719\n",
      "[1086]\tvalid_0's binary_logloss: 0.107644\n",
      "[1087]\tvalid_0's binary_logloss: 0.107582\n",
      "[1088]\tvalid_0's binary_logloss: 0.107517\n",
      "[1089]\tvalid_0's binary_logloss: 0.107477\n",
      "[1090]\tvalid_0's binary_logloss: 0.107416\n",
      "[1091]\tvalid_0's binary_logloss: 0.107347\n",
      "[1092]\tvalid_0's binary_logloss: 0.107307\n",
      "[1093]\tvalid_0's binary_logloss: 0.107236\n",
      "[1094]\tvalid_0's binary_logloss: 0.107167\n",
      "[1095]\tvalid_0's binary_logloss: 0.107069\n",
      "[1096]\tvalid_0's binary_logloss: 0.107009\n",
      "[1097]\tvalid_0's binary_logloss: 0.106933\n",
      "[1098]\tvalid_0's binary_logloss: 0.106865\n",
      "[1099]\tvalid_0's binary_logloss: 0.10683\n",
      "[1100]\tvalid_0's binary_logloss: 0.106755\n",
      "[1101]\tvalid_0's binary_logloss: 0.106689\n",
      "[1102]\tvalid_0's binary_logloss: 0.106596\n",
      "[1103]\tvalid_0's binary_logloss: 0.106522\n",
      "[1104]\tvalid_0's binary_logloss: 0.106453\n",
      "[1105]\tvalid_0's binary_logloss: 0.106419\n",
      "[1106]\tvalid_0's binary_logloss: 0.106355\n",
      "[1107]\tvalid_0's binary_logloss: 0.106304\n",
      "[1108]\tvalid_0's binary_logloss: 0.106261\n",
      "[1109]\tvalid_0's binary_logloss: 0.10616\n",
      "[1110]\tvalid_0's binary_logloss: 0.106102\n",
      "[1111]\tvalid_0's binary_logloss: 0.106035\n",
      "[1112]\tvalid_0's binary_logloss: 0.105974\n",
      "[1113]\tvalid_0's binary_logloss: 0.105934\n",
      "[1114]\tvalid_0's binary_logloss: 0.105879\n",
      "[1115]\tvalid_0's binary_logloss: 0.105782\n",
      "[1116]\tvalid_0's binary_logloss: 0.105714\n",
      "[1117]\tvalid_0's binary_logloss: 0.105666\n",
      "[1118]\tvalid_0's binary_logloss: 0.105599\n",
      "[1119]\tvalid_0's binary_logloss: 0.10556\n",
      "[1120]\tvalid_0's binary_logloss: 0.105513\n",
      "[1121]\tvalid_0's binary_logloss: 0.105449\n",
      "[1122]\tvalid_0's binary_logloss: 0.10535\n",
      "[1123]\tvalid_0's binary_logloss: 0.105296\n",
      "[1124]\tvalid_0's binary_logloss: 0.105241\n",
      "[1125]\tvalid_0's binary_logloss: 0.1052\n",
      "[1126]\tvalid_0's binary_logloss: 0.105141\n",
      "[1127]\tvalid_0's binary_logloss: 0.105042\n",
      "[1128]\tvalid_0's binary_logloss: 0.104989\n",
      "[1129]\tvalid_0's binary_logloss: 0.104886\n",
      "[1130]\tvalid_0's binary_logloss: 0.10483\n",
      "[1131]\tvalid_0's binary_logloss: 0.104791\n",
      "[1132]\tvalid_0's binary_logloss: 0.104725\n",
      "[1133]\tvalid_0's binary_logloss: 0.104679\n",
      "[1134]\tvalid_0's binary_logloss: 0.104582\n",
      "[1135]\tvalid_0's binary_logloss: 0.104525\n",
      "[1136]\tvalid_0's binary_logloss: 0.104462\n",
      "[1137]\tvalid_0's binary_logloss: 0.104356\n",
      "[1138]\tvalid_0's binary_logloss: 0.104314\n",
      "[1139]\tvalid_0's binary_logloss: 0.104209\n",
      "[1140]\tvalid_0's binary_logloss: 0.104146\n",
      "[1141]\tvalid_0's binary_logloss: 0.104094\n",
      "[1142]\tvalid_0's binary_logloss: 0.103998\n",
      "[1143]\tvalid_0's binary_logloss: 0.103943\n",
      "[1144]\tvalid_0's binary_logloss: 0.10389\n",
      "[1145]\tvalid_0's binary_logloss: 0.103852\n",
      "[1146]\tvalid_0's binary_logloss: 0.103796\n",
      "[1147]\tvalid_0's binary_logloss: 0.1037\n",
      "[1148]\tvalid_0's binary_logloss: 0.103657\n",
      "[1149]\tvalid_0's binary_logloss: 0.10362\n",
      "[1150]\tvalid_0's binary_logloss: 0.10357\n",
      "[1151]\tvalid_0's binary_logloss: 0.103511\n",
      "[1152]\tvalid_0's binary_logloss: 0.103405\n",
      "[1153]\tvalid_0's binary_logloss: 0.103349\n",
      "[1154]\tvalid_0's binary_logloss: 0.103247\n",
      "[1155]\tvalid_0's binary_logloss: 0.10321\n",
      "[1156]\tvalid_0's binary_logloss: 0.103157\n",
      "[1157]\tvalid_0's binary_logloss: 0.103104\n",
      "[1158]\tvalid_0's binary_logloss: 0.103049\n",
      "[1159]\tvalid_0's binary_logloss: 0.102995\n",
      "[1160]\tvalid_0's binary_logloss: 0.102893\n",
      "[1161]\tvalid_0's binary_logloss: 0.102841\n",
      "[1162]\tvalid_0's binary_logloss: 0.102788\n",
      "[1163]\tvalid_0's binary_logloss: 0.102756\n",
      "[1164]\tvalid_0's binary_logloss: 0.102702\n",
      "[1165]\tvalid_0's binary_logloss: 0.102601\n",
      "[1166]\tvalid_0's binary_logloss: 0.10255\n",
      "[1167]\tvalid_0's binary_logloss: 0.102495\n",
      "[1168]\tvalid_0's binary_logloss: 0.102443\n",
      "[1169]\tvalid_0's binary_logloss: 0.102343\n",
      "[1170]\tvalid_0's binary_logloss: 0.102312\n",
      "[1171]\tvalid_0's binary_logloss: 0.102261\n",
      "[1172]\tvalid_0's binary_logloss: 0.102207\n",
      "[1173]\tvalid_0's binary_logloss: 0.102108\n",
      "[1174]\tvalid_0's binary_logloss: 0.102058\n",
      "[1175]\tvalid_0's binary_logloss: 0.102004\n",
      "[1176]\tvalid_0's binary_logloss: 0.101944\n",
      "[1177]\tvalid_0's binary_logloss: 0.101849\n",
      "[1178]\tvalid_0's binary_logloss: 0.101819\n",
      "[1179]\tvalid_0's binary_logloss: 0.10177\n",
      "[1180]\tvalid_0's binary_logloss: 0.101721\n",
      "[1181]\tvalid_0's binary_logloss: 0.101635\n",
      "[1182]\tvalid_0's binary_logloss: 0.101578\n",
      "[1183]\tvalid_0's binary_logloss: 0.10148\n",
      "[1184]\tvalid_0's binary_logloss: 0.101433\n",
      "[1185]\tvalid_0's binary_logloss: 0.101338\n",
      "[1186]\tvalid_0's binary_logloss: 0.10131\n",
      "[1187]\tvalid_0's binary_logloss: 0.101262\n",
      "[1188]\tvalid_0's binary_logloss: 0.101215\n",
      "[1189]\tvalid_0's binary_logloss: 0.101169\n",
      "[1190]\tvalid_0's binary_logloss: 0.101075\n",
      "[1191]\tvalid_0's binary_logloss: 0.101047\n",
      "[1192]\tvalid_0's binary_logloss: 0.100999\n",
      "[1193]\tvalid_0's binary_logloss: 0.100952\n",
      "[1194]\tvalid_0's binary_logloss: 0.100905\n",
      "[1195]\tvalid_0's binary_logloss: 0.10086\n",
      "[1196]\tvalid_0's binary_logloss: 0.100764\n",
      "[1197]\tvalid_0's binary_logloss: 0.100665\n",
      "[1198]\tvalid_0's binary_logloss: 0.100639\n",
      "[1199]\tvalid_0's binary_logloss: 0.100591\n",
      "[1200]\tvalid_0's binary_logloss: 0.100544\n",
      "[1201]\tvalid_0's binary_logloss: 0.100498\n",
      "[1202]\tvalid_0's binary_logloss: 0.100455\n",
      "[1203]\tvalid_0's binary_logloss: 0.100356\n",
      "[1204]\tvalid_0's binary_logloss: 0.10033\n",
      "[1205]\tvalid_0's binary_logloss: 0.100284\n",
      "[1206]\tvalid_0's binary_logloss: 0.100237\n",
      "[1207]\tvalid_0's binary_logloss: 0.100192\n",
      "[1208]\tvalid_0's binary_logloss: 0.100136\n",
      "[1209]\tvalid_0's binary_logloss: 0.100081\n",
      "[1210]\tvalid_0's binary_logloss: 0.100026\n",
      "[1211]\tvalid_0's binary_logloss: 0.0999839\n",
      "[1212]\tvalid_0's binary_logloss: 0.0998928\n",
      "[1213]\tvalid_0's binary_logloss: 0.0998682\n",
      "[1214]\tvalid_0's binary_logloss: 0.0998225\n",
      "[1215]\tvalid_0's binary_logloss: 0.0997715\n",
      "[1216]\tvalid_0's binary_logloss: 0.0996811\n",
      "[1217]\tvalid_0's binary_logloss: 0.0996571\n",
      "[1218]\tvalid_0's binary_logloss: 0.0996116\n",
      "[1219]\tvalid_0's binary_logloss: 0.0995745\n",
      "[1220]\tvalid_0's binary_logloss: 0.0995212\n",
      "[1221]\tvalid_0's binary_logloss: 0.0994682\n",
      "[1222]\tvalid_0's binary_logloss: 0.0994279\n",
      "[1223]\tvalid_0's binary_logloss: 0.0993385\n",
      "[1224]\tvalid_0's binary_logloss: 0.0993155\n",
      "[1225]\tvalid_0's binary_logloss: 0.0992791\n",
      "[1226]\tvalid_0's binary_logloss: 0.0992365\n",
      "[1227]\tvalid_0's binary_logloss: 0.0992139\n",
      "[1228]\tvalid_0's binary_logloss: 0.0991764\n",
      "[1229]\tvalid_0's binary_logloss: 0.0991249\n",
      "[1230]\tvalid_0's binary_logloss: 0.0990894\n",
      "[1231]\tvalid_0's binary_logloss: 0.0990384\n",
      "[1232]\tvalid_0's binary_logloss: 0.0989503\n",
      "[1233]\tvalid_0's binary_logloss: 0.0988996\n",
      "[1234]\tvalid_0's binary_logloss: 0.0988494\n",
      "[1235]\tvalid_0's binary_logloss: 0.0988081\n",
      "[1236]\tvalid_0's binary_logloss: 0.0987868\n",
      "[1237]\tvalid_0's binary_logloss: 0.0987523\n",
      "[1238]\tvalid_0's binary_logloss: 0.0987027\n",
      "[1239]\tvalid_0's binary_logloss: 0.0986647\n",
      "[1240]\tvalid_0's binary_logloss: 0.0985777\n",
      "[1241]\tvalid_0's binary_logloss: 0.0985571\n",
      "[1242]\tvalid_0's binary_logloss: 0.0985232\n",
      "[1243]\tvalid_0's binary_logloss: 0.0984746\n",
      "[1244]\tvalid_0's binary_logloss: 0.0984346\n",
      "[1245]\tvalid_0's binary_logloss: 0.0984146\n",
      "[1246]\tvalid_0's binary_logloss: 0.0983813\n",
      "[1247]\tvalid_0's binary_logloss: 0.0983334\n",
      "[1248]\tvalid_0's binary_logloss: 0.0982858\n",
      "[1249]\tvalid_0's binary_logloss: 0.0982001\n",
      "[1250]\tvalid_0's binary_logloss: 0.0981809\n",
      "[1251]\tvalid_0's binary_logloss: 0.0981401\n",
      "[1252]\tvalid_0's binary_logloss: 0.0980932\n",
      "[1253]\tvalid_0's binary_logloss: 0.0980466\n",
      "[1254]\tvalid_0's binary_logloss: 0.0980107\n",
      "[1255]\tvalid_0's binary_logloss: 0.097964\n",
      "[1256]\tvalid_0's binary_logloss: 0.0979456\n",
      "[1257]\tvalid_0's binary_logloss: 0.0978468\n",
      "[1258]\tvalid_0's binary_logloss: 0.0978066\n",
      "[1259]\tvalid_0's binary_logloss: 0.0977887\n",
      "[1260]\tvalid_0's binary_logloss: 0.0977488\n",
      "[1261]\tvalid_0's binary_logloss: 0.0977033\n",
      "[1262]\tvalid_0's binary_logloss: 0.0976582\n",
      "[1263]\tvalid_0's binary_logloss: 0.0976133\n",
      "[1264]\tvalid_0's binary_logloss: 0.0975739\n",
      "[1265]\tvalid_0's binary_logloss: 0.097476\n",
      "[1266]\tvalid_0's binary_logloss: 0.097459\n",
      "[1267]\tvalid_0's binary_logloss: 0.0974\n",
      "[1268]\tvalid_0's binary_logloss: 0.0973349\n",
      "[1269]\tvalid_0's binary_logloss: 0.0973183\n",
      "[1270]\tvalid_0's binary_logloss: 0.0972598\n",
      "[1271]\tvalid_0's binary_logloss: 0.0971765\n",
      "[1272]\tvalid_0's binary_logloss: 0.0971604\n",
      "[1273]\tvalid_0's binary_logloss: 0.0971025\n",
      "[1274]\tvalid_0's binary_logloss: 0.0970449\n",
      "[1275]\tvalid_0's binary_logloss: 0.0970096\n",
      "[1276]\tvalid_0's binary_logloss: 0.096927\n",
      "[1277]\tvalid_0's binary_logloss: 0.0969115\n",
      "[1278]\tvalid_0's binary_logloss: 0.0968545\n",
      "[1279]\tvalid_0's binary_logloss: 0.0967586\n",
      "[1280]\tvalid_0's binary_logloss: 0.0967435\n",
      "[1281]\tvalid_0's binary_logloss: 0.0966871\n",
      "[1282]\tvalid_0's binary_logloss: 0.096631\n",
      "[1283]\tvalid_0's binary_logloss: 0.0965494\n",
      "[1284]\tvalid_0's binary_logloss: 0.0964935\n",
      "[1285]\tvalid_0's binary_logloss: 0.096479\n",
      "[1286]\tvalid_0's binary_logloss: 0.0964237\n",
      "[1287]\tvalid_0's binary_logloss: 0.0963427\n",
      "[1288]\tvalid_0's binary_logloss: 0.0963285\n",
      "[1289]\tvalid_0's binary_logloss: 0.0962737\n",
      "[1290]\tvalid_0's binary_logloss: 0.0962247\n",
      "[1291]\tvalid_0's binary_logloss: 0.0961705\n",
      "[1292]\tvalid_0's binary_logloss: 0.0960955\n",
      "[1293]\tvalid_0's binary_logloss: 0.0960819\n",
      "[1294]\tvalid_0's binary_logloss: 0.0960339\n",
      "[1295]\tvalid_0's binary_logloss: 0.0959615\n",
      "[1296]\tvalid_0's binary_logloss: 0.0959138\n",
      "[1297]\tvalid_0's binary_logloss: 0.095828\n",
      "[1298]\tvalid_0's binary_logloss: 0.0958149\n",
      "[1299]\tvalid_0's binary_logloss: 0.095767\n",
      "[1300]\tvalid_0's binary_logloss: 0.09572\n",
      "[1301]\tvalid_0's binary_logloss: 0.0956483\n",
      "[1302]\tvalid_0's binary_logloss: 0.0956357\n",
      "[1303]\tvalid_0's binary_logloss: 0.0955832\n",
      "[1304]\tvalid_0's binary_logloss: 0.0954984\n",
      "[1305]\tvalid_0's binary_logloss: 0.0954521\n",
      "[1306]\tvalid_0's binary_logloss: 0.0953812\n",
      "[1307]\tvalid_0's binary_logloss: 0.0953692\n",
      "[1308]\tvalid_0's binary_logloss: 0.0953223\n",
      "[1309]\tvalid_0's binary_logloss: 0.0952767\n",
      "[1310]\tvalid_0's binary_logloss: 0.0952063\n",
      "[1311]\tvalid_0's binary_logloss: 0.0951947\n",
      "[1312]\tvalid_0's binary_logloss: 0.0951586\n",
      "[1313]\tvalid_0's binary_logloss: 0.0951214\n",
      "[1314]\tvalid_0's binary_logloss: 0.095086\n",
      "[1315]\tvalid_0's binary_logloss: 0.0950495\n",
      "[1316]\tvalid_0's binary_logloss: 0.0950389\n",
      "[1317]\tvalid_0's binary_logloss: 0.0950043\n",
      "[1318]\tvalid_0's binary_logloss: 0.0949686\n",
      "[1319]\tvalid_0's binary_logloss: 0.0949347\n",
      "[1320]\tvalid_0's binary_logloss: 0.0949012\n",
      "[1321]\tvalid_0's binary_logloss: 0.0948447\n",
      "[1322]\tvalid_0's binary_logloss: 0.0948117\n",
      "[1323]\tvalid_0's binary_logloss: 0.0947169\n",
      "[1324]\tvalid_0's binary_logloss: 0.0946665\n",
      "[1325]\tvalid_0's binary_logloss: 0.0946164\n",
      "[1326]\tvalid_0's binary_logloss: 0.0945667\n",
      "[1327]\tvalid_0's binary_logloss: 0.0945579\n",
      "[1328]\tvalid_0's binary_logloss: 0.0945074\n",
      "[1329]\tvalid_0's binary_logloss: 0.0944583\n",
      "[1330]\tvalid_0's binary_logloss: 0.0944095\n",
      "[1331]\tvalid_0's binary_logloss: 0.0944012\n",
      "[1332]\tvalid_0's binary_logloss: 0.0943528\n",
      "[1333]\tvalid_0's binary_logloss: 0.0942706\n",
      "[1334]\tvalid_0's binary_logloss: 0.0942208\n",
      "[1335]\tvalid_0's binary_logloss: 0.094213\n",
      "[1336]\tvalid_0's binary_logloss: 0.0941653\n",
      "[1337]\tvalid_0's binary_logloss: 0.0940849\n",
      "[1338]\tvalid_0's binary_logloss: 0.0940322\n",
      "[1339]\tvalid_0's binary_logloss: 0.0939522\n",
      "[1340]\tvalid_0's binary_logloss: 0.0939449\n",
      "[1341]\tvalid_0's binary_logloss: 0.0938928\n",
      "[1342]\tvalid_0's binary_logloss: 0.0938261\n",
      "[1343]\tvalid_0's binary_logloss: 0.0937726\n",
      "[1344]\tvalid_0's binary_logloss: 0.0937659\n",
      "[1345]\tvalid_0's binary_logloss: 0.0937145\n",
      "[1346]\tvalid_0's binary_logloss: 0.0936616\n",
      "[1347]\tvalid_0's binary_logloss: 0.0936157\n",
      "[1348]\tvalid_0's binary_logloss: 0.0935486\n",
      "[1349]\tvalid_0's binary_logloss: 0.0935425\n",
      "[1350]\tvalid_0's binary_logloss: 0.093496\n",
      "[1351]\tvalid_0's binary_logloss: 0.0934294\n",
      "[1352]\tvalid_0's binary_logloss: 0.0934236\n",
      "[1353]\tvalid_0's binary_logloss: 0.0933734\n",
      "[1354]\tvalid_0's binary_logloss: 0.0932955\n",
      "[1355]\tvalid_0's binary_logloss: 0.0932457\n",
      "[1356]\tvalid_0's binary_logloss: 0.0931809\n",
      "[1357]\tvalid_0's binary_logloss: 0.0931756\n",
      "[1358]\tvalid_0's binary_logloss: 0.0931263\n",
      "[1359]\tvalid_0's binary_logloss: 0.0930753\n",
      "[1360]\tvalid_0's binary_logloss: 0.0930514\n",
      "[1361]\tvalid_0's binary_logloss: 0.0930009\n",
      "[1362]\tvalid_0's binary_logloss: 0.0929962\n",
      "[1363]\tvalid_0's binary_logloss: 0.0929486\n",
      "[1364]\tvalid_0's binary_logloss: 0.0928719\n",
      "[1365]\tvalid_0's binary_logloss: 0.0928676\n",
      "[1366]\tvalid_0's binary_logloss: 0.0928198\n",
      "[1367]\tvalid_0's binary_logloss: 0.0927909\n",
      "[1368]\tvalid_0's binary_logloss: 0.092748\n",
      "[1369]\tvalid_0's binary_logloss: 0.092664\n",
      "[1370]\tvalid_0's binary_logloss: 0.0926005\n",
      "[1371]\tvalid_0's binary_logloss: 0.0925969\n",
      "[1372]\tvalid_0's binary_logloss: 0.0925132\n",
      "[1373]\tvalid_0's binary_logloss: 0.092471\n",
      "[1374]\tvalid_0's binary_logloss: 0.0923958\n",
      "[1375]\tvalid_0's binary_logloss: 0.0923126\n",
      "[1376]\tvalid_0's binary_logloss: 0.0922499\n",
      "[1377]\tvalid_0's binary_logloss: 0.092167\n",
      "[1378]\tvalid_0's binary_logloss: 0.0921437\n",
      "[1379]\tvalid_0's binary_logloss: 0.0920612\n",
      "[1380]\tvalid_0's binary_logloss: 0.0920584\n",
      "[1381]\tvalid_0's binary_logloss: 0.0920118\n",
      "[1382]\tvalid_0's binary_logloss: 0.0919378\n",
      "[1383]\tvalid_0's binary_logloss: 0.0918556\n",
      "[1384]\tvalid_0's binary_logloss: 0.0917738\n",
      "[1385]\tvalid_0's binary_logloss: 0.0917121\n",
      "[1386]\tvalid_0's binary_logloss: 0.0916306\n",
      "[1387]\tvalid_0's binary_logloss: 0.0915574\n",
      "[1388]\tvalid_0's binary_logloss: 0.0915554\n",
      "[1389]\tvalid_0's binary_logloss: 0.0914743\n",
      "[1390]\tvalid_0's binary_logloss: 0.0914282\n",
      "[1391]\tvalid_0's binary_logloss: 0.0914061\n",
      "[1392]\tvalid_0's binary_logloss: 0.0913253\n",
      "[1393]\tvalid_0's binary_logloss: 0.091253\n",
      "[1394]\tvalid_0's binary_logloss: 0.0911726\n",
      "[1395]\tvalid_0's binary_logloss: 0.0911122\n",
      "[1396]\tvalid_0's binary_logloss: 0.0911106\n",
      "[1397]\tvalid_0's binary_logloss: 0.0910306\n",
      "[1398]\tvalid_0's binary_logloss: 0.0909843\n",
      "[1399]\tvalid_0's binary_logloss: 0.0909128\n",
      "[1400]\tvalid_0's binary_logloss: 0.0908289\n",
      "[1401]\tvalid_0's binary_logloss: 0.0907831\n",
      "[1402]\tvalid_0's binary_logloss: 0.0907076\n",
      "[1403]\tvalid_0's binary_logloss: 0.0906866\n",
      "[1404]\tvalid_0's binary_logloss: 0.0906034\n",
      "[1405]\tvalid_0's binary_logloss: 0.090558\n",
      "[1406]\tvalid_0's binary_logloss: 0.0904986\n",
      "[1407]\tvalid_0's binary_logloss: 0.0904159\n",
      "[1408]\tvalid_0's binary_logloss: 0.090371\n",
      "[1409]\tvalid_0's binary_logloss: 0.0903263\n",
      "[1410]\tvalid_0's binary_logloss: 0.090306\n",
      "[1411]\tvalid_0's binary_logloss: 0.0902239\n",
      "[1412]\tvalid_0's binary_logloss: 0.0901797\n",
      "[1413]\tvalid_0's binary_logloss: 0.0900998\n",
      "[1414]\tvalid_0's binary_logloss: 0.0900798\n",
      "[1415]\tvalid_0's binary_logloss: 0.0899984\n",
      "[1416]\tvalid_0's binary_logloss: 0.0899546\n",
      "[1417]\tvalid_0's binary_logloss: 0.0898963\n",
      "[1418]\tvalid_0's binary_logloss: 0.0898109\n",
      "[1419]\tvalid_0's binary_logloss: 0.0897676\n",
      "[1420]\tvalid_0's binary_logloss: 0.0897245\n",
      "[1421]\tvalid_0's binary_logloss: 0.0897053\n",
      "[1422]\tvalid_0's binary_logloss: 0.0896205\n",
      "[1423]\tvalid_0's binary_logloss: 0.0895779\n",
      "[1424]\tvalid_0's binary_logloss: 0.0895043\n",
      "[1425]\tvalid_0's binary_logloss: 0.0894621\n",
      "[1426]\tvalid_0's binary_logloss: 0.0894646\n",
      "[1427]\tvalid_0's binary_logloss: 0.0893806\n",
      "[1428]\tvalid_0's binary_logloss: 0.0893387\n",
      "[1429]\tvalid_0's binary_logloss: 0.0892827\n",
      "[1430]\tvalid_0's binary_logloss: 0.0892096\n",
      "[1431]\tvalid_0's binary_logloss: 0.0891665\n",
      "[1432]\tvalid_0's binary_logloss: 0.0891714\n",
      "[1433]\tvalid_0's binary_logloss: 0.0891295\n",
      "[1434]\tvalid_0's binary_logloss: 0.0891329\n",
      "[1435]\tvalid_0's binary_logloss: 0.0890541\n",
      "[1436]\tvalid_0's binary_logloss: 0.0889983\n",
      "[1437]\tvalid_0's binary_logloss: 0.0889926\n",
      "[1438]\tvalid_0's binary_logloss: 0.0889503\n",
      "[1439]\tvalid_0's binary_logloss: 0.0889094\n",
      "[1440]\tvalid_0's binary_logloss: 0.0888543\n",
      "[1441]\tvalid_0's binary_logloss: 0.0888492\n",
      "[1442]\tvalid_0's binary_logloss: 0.0888227\n",
      "[1443]\tvalid_0's binary_logloss: 0.0887825\n",
      "[1444]\tvalid_0's binary_logloss: 0.0887114\n",
      "[1445]\tvalid_0's binary_logloss: 0.0886947\n",
      "[1446]\tvalid_0's binary_logloss: 0.0887007\n",
      "[1447]\tvalid_0's binary_logloss: 0.0886597\n",
      "[1448]\tvalid_0's binary_logloss: 0.0886563\n",
      "[1449]\tvalid_0's binary_logloss: 0.0886534\n",
      "[1450]\tvalid_0's binary_logloss: 0.0886598\n",
      "[1451]\tvalid_0's binary_logloss: 0.0886193\n",
      "[1452]\tvalid_0's binary_logloss: 0.0886164\n",
      "[1453]\tvalid_0's binary_logloss: 0.088614\n",
      "[1454]\tvalid_0's binary_logloss: 0.08862\n",
      "[1455]\tvalid_0's binary_logloss: 0.0886176\n",
      "[1456]\tvalid_0's binary_logloss: 0.0886155\n",
      "[1457]\tvalid_0's binary_logloss: 0.0885756\n",
      "[1458]\tvalid_0's binary_logloss: 0.0885771\n",
      "[1459]\tvalid_0's binary_logloss: 0.0885751\n",
      "[1460]\tvalid_0's binary_logloss: 0.0885735\n",
      "[1461]\tvalid_0's binary_logloss: 0.0885719\n",
      "[1462]\tvalid_0's binary_logloss: 0.0885706\n",
      "[1463]\tvalid_0's binary_logloss: 0.0885725\n",
      "[1464]\tvalid_0's binary_logloss: 0.0885333\n",
      "[1465]\tvalid_0's binary_logloss: 0.0885322\n",
      "[1466]\tvalid_0's binary_logloss: 0.0885313\n",
      "[1467]\tvalid_0's binary_logloss: 0.0885336\n",
      "[1468]\tvalid_0's binary_logloss: 0.0884949\n",
      "[1469]\tvalid_0's binary_logloss: 0.0884943\n",
      "[1470]\tvalid_0's binary_logloss: 0.0884729\n",
      "[1471]\tvalid_0's binary_logloss: 0.0884861\n",
      "[1472]\tvalid_0's binary_logloss: 0.0884636\n",
      "[1473]\tvalid_0's binary_logloss: 0.0884427\n",
      "[1474]\tvalid_0's binary_logloss: 0.0884425\n",
      "[1475]\tvalid_0's binary_logloss: 0.0884561\n",
      "[1476]\tvalid_0's binary_logloss: 0.0884342\n",
      "[1477]\tvalid_0's binary_logloss: 0.0884139\n",
      "[1478]\tvalid_0's binary_logloss: 0.0884141\n",
      "[1479]\tvalid_0's binary_logloss: 0.0883927\n",
      "[1480]\tvalid_0's binary_logloss: 0.0883548\n",
      "[1481]\tvalid_0's binary_logloss: 0.0883632\n",
      "[1482]\tvalid_0's binary_logloss: 0.0883639\n",
      "[1483]\tvalid_0's binary_logloss: 0.0883442\n",
      "[1484]\tvalid_0's binary_logloss: 0.0883585\n",
      "[1485]\tvalid_0's binary_logloss: 0.0883378\n",
      "[1486]\tvalid_0's binary_logloss: 0.0883389\n",
      "[1487]\tvalid_0's binary_logloss: 0.0883198\n",
      "[1488]\tvalid_0's binary_logloss: 0.0882826\n",
      "[1489]\tvalid_0's binary_logloss: 0.0882916\n",
      "[1490]\tvalid_0's binary_logloss: 0.0882932\n",
      "[1491]\tvalid_0's binary_logloss: 0.0882745\n",
      "[1492]\tvalid_0's binary_logloss: 0.0882745\n",
      "[1493]\tvalid_0's binary_logloss: 0.0882894\n",
      "[1494]\tvalid_0's binary_logloss: 0.0882697\n",
      "[1495]\tvalid_0's binary_logloss: 0.0882502\n",
      "[1496]\tvalid_0's binary_logloss: 0.0882137\n",
      "[1497]\tvalid_0's binary_logloss: 0.0882233\n",
      "[1498]\tvalid_0's binary_logloss: 0.0882257\n",
      "[1499]\tvalid_0's binary_logloss: 0.0882079\n",
      "[1500]\tvalid_0's binary_logloss: 0.0881977\n",
      "[1501]\tvalid_0's binary_logloss: 0.0882133\n",
      "[1502]\tvalid_0's binary_logloss: 0.0881945\n",
      "[1503]\tvalid_0's binary_logloss: 0.0881773\n",
      "[1504]\tvalid_0's binary_logloss: 0.0881414\n",
      "[1505]\tvalid_0's binary_logloss: 0.0881517\n",
      "[1506]\tvalid_0's binary_logloss: 0.0881549\n",
      "[1507]\tvalid_0's binary_logloss: 0.0881381\n",
      "[1508]\tvalid_0's binary_logloss: 0.0881288\n",
      "[1509]\tvalid_0's binary_logloss: 0.0881449\n",
      "[1510]\tvalid_0's binary_logloss: 0.0881272\n",
      "[1511]\tvalid_0's binary_logloss: 0.0881109\n",
      "[1512]\tvalid_0's binary_logloss: 0.0880757\n",
      "[1513]\tvalid_0's binary_logloss: 0.0880865\n",
      "[1514]\tvalid_0's binary_logloss: 0.0880887\n",
      "[1515]\tvalid_0's binary_logloss: 0.0880728\n",
      "[1516]\tvalid_0's binary_logloss: 0.0880753\n",
      "[1517]\tvalid_0's binary_logloss: 0.088092\n",
      "[1518]\tvalid_0's binary_logloss: 0.0880752\n",
      "[1519]\tvalid_0's binary_logloss: 0.0880586\n",
      "[1520]\tvalid_0's binary_logloss: 0.0880239\n",
      "[1521]\tvalid_0's binary_logloss: 0.0880353\n",
      "[1522]\tvalid_0's binary_logloss: 0.0880384\n",
      "[1523]\tvalid_0's binary_logloss: 0.0880234\n",
      "[1524]\tvalid_0's binary_logloss: 0.0880159\n",
      "[1525]\tvalid_0's binary_logloss: 0.0880452\n",
      "[1526]\tvalid_0's binary_logloss: 0.0880293\n",
      "[1527]\tvalid_0's binary_logloss: 0.0879952\n",
      "[1528]\tvalid_0's binary_logloss: 0.0879953\n",
      "[1529]\tvalid_0's binary_logloss: 0.087981\n",
      "[1530]\tvalid_0's binary_logloss: 0.087985\n",
      "[1531]\tvalid_0's binary_logloss: 0.0879927\n",
      "[1532]\tvalid_0's binary_logloss: 0.0879775\n",
      "[1533]\tvalid_0's binary_logloss: 0.0879395\n",
      "[1534]\tvalid_0's binary_logloss: 0.0879405\n",
      "[1535]\tvalid_0's binary_logloss: 0.0879269\n",
      "[1536]\tvalid_0's binary_logloss: 0.0879209\n",
      "[1537]\tvalid_0's binary_logloss: 0.0879291\n",
      "[1538]\tvalid_0's binary_logloss: 0.0878868\n",
      "[1539]\tvalid_0's binary_logloss: 0.0878736\n",
      "[1540]\tvalid_0's binary_logloss: 0.0878607\n",
      "[1541]\tvalid_0's binary_logloss: 0.0878657\n",
      "[1542]\tvalid_0's binary_logloss: 0.0878959\n",
      "[1543]\tvalid_0's binary_logloss: 0.0878832\n",
      "[1544]\tvalid_0's binary_logloss: 0.0878722\n",
      "[1545]\tvalid_0's binary_logloss: 0.0878586\n",
      "[1546]\tvalid_0's binary_logloss: 0.0878233\n",
      "[1547]\tvalid_0's binary_logloss: 0.0878112\n",
      "[1548]\tvalid_0's binary_logloss: 0.0878065\n",
      "[1549]\tvalid_0's binary_logloss: 0.087837\n",
      "[1550]\tvalid_0's binary_logloss: 0.0878268\n",
      "[1551]\tvalid_0's binary_logloss: 0.087815\n",
      "[1552]\tvalid_0's binary_logloss: 0.0877853\n",
      "[1553]\tvalid_0's binary_logloss: 0.0877947\n",
      "[1554]\tvalid_0's binary_logloss: 0.0877653\n",
      "[1555]\tvalid_0's binary_logloss: 0.0877558\n",
      "[1556]\tvalid_0's binary_logloss: 0.0877522\n",
      "[1557]\tvalid_0's binary_logloss: 0.0877431\n",
      "[1558]\tvalid_0's binary_logloss: 0.0877318\n",
      "[1559]\tvalid_0's binary_logloss: 0.0876908\n",
      "[1560]\tvalid_0's binary_logloss: 0.0877008\n",
      "[1561]\tvalid_0's binary_logloss: 0.0876898\n",
      "[1562]\tvalid_0's binary_logloss: 0.0876613\n",
      "[1563]\tvalid_0's binary_logloss: 0.0876583\n",
      "[1564]\tvalid_0's binary_logloss: 0.0876686\n",
      "[1565]\tvalid_0's binary_logloss: 0.0876613\n",
      "[1566]\tvalid_0's binary_logloss: 0.0876261\n",
      "[1567]\tvalid_0's binary_logloss: 0.0875862\n",
      "[1568]\tvalid_0's binary_logloss: 0.0875968\n",
      "[1569]\tvalid_0's binary_logloss: 0.0875863\n",
      "[1570]\tvalid_0's binary_logloss: 0.0875842\n",
      "[1571]\tvalid_0's binary_logloss: 0.087577\n",
      "[1572]\tvalid_0's binary_logloss: 0.0875183\n",
      "[1573]\tvalid_0's binary_logloss: 0.0875293\n",
      "[1574]\tvalid_0's binary_logloss: 0.0875193\n",
      "[1575]\tvalid_0's binary_logloss: 0.0874562\n",
      "[1576]\tvalid_0's binary_logloss: 0.0874465\n",
      "[1577]\tvalid_0's binary_logloss: 0.0874452\n",
      "[1578]\tvalid_0's binary_logloss: 0.0874389\n",
      "[1579]\tvalid_0's binary_logloss: 0.087471\n",
      "[1580]\tvalid_0's binary_logloss: 0.0874616\n",
      "[1581]\tvalid_0's binary_logloss: 0.0874041\n",
      "[1582]\tvalid_0's binary_logloss: 0.0874034\n",
      "[1583]\tvalid_0's binary_logloss: 0.0873978\n",
      "[1584]\tvalid_0's binary_logloss: 0.0874088\n",
      "[1585]\tvalid_0's binary_logloss: 0.0874245\n",
      "[1586]\tvalid_0's binary_logloss: 0.0873628\n",
      "[1587]\tvalid_0's binary_logloss: 0.0873542\n",
      "[1588]\tvalid_0's binary_logloss: 0.0873543\n",
      "[1589]\tvalid_0's binary_logloss: 0.0873494\n",
      "[1590]\tvalid_0's binary_logloss: 0.087361\n",
      "[1591]\tvalid_0's binary_logloss: 0.0873465\n",
      "[1592]\tvalid_0's binary_logloss: 0.0873027\n",
      "[1593]\tvalid_0's binary_logloss: 0.0873147\n",
      "[1594]\tvalid_0's binary_logloss: 0.0873155\n",
      "[1595]\tvalid_0's binary_logloss: 0.0873112\n",
      "[1596]\tvalid_0's binary_logloss: 0.087297\n",
      "[1597]\tvalid_0's binary_logloss: 0.087249\n",
      "[1598]\tvalid_0's binary_logloss: 0.0872614\n",
      "[1599]\tvalid_0's binary_logloss: 0.087274\n",
      "[1600]\tvalid_0's binary_logloss: 0.0872755\n",
      "[1601]\tvalid_0's binary_logloss: 0.0872719\n",
      "[1602]\tvalid_0's binary_logloss: 0.0873051\n",
      "[1603]\tvalid_0's binary_logloss: 0.0873181\n",
      "[1604]\tvalid_0's binary_logloss: 0.0872707\n",
      "[1605]\tvalid_0's binary_logloss: 0.0872839\n",
      "[1606]\tvalid_0's binary_logloss: 0.0873016\n",
      "[1607]\tvalid_0's binary_logloss: 0.0873053\n",
      "[1608]\tvalid_0's binary_logloss: 0.0873224\n",
      "[1609]\tvalid_0's binary_logloss: 0.0873263\n",
      "[1610]\tvalid_0's binary_logloss: 0.0873238\n",
      "[1611]\tvalid_0's binary_logloss: 0.0873419\n",
      "[1612]\tvalid_0's binary_logloss: 0.0873756\n",
      "[1613]\tvalid_0's binary_logloss: 0.0873801\n",
      "[1614]\tvalid_0's binary_logloss: 0.0873781\n",
      "[1615]\tvalid_0's binary_logloss: 0.0873964\n",
      "[1616]\tvalid_0's binary_logloss: 0.0873501\n",
      "[1617]\tvalid_0's binary_logloss: 0.0873678\n",
      "[1618]\tvalid_0's binary_logloss: 0.0873865\n",
      "[1619]\tvalid_0's binary_logloss: 0.0873916\n",
      "[1620]\tvalid_0's binary_logloss: 0.0874058\n",
      "[1621]\tvalid_0's binary_logloss: 0.0873646\n",
      "[1622]\tvalid_0's binary_logloss: 0.0873927\n",
      "[1623]\tvalid_0's binary_logloss: 0.0873982\n",
      "[1624]\tvalid_0's binary_logloss: 0.0874128\n",
      "[1625]\tvalid_0's binary_logloss: 0.0874119\n",
      "[1626]\tvalid_0's binary_logloss: 0.087418\n",
      "[1627]\tvalid_0's binary_logloss: 0.0874175\n",
      "[1628]\tvalid_0's binary_logloss: 0.087446\n",
      "[1629]\tvalid_0's binary_logloss: 0.0874608\n",
      "[1630]\tvalid_0's binary_logloss: 0.0874674\n",
      "[1631]\tvalid_0's binary_logloss: 0.0874676\n",
      "[1632]\tvalid_0's binary_logloss: 0.0874827\n",
      "[1633]\tvalid_0's binary_logloss: 0.0875115\n",
      "[1634]\tvalid_0's binary_logloss: 0.0875186\n",
      "[1635]\tvalid_0's binary_logloss: 0.0875193\n",
      "[1636]\tvalid_0's binary_logloss: 0.0875347\n",
      "[1637]\tvalid_0's binary_logloss: 0.0874755\n",
      "[1638]\tvalid_0's binary_logloss: 0.0874833\n",
      "[1639]\tvalid_0's binary_logloss: 0.0874845\n",
      "[1640]\tvalid_0's binary_logloss: 0.0875137\n",
      "[1641]\tvalid_0's binary_logloss: 0.0875331\n",
      "[1642]\tvalid_0's binary_logloss: 0.0874744\n",
      "[1643]\tvalid_0's binary_logloss: 0.0874828\n",
      "[1644]\tvalid_0's binary_logloss: 0.0874582\n",
      "[1645]\tvalid_0's binary_logloss: 0.0874464\n",
      "[1646]\tvalid_0's binary_logloss: 0.0874485\n",
      "[1647]\tvalid_0's binary_logloss: 0.0873904\n",
      "[1648]\tvalid_0's binary_logloss: 0.0873994\n",
      "[1649]\tvalid_0's binary_logloss: 0.0873755\n",
      "[1650]\tvalid_0's binary_logloss: 0.0873933\n",
      "[1651]\tvalid_0's binary_logloss: 0.0873962\n",
      "[1652]\tvalid_0's binary_logloss: 0.0874261\n",
      "[1653]\tvalid_0's binary_logloss: 0.0874357\n",
      "[1654]\tvalid_0's binary_logloss: 0.0874503\n",
      "[1655]\tvalid_0's binary_logloss: 0.087393\n",
      "[1656]\tvalid_0's binary_logloss: 0.0873697\n",
      "[1657]\tvalid_0's binary_logloss: 0.0873798\n",
      "[1658]\tvalid_0's binary_logloss: 0.0873946\n",
      "[1659]\tvalid_0's binary_logloss: 0.0873994\n",
      "[1660]\tvalid_0's binary_logloss: 0.0873427\n",
      "[1661]\tvalid_0's binary_logloss: 0.0873534\n",
      "[1662]\tvalid_0's binary_logloss: 0.0873308\n",
      "[1663]\tvalid_0's binary_logloss: 0.087346\n",
      "[1664]\tvalid_0's binary_logloss: 0.0873765\n",
      "[1665]\tvalid_0's binary_logloss: 0.0873877\n",
      "[1666]\tvalid_0's binary_logloss: 0.0874031\n",
      "[1667]\tvalid_0's binary_logloss: 0.0873809\n",
      "[1668]\tvalid_0's binary_logloss: 0.087386\n",
      "[1669]\tvalid_0's binary_logloss: 0.0873301\n",
      "[1670]\tvalid_0's binary_logloss: 0.0873419\n",
      "[1671]\tvalid_0's binary_logloss: 0.0873276\n",
      "[1672]\tvalid_0's binary_logloss: 0.0873332\n",
      "[1673]\tvalid_0's binary_logloss: 0.0873507\n",
      "[1674]\tvalid_0's binary_logloss: 0.0872953\n",
      "[1675]\tvalid_0's binary_logloss: 0.0872459\n",
      "[1676]\tvalid_0's binary_logloss: 0.0872318\n",
      "[1677]\tvalid_0's binary_logloss: 0.0872489\n",
      "[1678]\tvalid_0's binary_logloss: 0.0872614\n",
      "[1679]\tvalid_0's binary_logloss: 0.0872133\n",
      "[1680]\tvalid_0's binary_logloss: 0.0871994\n",
      "[1681]\tvalid_0's binary_logloss: 0.0871448\n",
      "[1682]\tvalid_0's binary_logloss: 0.0871523\n",
      "[1683]\tvalid_0's binary_logloss: 0.087116\n",
      "[1684]\tvalid_0's binary_logloss: 0.0871335\n",
      "[1685]\tvalid_0's binary_logloss: 0.087159\n",
      "[1686]\tvalid_0's binary_logloss: 0.0871799\n",
      "[1687]\tvalid_0's binary_logloss: 0.0871978\n",
      "[1688]\tvalid_0's binary_logloss: 0.0872084\n",
      "[1689]\tvalid_0's binary_logloss: 0.0872156\n",
      "[1690]\tvalid_0's binary_logloss: 0.0871574\n",
      "[1691]\tvalid_0's binary_logloss: 0.0871757\n",
      "[1692]\tvalid_0's binary_logloss: 0.0871867\n",
      "[1693]\tvalid_0's binary_logloss: 0.0872053\n",
      "[1694]\tvalid_0's binary_logloss: 0.0872165\n",
      "[1695]\tvalid_0's binary_logloss: 0.087238\n",
      "[1696]\tvalid_0's binary_logloss: 0.0872458\n",
      "[1697]\tvalid_0's binary_logloss: 0.0872648\n",
      "[1698]\tvalid_0's binary_logloss: 0.0872763\n",
      "[1699]\tvalid_0's binary_logloss: 0.0872189\n",
      "[1700]\tvalid_0's binary_logloss: 0.0872382\n",
      "[1701]\tvalid_0's binary_logloss: 0.0872481\n",
      "[1702]\tvalid_0's binary_logloss: 0.0872702\n",
      "[1703]\tvalid_0's binary_logloss: 0.0872786\n",
      "[1704]\tvalid_0's binary_logloss: 0.0872983\n",
      "[1705]\tvalid_0's binary_logloss: 0.0873085\n",
      "[1706]\tvalid_0's binary_logloss: 0.0873171\n",
      "[1707]\tvalid_0's binary_logloss: 0.0873372\n",
      "[1708]\tvalid_0's binary_logloss: 0.0873476\n",
      "[1709]\tvalid_0's binary_logloss: 0.0873092\n",
      "[1710]\tvalid_0's binary_logloss: 0.0873296\n",
      "[1711]\tvalid_0's binary_logloss: 0.0873391\n",
      "[1712]\tvalid_0's binary_logloss: 0.0873487\n",
      "[1713]\tvalid_0's binary_logloss: 0.0873106\n",
      "[1714]\tvalid_0's binary_logloss: 0.0873315\n",
      "[1715]\tvalid_0's binary_logloss: 0.0873413\n",
      "[1716]\tvalid_0's binary_logloss: 0.0873513\n",
      "[1717]\tvalid_0's binary_logloss: 0.0873621\n",
      "[1718]\tvalid_0's binary_logloss: 0.0873852\n",
      "[1719]\tvalid_0's binary_logloss: 0.0874066\n",
      "[1720]\tvalid_0's binary_logloss: 0.0874168\n",
      "[1721]\tvalid_0's binary_logloss: 0.0874264\n",
      "[1722]\tvalid_0's binary_logloss: 0.087389\n",
      "[1723]\tvalid_0's binary_logloss: 0.0874107\n",
      "[1724]\tvalid_0's binary_logloss: 0.0874225\n",
      "[1725]\tvalid_0's binary_logloss: 0.0874445\n",
      "[1726]\tvalid_0's binary_logloss: 0.0874551\n",
      "[1727]\tvalid_0's binary_logloss: 0.0874693\n",
      "[1728]\tvalid_0's binary_logloss: 0.0874929\n",
      "[1729]\tvalid_0's binary_logloss: 0.0875032\n",
      "[1730]\tvalid_0's binary_logloss: 0.0875256\n",
      "[1731]\tvalid_0's binary_logloss: 0.0875378\n",
      "[1732]\tvalid_0's binary_logloss: 0.087501\n",
      "[1733]\tvalid_0's binary_logloss: 0.0875238\n",
      "[1734]\tvalid_0's binary_logloss: 0.087537\n",
      "[1735]\tvalid_0's binary_logloss: 0.0875613\n",
      "[1736]\tvalid_0's binary_logloss: 0.0875746\n",
      "[1737]\tvalid_0's binary_logloss: 0.0875881\n",
      "[1738]\tvalid_0's binary_logloss: 0.0875518\n",
      "[1739]\tvalid_0's binary_logloss: 0.0875752\n",
      "[1740]\tvalid_0's binary_logloss: 0.0875869\n",
      "[1741]\tvalid_0's binary_logloss: 0.0875987\n",
      "[1742]\tvalid_0's binary_logloss: 0.0876238\n",
      "[1743]\tvalid_0's binary_logloss: 0.0876358\n",
      "[1744]\tvalid_0's binary_logloss: 0.0876466\n",
      "[1745]\tvalid_0's binary_logloss: 0.0876108\n",
      "[1746]\tvalid_0's binary_logloss: 0.0875909\n",
      "[1747]\tvalid_0's binary_logloss: 0.087601\n",
      "[1748]\tvalid_0's binary_logloss: 0.08761\n",
      "[1749]\tvalid_0's binary_logloss: 0.0875661\n",
      "[1750]\tvalid_0's binary_logloss: 0.0875775\n",
      "[1751]\tvalid_0's binary_logloss: 0.087542\n",
      "[1752]\tvalid_0's binary_logloss: 0.0875235\n",
      "[1753]\tvalid_0's binary_logloss: 0.0875328\n",
      "[1754]\tvalid_0's binary_logloss: 0.0874893\n",
      "[1755]\tvalid_0's binary_logloss: 0.0875012\n",
      "[1756]\tvalid_0's binary_logloss: 0.087526\n",
      "[1757]\tvalid_0's binary_logloss: 0.0875374\n",
      "[1758]\tvalid_0's binary_logloss: 0.0875023\n",
      "[1759]\tvalid_0's binary_logloss: 0.0875139\n",
      "[1760]\tvalid_0's binary_logloss: 0.0875254\n",
      "[1761]\tvalid_0's binary_logloss: 0.0874825\n",
      "[1762]\tvalid_0's binary_logloss: 0.0874964\n",
      "[1763]\tvalid_0's binary_logloss: 0.0874776\n",
      "[1764]\tvalid_0's binary_logloss: 0.0874931\n",
      "[1765]\tvalid_0's binary_logloss: 0.0874588\n",
      "[1766]\tvalid_0's binary_logloss: 0.0874844\n",
      "[1767]\tvalid_0's binary_logloss: 0.0874953\n",
      "[1768]\tvalid_0's binary_logloss: 0.0875063\n",
      "[1769]\tvalid_0's binary_logloss: 0.0874723\n",
      "[1770]\tvalid_0's binary_logloss: 0.0874525\n",
      "[1771]\tvalid_0's binary_logloss: 0.0874667\n",
      "[1772]\tvalid_0's binary_logloss: 0.0874245\n",
      "[1773]\tvalid_0's binary_logloss: 0.0874358\n",
      "[1774]\tvalid_0's binary_logloss: 0.0874021\n",
      "[1775]\tvalid_0's binary_logloss: 0.0873836\n",
      "[1776]\tvalid_0's binary_logloss: 0.0873794\n",
      "[1777]\tvalid_0's binary_logloss: 0.087391\n",
      "[1778]\tvalid_0's binary_logloss: 0.0873493\n",
      "[1779]\tvalid_0's binary_logloss: 0.0873298\n",
      "[1780]\tvalid_0's binary_logloss: 0.087342\n",
      "[1781]\tvalid_0's binary_logloss: 0.0873087\n",
      "[1782]\tvalid_0's binary_logloss: 0.0872893\n",
      "[1783]\tvalid_0's binary_logloss: 0.0873035\n",
      "Early stopping, best iteration is:\n",
      "[1683]\tvalid_0's binary_logloss: 0.087116\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}